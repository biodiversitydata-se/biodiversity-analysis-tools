# Data cleaning

Biodiversity data repositories work hard to maintain the accuracy of their holdings. When multiple sources are involved several problems may arise. Here we shall quickly outline what they are and possible solutions.

## Resources
There are a number of libraries, work flows and online resources for automating downloading and cleaning of data. THese include:

  - The [BDverse](https://bdverse.org/). A group of libraries for cleaning biodiversity data
  - [Kurator]([http://kurator.acis.ufl.edu/kurator-web/)
  - [Wallace](https://wallaceecomod.github.io/)
  

## Taxonomies

It is important to be aware of likely taxonomic anomalies prior to working within a region. Checklists are very important, especially if working over several regions / countries. Whilst there are many things that will automatically look for the validity of a name they do not check for the validity of that species occurrence. For example *Sphagnum auriculatum* Schimp. and *Sphagnum denticulatum* Bridel, 1826 are both valid names. *S. auriculatum* is the currently accepted species in Europe but in the British Isles, Ireland and the Netherlands *s. denticulatum* is the most recorded taxa. Both  are legitimate names but they are a synonymy. Both names have been used in Europe but in distinct countries. The current European checklist [@hodgetts_annotated_2020] has *Sphagnum auriculatum* Schimp. as the accepted taxon occurring in Europe. Naive downloading would result in two taxa being present when in fact it is two interpretations of the same taxon. Using data from across the European region without acknowledging this disagreement would impact the results of any research undertaken. For taxa which are known to be capable of dispersing great distances (eg. birds) this becomes even more difficult especially when using community sourced data.

```{r start-up, warning=FALSE, echo=FALSE, message=FALSE}
library(data.table)
library(sf)
library(ggplot2)
library(countrycode)
library(rnaturalearth)
library(cowplot)
library(dplyr)
library(kableExtra)
theme_set(theme_cowplot())
library(rnaturalearthdata)
library(rgbif)
library(BIRDS)

library(conflicted)
conflicts_prefer( dplyr::select, dplyr::filter)

# Data for Sphagnum denticulatum doi: 10.15468/dl.rrp4p4
# Data for Sphagnum auriculatum doi: 10.15468/dl.3yrtw7

df <- lapply(list.files("./data/Downloads/",
                        full.names = TRUE,
                        pattern = ".csv"),
             function(q){
               fread(q,encoding = "UTF-8")
             })
df <- do.call("rbind",df)
EUR <- coastline110
EUR <- st_as_sf(EUR)

ggplot(data = EUR) +
  geom_sf() +
  xlim(-20,30) +
  ylim(45,75) +
  geom_point(data = df, 
             aes(x = decimalLongitude,
                 y = decimalLatitude,
                 colour = species),
             alpha = 0.5) +
  theme(legend.position = "bottom",
        axis.title = element_blank()) +
  scale_color_discrete(name = "Species")

```
The example above uses data downloaded from GBIF. 

- Data for *Sphagnum denticulatum* doi: 10.15468/dl.rrp4p4
- Data for *Sphagnum auriculatum* doi: 10.15468/dl.3yrtw7

Within Sweden there is an agreed taxonomy for all extant taxa accessible through [Dyntaxa](https://www.dyntaxa.se). Checking of species lists can be done by directly copying species names into a dialogue box or uploading an excel spreadsheet in the correct format.


## Location data 

### Locality information

Many records have locality information attached to them. Where there are no coordinates attached this information can be used to locate the record to within an area of where records most likely came from. There are functions for which geocoding can be done automatically within R. Geocoding is finding the coordinate for a known locality. These are included in the libraries:

- **ggmap** *requires google API key*
- **tidygeocoder**

Many localities may not be included in the gazetteers associated with these libraries but may be located using online or printed maps. This takes time but may be useful. Unique localities may be extracted, geocoded and then merged back with the data set.

```{r localities, eval = FALSE, echo=FALSE}
## this example is not relevant as the data is not in the repository and doesn't shown how to geolocate
localities <- df |>
  filter(is.na(decimalLatitude))|>
  select(locality)|>
  distinct()

write.csv(localities,"DataFrameOfLocalities.csv",row.names = FALSE)

# Add columns to each row of the exported localities with any of the locations that may be found

localities <- read.csv("DataFrameOfLocalitiesGeodcoded.csv")

recordsDataFrame <- merge(recordsDataFrame, locallities, by = "localities", all = TRUE)

```

This becomes especially important when extracting records across country boundaries as countries have different numbers of georeferenced observations.
```{r georef, echo = FALSE}
EU <- c('AL', 'AD', 'AM', 'AT', 'BY', 'BE', 'BA', 'BG', 'CH', 'CY', 'CZ', 'DE',
        'DK', 'EE', 'ES', 'FO', 'FI', 'FR', 'GB', 'GE', 'GI', 'GR', 'HU', 'HR',
        'IE', 'IS', 'IT', 'LI', 'LT', 'LU', 'LV', 'MC', 'MK', 'MT', 'NO', 'NL', 'PL',
        'PT', 'RO', 'RU', 'SE', 'SI', 'SK', 'SM', 'TR', 'UA', 'VA')

Nrecs <- data.frame(
  country = countrycode(EU,"iso2c","country.name.en"),
  TR = sapply(EU,function(q){occ_count(taxonKey = 35, country = q)}),
  Gr = sapply(EU,function(q){occ_count(taxonKey = 35, country = q, georeferenced = TRUE)}),
  Nc = sapply(EU,function(q){occ_count(taxonKey = 35, country = q, georeferenced = FALSE)})
)

Nrecs <- Nrecs |>
  mutate(GrP = round((Gr/TR)*100,2),
         NcP = round((Nc/TR)*100,2)) |>
  arrange(country)


kable(Nrecs |>
  select(country,TR,GrP,NcP),
  col.names = c("Country", "Total Records",  "% Records with coordinate","% Records without coordinates"),
  caption = "The number and percentage of records of bryophyta per EU country with and without coordinates") |>
  scroll_box(height = "350px",width = "100%") |>
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = TRUE)
```
As can be seen from the above example for European Bryophyta (Hornworts, Liverworts and Mosses) the number of records with coordinates vastly varies between countries. For example of 959444 records in Sweden 88.24 % have coordinates in Switzerland of 117132 records only 2.29 % have coordinates.

### Coordinate uncertainty
In many cases there is now an abundance of biodiversity data with coordinates. As can be seen from a summary of the above *Sphagnum* data coordinate uncertainty can vary from less than 1 meter to multiple kilometers. 

```{r coord_uncertainty, echo=FALSE}
dat <- rbind(as.data.frame(table(cut(df$coordinateUncertaintyInMeters,
                                     breaks = c(0,1,10,100,500,1000,5000,10000,50000,
                                                max(df$coordinateUncertaintyInMeters,
                                                    na.rm = TRUE))))),
             data.frame(Var1 = "NA", Freq = sum(is.na(df$coordinateUncertaintyInMeters))))

  dat$Var1 <- c("<1 m",
                "1-10 m",
                "10-100 m",
                "100-500 m",
                "500-1000 m",
                "1-5 km",
                "5-10 km",
                "10-50 km",
                ">50 km",
                "None")
names(dat) <- c("Coordinate uncertainty", "Frequency")

kable(t(dat)) |>
  kable_styling() |>
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = TRUE)
```

In the above example it can be seen that two have uncertainties greater than 50 km and several thousand records that have no known error margin the location. It is important to consider what the error is and removing those records for which the uncertainty is too high. Where this point is will be dependent on the scale of the research.

<!-- Coordinates with NA as the uncertainty and those that fall below a particular threshold may be removed by: -->

```{r filter_coord, eval=FALSE, echo=FALSE}
df <- df |>
  filter(is.na(coordinateUncertaintyInMeters) == FALSE) |>
  filter(coordinateUncertaintyInMeters < 50000)
```

### Coordinate errors

Besides the declared uncertainty of the the coordinates, coordinate errors may occur for a variety of reasons. The library **coordinateCleaner** is very useful for removing some of the most common errors. These include:

- 0 latitude, 0 longitude
- swapping of latitude and longitude
- The location of the institution holding a preserved sample rather than the location of origin of that sample

## Examples
### Data cleaning Cyperaceae in Sweden

In this example I have downloaded the occurrences of the Cyperaceae family in Sweden from SBDI since 2000. This is a single data set from an area with an agreed taxonomy [dyntaxa](www.dyntaxa.se). Our aim here is to download a group of records and clean them to be in such a state than they reflect the question the researcher has in mind.

First we give each record in the assembled data set a unique ID. In this case it could be possible to use the GBIFid. In data sets sourced from multiple sources it is a good idea to create a unique ID related to the data source.

```{r hiden-data-search, warning=FALSE, error=FALSE, message=FALSE, eval=FALSE, echo=FALSE}
library(BIRDS)
library(SBDI4R)

## TODO optimize the search so that the dataset is smaller than 5mb
Carex <- occurrences(taxon="Carex",
                     fq="year:[2000 TO 2020]",
                     email="test@test.org", 
                     download_reason_id=10)

#it creates month data that is currently lacking from the SBDI API
Carex$data$day <- ifelse(is.na(Carex$data$day), 
                       sample(c(1:31), 1, replace = TRUE),
                       Carex$data$day)
Carex$data$month <- ifelse(Carex$data$day == 31, 
                       sample(c(1,3,5,7,8,10,12), 1, replace = TRUE),
                       ifelse(Carex$data$day>=28, 
                              sample(c(1,3:12), 1, replace = TRUE), #not February
                              sample(c(1:12), 1, replace = TRUE)
                              )
                       )
Carex <- Carex$data
save(Carex, file = "./data/Cyperacea_SWE.rdata")
```

```{r loadCarex, error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
load("./data/Cyperacea_SWE.rdata") 

```

In cleaning data there are a number of dimensions of uncertainty we shall first check.

### Taxonomy
In the species column there are records with no Species given in the species column. We then check the `scientificName` for whether there is useful information there
```{r taxonomy, warning=FALSE, error=FALSE, message=FALSE}
kable(Carex |>
  filter(species == "") |>
  distinct(scientificName)) |>
   kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

```
In this case there are only genre recorded.

We then check for data about species in the locality information for which there are no species data given in the species column. Here there are enough unique values to be checked easily within R. It is possible that in large data sets that there may be very many records for which there is useful information 

```{r select_local, eval=FALSE, error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
kable(head(Carex |>
  filter(species == "") |>
  select(locality) |>
  distinct())) |>
   kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)
```

There are no species data in the locality field and so we begin a vector of IDs that are not at the required data resolution.
```{r unuseful, warning=FALSE, error=FALSE, message=FALSE }
unUseful <- Carex |>
            filter(scientificName == "") |>
            select(id)

```

We now need to check that the species names in the records that we are using are valid for the area which we are looking at i.e. Sweden. There are several resources out there that do this. As we are looking specifically and solely at Sweden there is [Dyntaxa.se](https://www.dyntaxa.se/). There is also an R package as part of the [Swedish Biodiversity Data Infrastructure](https://github.com/biodiversitydata-se/SBDI4R) with examples and documentation. For wider applicability we will export the unique names from the Carex data frame. These will then be copied to the [Dyntaxa portal](https://www.dyntaxa.se/Match/Settings/0) for matching multiple names. The results are checked against the Swedish lists and unknown species or uncertainties are flagged. Names can be copied directly into a box in Dyntaxa or may be imported directly as an `xlsx` file. Here we shall export just the species names. There are multiple options. 

```{r filter_unusef, warning=FALSE, error=FALSE, message=FALSE, eval=FALSE}

SpeciesNames <- Carex |>
  filter(!id %in% unUseful) |>
  select("scientificName") |>
  distinct()

# library(openxlsx)
#write.xlsx(SpeciesNames,"./data/BDcleaner_Scripts/Example/UniqueCarexTaxa.xlsx")

```  

Dyntaxa provides options for any taxononmic uncertainties and lists the species for which no match can be made. Having made selections it exports as a `.xlslx` file. We can then check what those species for which there is no information about what they are. In this case there are an number of species which are likely to be horticultural, a few taxa which are of hybrid origin which are in the Dyntaxa with the addition of 'Ã—' between genus and species, and *Carex utriculata* a species from North America. This last could be a misidentification of *C. rostrata*. We will exclude the horticultural and uncertain species. We do this by adding a **Species** column in the Dyntaxa file and then load that file into R. 


```{r merge_names, warning=FALSE, error=FALSE, message=FALSE}
library(openxlsx)
SpeciesNames <- read.xlsx("./data/matchCarex.xlsx") # From Dyntaxa

Carex <- merge(Carex, 
               SpeciesNames[,c("Provided.string","SpeciesDyn")],
               by.x = "species",by.y = "Provided.string",
               all.x = TRUE)
```
We then add IDs of the rows with taxa which are to be excluded to the unUseful vector. It is possible that there is overlap between these vectors. Rather than overwriting the vector we join the vectors together and use unique to get a vector with no duplications.  

```{r unuseful2, warning=FALSE, error=FALSE, message=FALSE}
unUseful <- unique(
  c(unUseful, 
    Carex |>
      filter(is.na(SpeciesDyn)) |>
      select(id)
    )
  )
```

###Coordinate Cleaning

We need to extract locality data for rows where there is no lat/lon information to geocode it, producing a latitude and longitude for each locality. This can be carried out in a similar way as for species ie the merging of data frames. This can be also be done automatically in R using the geocode feature of the libraries tidygeocoder or ggmap. Some of these services require an API key (eg. Google). See the documentation for the link for more information  

```{r filter_local, warning=FALSE, error=FALSE, message=FALSE}
Localities <- Carex |>
              filter(is.na(longitude)) |>
              select(locality) |>
              distinct()
write.csv(Localities,"./data/Localities.csv",row.names = FALSE)
```

For brevity we will simply include the records with no lat/lon info in the unUseful vector. These will ultimately not be used.  

```{r unusefeul3, warning=FALSE, error=FALSE, message=FALSE}
unUseful <- unique(c(unUseful,
                     Carex |>
                       filter(is.na(latitude)) |>
                       select(id)
                     )
                   )
```

Before cleaning the coordinates we simplify the data retaining columns that have information directly related to the collection of the data.

```{r carex_select, warning=FALSE, error=FALSE, message=FALSE}
Carex <- Carex |>
          select(id, 
                 SpeciesDyn,
                 longitude,
                 latitude,
                 coordinateUncertaintyInMetres,
                 locality,
                 # recordedBy,
                 # countryCode,
                 # eventDate,
                 year,
                 month,
                 day,
                 institutionCode,
                 collectionCode,
                 catalogueNumber)
```


We then filter out the data rows for which we can not use ie those of too great taxonomic uncertainty. The data may then be cleaned. 

There are a number of things to consider when cleaning data:

  * How precise are the locations? Coordinate uncertainty ranges in the Cyperaceae of Sweden from 1 m - 30.5 km.
  * Are interpreted coordinates sufficient? These interpreted coordinates may imply a greater precision than is necessarily true
  * Are the locations likely to be errors? Errors can relate to where a sample is (eg. herbarium location) rather than where it came from.
  * Are the locations in the country claimed? This can arise through swapping of lat and lon; duplication of latitude numbers in the longitude; or simply incorrect coordinates being given.


```{r unuseful4}
unUseful <- unique( c(unUseful, 
                      Carex |>
                        filter(is.na(coordinateUncertaintyInMetres)) |>
                        select(id)
                      )
                    )
# kable( t(table( round( Carex$coordinateUncertaintyInMetres, -2))))

```
We shall filter out all records with unknown coordinate uncertainty and all with a coordinate uncertainty more than 12.5 km.


```{r unuseful5, warning=FALSE, error=FALSE, message=FALSE}

unUseful <- unique( 
  c(unUseful, 
    Carex |>
      filter(is.na(coordinateUncertaintyInMetres)) |>
      select(id)
    )
  )
unUseful <- unique( 
  c(unUseful, 
    Carex |>
      filter(coordinateUncertaintyInMetres  > 12500) |>
      select(id)
    )
  )

```

We now have an index of records we can't use owing to incomplete taxonomy or incomplete location information. We save these before cleaning the coordinates.

```{r save_unu, warning=FALSE, error=FALSE, message=FALSE}
save(unUseful, file = "./data/Cyperacea_SWE_unsuseful.rdata")
```

We first remove the already labeled inaccurate data. 

```{r carex_filter,warning=FALSE, error=FALSE, message=FALSE}
Carex <- Carex[-which(Carex$id %in% unUseful),]
```

We then use the library CoordinateCleaner to automatically flag coordinates that may be errors. Outputs of this can be a cleaned data.frame or additional columns with doubtful records flagged. 

```{r carex_kable, warning=FALSE, error=FALSE, message=FALSE, eval=FALSE}
Carex$countryCode <- countrycode::countrycode(Carex$countryCode,"iso2c","iso3c") #converts ISO2 country codes to ISO3

Carex <- Carex |>
          filter(!id %in% unUseful)

Carex <- Carex[-which(is.na(Carex$latitude)),]

Carex <- clean_coordinates(Carex,
                  lon = "longitude",
                  lat = "latitude",
                  species = "SpeciesDyn",
                  countries = "countryCode",
                  tests = c("capitals", "centroids", "equal", "gbif", "institutions",
                            "outliers", "seas", "zeros","countries")
                  )

kable( Carex |>
  summarise(`Invalid coords` = sum(.val==FALSE),
            `Equal coords` = sum(.equ==FALSE),
            `0 coords` = sum(.zer==FALSE),
            `capitals` = sum(.cap==FALSE),
            `country centre` = sum(.cen==FALSE),
            `Country Border` = sum(.con==FALSE),
            `outlier` = sum(.otl==FALSE),
            `Gbif HQ` = sum(.gbf==FALSE),
            `Insitution` = sum(.inst==FALSE),
            `Summary` = sum(.summary==FALSE)), 
  col.names = c("Invalid coords","Equal coords","0 coords","capitals", 
                "country centre","Country Border","outlier","Gbif HQ",
                "Insitution", "Summary"))
```

```{r carex_kable2, warning=FALSE, error=FALSE, message=FALSE, eval=FALSE, echo=FALSE}
kable(Carex |>
  group_by(SpeciesDyn) |>
  summarise(`Invalid coords` = sum(.val==FALSE),
            `Equal coords` = sum(.equ==FALSE),
            `0 coords` = sum(.zer==FALSE),
            `capitals` = sum(.cap==FALSE),
            `country centre` = sum(.cen==FALSE),
            `Country Border` = sum(.con==FALSE),
            `outlier` = sum(.otl==FALSE),
            `Gbif HQ` = sum(.gbf==FALSE),
            `Insitution` = sum(.inst==FALSE),
            `Summary` = sum(.summary==FALSE)))|>
    kable_styling() |>
  scroll_box(height = "300px")
```
Checking the distribution of the observations

```{r plot_map, warning=FALSE, error=FALSE, message=FALSE}

Sweden <- raster::getData("GADM", 
                  country = "SWE",
                  path = "./data/",
                  level = 0)
Sweden_sf <- st_as_sf(Sweden)

BolMar <- ggplot(Sweden_sf) +
  geom_sf() +
  geom_point(data = Carex |>
                filter(SpeciesDyn == "Carex capillaris"),# |>
                # filter(.con == FALSE),
             aes(x = longitude,
                 y = latitude)) +
  theme_cowplot() +
  ggtitle("Carex capillaris")

CarAct <- ggplot(Sweden_sf) +
  geom_sf() +
  geom_point(data = Carex |>
              filter(SpeciesDyn == "Carex acuta"), # |>
                 # filter(.otl == TRUE),
             aes(x = longitude,
                 y = latitude)) + #,
                 # colour = .otl))+
  theme_cowplot() +
  ggtitle("Carex acuta")

```



<!-- I think some form of leaflet plot would be great here so that it is interactive -->
```{r summ_birds, warning=FALSE, error=FALSE, message=FALSE, eval=FALSE}

grid_Sweden <- makeGrid(Sweden, 25)

OB <- organiseBirds(Carex, 
                    idCols = c("locality"),
                    xyCols = c("longitude", "latitude"),
                    sppCol = "SpeciesDyn")

SB <- summariseBirds(OB, grid_Sweden)
grid_Sweden <- st_as_sf(grid_Sweden)
Sweden_sf <- st_as_sf(Sweden)

```

```{r plot_ign, warning=FALSE, error=FALSE, message=FALSE}
library(colorRamps)
SB_SPat <- st_as_sf(SB$spatial)

nSpecies <- ggplot(data = SB_SPat,aes(fill = nSpp)) +
  geom_sf() +
  theme_cowplot() +
  scale_fill_gradientn(colors = matlab.like2(100))

nVisits <- ggplot(data = SB_SPat,aes(fill = nVis)) +
  geom_sf() +
  theme_cowplot() +
  scale_fill_gradientn(colors = matlab.like2(100))

nObs <- ggplot(data = SB_SPat,aes(fill = nObs)) +
  geom_sf() +
  theme_cowplot() +
  scale_fill_gradientn(colors = matlab.like2(100))

plot_grid(nSpecies, nVisits, nObs, ncol = 1)
```
