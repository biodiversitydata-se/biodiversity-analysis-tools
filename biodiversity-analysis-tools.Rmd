--- 
title: "<img src='images/sbdi-logo-orginal-large.png' class='cover'/> <br><br> A general workflow for analysis of primary biodiversity  data"
author: "Debora Arlt, Alejandro Ruete and Charles Campbell <br>for the Swedish Biodiversity Data Infrastructure"
date: "`r Sys.Date()`"
description: ""
output: bookdown::gitbook
site: bookdown::bookdown_site
documentclass: book
bibliography:
- references.bib
biblio-style: apalike
link-citations: yes
github-repo: https://github.com/biodiversitydata-se/biodiversity-analysis-tools
cover-image: images/sbdi-logo-orginal-large.png
---
```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth", 
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```


# Introduction


Each research question draws its own challenges which are unique in themselves. Our aim here is to provide a workflow which answers and / or prompts the large scale questions that should be asked at each stage of the process. We point to resources, methods and facilities that may be useful in answering a particular question. We assume some knowledge of statistical inference and it limitations. The validity and appropriateness of a particular method is dependent on the individual researcher(s). This workflow is focused on the statistical programming language R, as an environment where the complete analysis workflow can take place and can be documented in a fully reproducible way. However, we also point to other tools that can be used at different stages, and ways to import and export the data from and to those tools.

Biodiversity resources are increasingly international. We focus on biodiversity data and resources from Sweden but our aim is to present considerations and methods that can be applied beyond Sweden's borders.

General description of the workflow here. Data-->Cleaning-->Fitness evaluation-->Analysis always exploring and filtering the data in light of the research question. Also say here that if known from before, the cleaning and filtering criteria can be set directly on the query! 

`r htmltools::includeHTML("images/Workflow Overview Horizontal.html")`

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth", 
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```
# The importance of questions and sources of data

## Questions
Any question to be asked of biodiversity data should be put as simply and succinctly as possible. With the number of different subject areas and techniques used, analyses can quickly become complex.

## Taxonomies
It is important to be aware of likely taxonomic anomalies prior to working within a region. Check-lists are very important, especially if working over several regions / countries. Whilst there are many things that will automatically look for the validity of a name they do not check for the validity of that species occurrence. For example *Sphagnum auriculatum* and *S. denticulatum* are both valid names. *S. auriculatum* is the currently accepted species in Europe but in the British Isles, Ireland and the Netherlands *s. denticulatum* is the most recorded taxa. Using data from across the European region without acknowledging this disagreement would impact the results of any research undertaken. For taxa which are known to be capable of dispersing great distances (eg birds) this becomes even more difficult especially when using community sourced data.

For Sweden there is an agreed taxonomy for species accessible through [dyntaxa](https://www.dyntaxa.se/) and the R library **dyntaxa**.


## Data Sources
Depending on what questions are being asked there are many different resources available. We focus on biodiversity data 

### Biodiversity record data
There are a large number of available on-line resources. These include but are not limited to (specific R libraries that connect to these databases are provided in **bold**):

- [Swedish Biodiversity Data Infrastructure](https://www.artportalen.se/) - Sweden's data portal for biodiversity data 
- [Global Biodiversity Information Facility](http://www.gbif.org/) - International organization aggregating biodiversity data. Contains data from a mixture of sources; curated collections, community science data, ecological research projects etc. **rgbif, spocc** 
- [BioCASE](https://www.biocase.org/) - A European transnational biodiversity repository
- [eBird](http://ebird.org/content/ebird/) - American database of bird observations **auk, rebird,spocc**
- [iNaturalist](http://www.inaturalist.org/) - International community science observation repository **spocc**
- [Berkeley ecoengine](https://ecoengine.berkeley.edu) - Access to UC Berkley's Natural history data **spocc**
- [VertNet](http://vertnet.org/) - vertebrate biodiversity collections **rvert, spocc**
- [iDigBio](https://www.idigbio.org/) - Integrated digitise biodiversity collections **ridigbio**
- [OBIS](http://www.iobis.org/) - Ocean biodiversity information system **robis**
- [ALA](http://www.ala.org.au/) - Atlas of living Australia **ALA4r**
- [Neotoma](https://www.neotomadb.org/) Palaeoecology databas **neotoma**
- ...

### Taxonomic diversity
To keep track of ever changing taxonomy of species there are different databases that follow different standars.

- [dyntaxa](https://www.dyntaxa.se/)
- [GBIF backbone](http://www.gbif.org/)
- ...

### Functional diversity
Very broadly functional diversity is the diversity of what organisms do [@petchey_functional_2006]. Such diversity can be direct physical measurements of traits of the organisms involved and / or data summarized from published works. There are databases dedicated to the distribution of scientific data that may be used. Such resources include:

- [Dryad](https://datadryad.org/stash)
- [TRY: Plant Trait Database](https://www.try-db.org/TryWeb/Home.php)
- Ecological indicator and traits values for Swedish vascular plants[@tyler_ecological_2021] [https://doi.org/10.1016/j.ecolind.2020.106923]
- [Vertnet](http://vertnet.org/)
- ...

### Genetic data bases

Genetic data may be related directly to the samples used, phylogenetic trees generated from some other data set set or some other genetic aspect. Such resources include:  

- [BOLD: Barcode Of Life Data system](https://www.boldsystems.org/) Repository of gene sequences
- [Genbank](https://www.ncbi.nlm.nih.gov/genbank/) A genetic sequence repository 
- [Treebase](https://www.treebase.org/treebase-web/home.html) A data base of phylogenetic trees
- ...


### Environemntal data

There are a number of environmental repositories available. Static data sets for global resources include:

#### Sweden

- [Skogsstyrelsen](https://www.skogsstyrelsen.se/sjalvservice/karttjanster/geodatatjanster/oppna-data/) - Open data supplied by Skogsstyrelsen
- [Sveriges dataportal](https://www.dataportal.se/) - Swedish open data provided by governmental bodies.
- [Milj√∂data MVM](https://miljodata.slu.se/mvm/) - Environmental data from SLU for Sweden

#### Global and European 
- [wordclim](https://www.worldclim.org/) - Global gridded climate data at various resolutions
- [climond](https://www.climond.org/) - Global gridded climate data at various resolutions
- [soilgrids](https://soilgrids.org/) - Global gridded soil data
- [Copernicus Land monitoring service](https://land.copernicus.eu/pan-european/corine-land-cover) - Land monitoring data from across the EU

<!--chapter:end:01-research-question.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth", 
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```
# Data cleaning

Biodiversity data repositories work hard to maintain the accuracy of their holdings. When multiple sources are involved several problems may arise. Here we shall quickly outline what they are and possible solutions.

## Resources
There are a number of libraries, work flows and online resources for automating downloading and cleaning of data. THese include:

  - The [BDverse](https://bdverse.org/). A group of libraries for cleaning biodiversity data
  - [Kurator]([http://kurator.acis.ufl.edu/kurator-web/)
  - [Wallace](https://wallaceecomod.github.io/)
  

## Taxonomies

It is important to be aware of likely taxonomic anomalies prior to working within a region. Checklists are very important, especially if working over several regions / countries. Whilst there are many things that will automatically look for the validity of a name they do not check for the validity of that species occurrence. For example *Sphagnum auriculatum* Schimp. and *Sphagnum denticulatum* Bridel, 1826 are both valid names. *S. auriculatum* is the currently accepted species in Europe but in the British Isles, Ireland and the Netherlands *s. denticulatum* is the most recorded taxa. Both  are legitimate names but they are a synonymy. Both names have been used in Europe but in distinct countries. The current European checklist [@hodgetts_annotated_2020] has *Sphagnum auriculatum* Schimp. as the accepted taxon occurring in Europe. Naive downloading would result in two taxa being present when in fact it is two interpretations of the same taxon. Using data from across the European region without acknowledging this disagreement would impact the results of any research undertaken. For taxa which are known to be capable of dispersing great distances (eg. birds) this becomes even more difficult especially when using community sourced data.

```{r start-up, warning=FALSE, echo=FALSE, message=FALSE}
library(data.table)
library(sf)
library(ggplot2)
library(countrycode)
library(rnaturalearth)
library(cowplot)
library(dplyr)
library(kableExtra)
theme_set(theme_cowplot())
library(rnaturalearthdata)
library(rgbif)

# Data for Sphagnum denticulatum doi: 10.15468/dl.rrp4p4
# Data for Sphagnum auriculatum doi: 10.15468/dl.3yrtw7

df <- lapply(list.files("./data/Downloads/",
                        full.names = TRUE,
                        pattern = ".csv"),
             function(q){
               fread(q,encoding = "UTF-8")
             })
df <- do.call("rbind",df)
EUR <- coastline110
EUR <- st_as_sf(EUR)

ggplot(data = EUR)+
  geom_sf()+
  xlim(-20,30)+
  ylim(45,75)+
  geom_point(data = df, 
             aes(x = decimalLongitude,
                 y = decimalLatitude,
                 colour = species),
             alpha = 0.5) +
  theme(legend.position = "bottom",
        axis.title = element_blank()) +
  scale_color_discrete(name = "Species")

```
The example above uses data downloaded from GBIF. 

- Data for *Sphagnum denticulatum* doi: 10.15468/dl.rrp4p4
- Data for *Sphagnum auriculatum* doi: 10.15468/dl.3yrtw7

Within Sweden there is an agreed taxonomy for all extant taxa accessible through [Dyntaxa](https://www.dyntaxa.se). Checking of species lists can be done by directly copying species names into a dialogue box or uploading an excel spreadsheet in the correct format.


## Location data 

### Locality information

Many records have locality information attached to them. Where there are no coordinates attached this information can be used to locate the record to within an area of where records most likely came from. There are functions for which geocoding can be done automatically within R. Geocoding is finding the coordinate for a known locality. These are included in the libraries:

- **ggmap** *requires google API key*
- **tidygeocoder**

Many localities may not be included in the gazetteers associated with these libraries but may be located using online or printed maps. This takes time but may be useful. Unique localities may be extracted, geocoded and then merged back with the data set.

```{r, eval = FALSE, echo=FALSE}
## this example is not relevant as the data is not in the repository and doesn't shown how to geolocate
localities <- df %>%
  filter(is.na(decimalLatitude))%>%
  dplyr::select(locality)%>%
  distinct()

write.csv(localities,"DataFrameOfLocalities.csv",row.names = FALSE)

# Add columns to each row of the exported localities with any of the locations that may be found

localities <- read.csv("DataFrameOfLocalitiesGeodcoded.csv")

recordsDataFrame <- merge(recordsDataFrame, locallities, by = "localities", all = TRUE)

```

This becomes especially important when extracting records across country boundaries as countries have different numbers of georeferenced observations.
```{r echo = FALSE}
EU <- c('AL', 'AD', 'AM', 'AT', 'BY', 'BE', 'BA', 'BG', 'CH', 'CY', 'CZ', 'DE',
        'DK', 'EE', 'ES', 'FO', 'FI', 'FR', 'GB', 'GE', 'GI', 'GR', 'HU', 'HR',
        'IE', 'IS', 'IT', 'LI', 'LT', 'LU', 'LV', 'MC', 'MK', 'MT', 'NO', 'NL', 'PL',
        'PT', 'RO', 'RU', 'SE', 'SI', 'SK', 'SM', 'TR', 'UA', 'VA')

Nrecs <- data.frame(
  country = countrycode(EU,"iso2c","country.name.en"),
  TR = sapply(EU,function(q){occ_count(taxonKey = 35, country = q)}),
  Gr = sapply(EU,function(q){occ_count(taxonKey = 35, country = q, georeferenced = TRUE)}),
  Nc = sapply(EU,function(q){occ_count(taxonKey = 35, country = q, georeferenced = FALSE)})
)

Nrecs <- Nrecs %>%
  mutate(GrP = round((Gr/TR)*100,2),
         NcP = round((Nc/TR)*100,2))%>%
  arrange(country)


kable(Nrecs %>%
  dplyr::select(country,TR,GrP,NcP),
  col.names = c("Country", "Total Records",  "% Records with coordinate","% Records without coordinates"),
  caption = "The number and percentage of records of bryophyta per EU country with and without coordinates") %>%
  scroll_box(height = "350px",width = "100%") %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = TRUE)
```
As can be seen from the above example for European Bryophyta (Hornworts, Liverworts and Mosses) the number of records with coordinates vastly varies between countries. For example of 959444 records in Sweden 88.24 % have coordinates in Switzerland of 117132 records only 2.29 % have coordinates.

### Coordinate uncertainty
In many cases there is now an abundance of biodiversity data with coordinates. As can be seen from a summary of the above *Sphagnum* data coordinate uncertainty can vary from less than 1 meter to multiple kilometers. 

```{r, echo=FALSE}
dat <- rbind(as.data.frame(table(cut(df$coordinateUncertaintyInMeters,breaks = c(0,1,10,100,500,1000,5000,10000,50000,max(df$coordinateUncertaintyInMeters,na.rm = TRUE))))),
                   data.frame(Var1 = "NA", Freq = sum(is.na(df$coordinateUncertaintyInMeters))))

  dat$Var1<-c("<1 m",
"1-10 m",
"10-100 m",
"100-500 m",
"500-1000 m",
"1-5 km",
"5-10 km",
"10-50 km",
">50 km",
  "None")
names(dat) <- c("Coordinate uncertainty", "Frequency")

kable(t(dat)) %>%
  kable_styling() %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = TRUE)
```

In the above example it can be seen that two have uncertainties greater than 50 km and several thousand records that have no known error margin the location. It is important to consider what the error is and removing those records for which the uncertainty is too high. Where this point is will be dependent on the scale of the research.

<!-- Coordinates with NA as the uncertainty and those that fall below a particular threshold may be removed by: -->

```{r, eval=FALSE, echo=FALSE}
df <- df %>%
  filter(is.na(coordinateUncertaintyInMeters) == FALSE) %>%
  filter(coordinateUncertaintyInMeters < 50000)
```

### Coordinate errors

Besides the declared uncertainty of the the coordinates, coordinate errors may occur for a variety of reasons. The library **coordinateCleaner** is very useful for removing some of the most common errors. These include:

- 0 latitude, 0 longitude
- swapping of latitude and longitude
- The location of the institution holding a preserved sample rather than the location of origin of that sample

## Examples
### Data cleaning Cyperaceae in Sweden

In this example I have downloaded the occurrences of the Cyperaceae family in Sweden from SBDI since 2000. This is a single data set from an area with an agreed taxonomy [dyntaxa](www.dyntaxa.se). Our aim here is to download a group of records and clean them to be in such a state than they reflect the question the researcher has in mind.

First we give each record in the assembled data set a unique ID. In this case it could be possible to use the GBIFid. In data sets sourced from multiple sources it is a good idea to create a unique ID related to the data source.

```{r hiden-data-search, warning=FALSE, error=FALSE, message=FALSE, eval=FALSE, echo=FALSE}
library(BIRDS)
library(SBDI4R)

## TODO optimize the search so that the dataset is smaller than 5mb
Carex <- occurrences(taxon="Carex",
                     fq="year:[2000 TO 2020]",
                     email="test@test.org", 
                     download_reason_id=10)

#it creates month data that is currently lacking from the SBDI API
Carex$data$day <- ifelse(is.na(Carex$data$day), 
                       sample(c(1:31), 1, replace = TRUE),
                       Carex$data$day)
Carex$data$month <- ifelse(Carex$data$day == 31, 
                       sample(c(1,3,5,7,8,10,12), 1, replace = TRUE),
                       ifelse(Carex$data$day>=28, 
                              sample(c(1,3:12), 1, replace = TRUE), #not February
                              sample(c(1:12), 1, replace = TRUE)
                              )
                       )
Carex <- Carex$data
save(Carex, file = "./data/Cyperacea_SWE.rdata")
```

```{r loadCarex, error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
load("./data/Cyperacea_SWE.rdata") 

```

In cleaning data there are a number of dimensions of uncertainty we shall first check.

### Taxonomy
In the species column there are records with no Species given in the species column. We then check the `scientificName` for whether there is useful information there
```{r , warning=FALSE, error=FALSE, message=FALSE}
kable(Carex %>%
  filter(species == "") %>%
  distinct(scientificName)) %>%
   kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

```
In this case there are only genre recorded.

We then check for data about species in the locality information for which there are no species data given in the species column. Here there are enough unique values to be checked easily within R. It is possible that in large data sets that there may be very many records for which there is useful information 

```{r, eval=FALSE, error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
kable(head(Carex %>%
  filter(species == "") %>%
  dplyr::select(locality) %>%
  distinct())) %>%
   kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)
```

There are no species data in the locality field and so we begin a vector of IDs that are not at the required data resolution.
```{r, warning=FALSE, error=FALSE, message=FALSE }
unUseful <- Carex %>%
            filter(Carex$scientificName == "") %>%
            .$id

```

We now need to check that the species names in the records that we are using are valid for the area which we are looking at i.e. Sweden. There are several resources out there that do this. As we are looking specifically and solely at Sweden there is [Dyntaxa.se](https://www.dyntaxa.se/). There is also an R package as part of the [Swedish Biodiversity Data Infrastructure](https://github.com/biodiversitydata-se/SBDI4R) with examples and documentation. For wider applicability we will export the unique names from the Carex data frame. These will then be copied to the [Dyntaxa portal](https://www.dyntaxa.se/Match/Settings/0) for matching multiple names. The results are checked against the Swedish lists and unknown species or uncertainties are flagged. Names can be copied directly into a box in Dyntaxa or may be imported directly as an `xlsx` file. Here we shall export just the species names. There are multiple options. 

```{r, warning=FALSE, error=FALSE, message=FALSE, eval=FALSE}

SpeciesNames <- Carex %>%
  filter(!id %in% unUseful) %>%
  dplyr::select("scientificName") %>%
  distinct()

# library(openxlsx)
#write.xlsx(SpeciesNames,"./data/BDcleaner_Scripts/Example/UniqueCarexTaxa.xlsx")

```  

Dyntaxa provides options for any taxononmic uncertainties and lists the species for which no match can be made. Having made selections it exports as a `.xlslx` file. We can then check what those species for which there is no information about what they are. In this case there are an number of species which are likely to be horticultural, a few taxa which are of hybrid origin which are in the Dyntaxa with the addition of '√ó' between genus and species, and *Carex utriculata* a species from North America. This last could be a misidentification of *C. rostrata*. We will exclude the horticultural and uncertain species. We do this by adding a **Species** column in the Dyntaxa file and then load that file into R. 


```{r, warning=FALSE, error=FALSE, message=FALSE}
library(openxlsx)
SpeciesNames <- read.xlsx("./data/matchCarex.xlsx") # From Dyntaxa

Carex <- merge(Carex, 
               SpeciesNames[,c("Provided.string","SpeciesDyn")],
               by.x = "species",by.y = "Provided.string",
               all.x = TRUE)
```
We then add IDs of the rows with taxa which are to be excluded to the unUseful vector. It is possible that there is overlap between these vectors. Rather than overwriting the vector we join the vectors together and use unique to get a vector with no duplications.  

```{r, warning=FALSE, error=FALSE, message=FALSE}
unUseful <- unique( 
  c(unUseful,
  Carex %>%
  filter(is.na(SpeciesDyn)) %>%
  .$id))
```

###Coordinate Cleaning

We need to extract locality data for rows where there is no lat/lon information to geocode it, producing a latitude and longitude for each locality. This can be carried out in a similar way as for species ie the merging of data frames. This can be also be done automatically in R using the geocode feature of the libraries tidygeocoder or ggmap. Some of these services require an API key (eg. Google). See the documentation for the link for more information  

```{r, warning=FALSE, error=FALSE, message=FALSE}
Localities <- Carex %>%
              filter(is.na(longitude)) %>%
              dplyr::select(locality) %>%
              distinct()
write.csv(Localities,"./data/Localities.csv",row.names = FALSE)
```

For brevity we will simply include the records with no lat/lon info in the unUseful vector. These will ultimately not be used.  

```{r, warning=FALSE, error=FALSE, message=FALSE}
unUseful <- unique(c(unUseful,
                     Carex %>%
                       filter(is.na(latitude))%>%
                       .$id))
```

Before cleaning the coordinates we simplify the data retaining columns that have information directly related to the collection of the data.

```{r, warning=FALSE, error=FALSE, message=FALSE}
Carex <- Carex %>%
          dplyr::select(SpeciesDyn,
               longitude,
               latitude,
               coordinateUncertaintyInMetres,
               locality,
               # recordedBy,
               # countryCode,
               # eventDate,
               year,
               month,
               day,
               institutionCode,
               collectionCode,
               catalogueNumber)
```


We then filter out the data rows for which we can not use ie those of too great taxonomic uncertainty. The data may then be cleaned. 

There are a number of things to consider when cleaning data:

  * How precise are the locations? Coordinate uncertainty ranges in the Cyperaceae of Sweden from 1 m - 30.5 km.
  * Are interpreted coordinates sufficient? These interpreted coordinates may imply a greater precision than is necessarily true
  * Are the locations likely to be errors? Errors can relate to where a sample is (eg. herbarium location) rather than where it came from.
  * Are the locations in the country claimed? This can arise through swapping of lat and lon; duplication of latitude numbers in the longitude; or simply incorrect coordinates being given.


```{r}
unUseful <- unique( c(unUseful, Carex %>%
                        filter(is.na(coordinateUncertaintyInMetres)) %>%
                        .$id))
# kable( t(table( round( Carex$coordinateUncertaintyInMetres, -2))))

```
We shall filter out all records with unknown coordinate uncertainty and all with a coordinate uncertainty more than 12.5 km.


```{r, warning=FALSE, error=FALSE, message=FALSE}

unUseful <- unique( c(unUseful, Carex %>%
  filter(is.na(coordinateUncertaintyInMetres))%>%
  .$id))
unUseful <- unique( c(unUseful, Carex %>%
  filter(coordinateUncertaintyInMetres  > 12500)%>%
  .$id))

```

We now have an index of records we can't use owing to incomplete taxonomy or incomplete location information. We save these before cleaning the coordinates.

```{r, warning=FALSE, error=FALSE, message=FALSE}
save(unUseful, file = "./data/Cyperacea_SWE_unsuseful.rdata")
```

We first remove the already labeled inaccurate data. 

```{r,warning=FALSE, error=FALSE, message=FALSE}
Carex <- Carex[-which(Carex$id %in% unUseful),]
```

We then use the library CoordinateCleaner to automatically flag coordinates that may be errors. Outputs of this can be a cleaned data.frame or additional columns with doubtful records flagged. 

```{r,warning=FALSE, error=FALSE, message=FALSE, eval=FALSE}
Carex$countryCode <- countrycode::countrycode(Carex$countryCode,"iso2c","iso3c") #converts ISO2 country codes to ISO3

Carex <- Carex %>%
          filter(!id %in% unUseful)

Carex <- Carex[-which(is.na(Carex$latitude)),]

Carex <- clean_coordinates(Carex,
                  lon = "longitude",
                  lat = "latitude",
                  species = "SpeciesDyn",
                  countries = "countryCode",
                  tests = c("capitals", "centroids", "equal", "gbif", "institutions",
                            "outliers", "seas", "zeros","countries")
                  )

kable( Carex %>%
  summarise(`Invalid coords` = sum(.val==FALSE),
            `Equal coords` = sum(.equ==FALSE),
            `0 coords` = sum(.zer==FALSE),
            `capitals` = sum(.cap==FALSE),
            `country centre` = sum(.cen==FALSE),
            `Country Border` = sum(.con==FALSE),
            `outlier` = sum(.otl==FALSE),
            `Gbif HQ` = sum(.gbf==FALSE),
            `Insitution` = sum(.inst==FALSE),
            `Summary` = sum(.summary==FALSE)), 
  col.names = c("Invalid coords","Equal coords","0 coords","capitals", 
                "country centre","Country Border","outlier","Gbif HQ",
                "Insitution", "Summary"))
```

```{r, warning=FALSE, error=FALSE, message=FALSE, eval=FALSE, echo=FALSE}
kable(Carex %>%
  group_by(SpeciesDyn) %>%
  summarise(`Invalid coords` = sum(.val==FALSE),
            `Equal coords` = sum(.equ==FALSE),
            `0 coords` = sum(.zer==FALSE),
            `capitals` = sum(.cap==FALSE),
            `country centre` = sum(.cen==FALSE),
            `Country Border` = sum(.con==FALSE),
            `outlier` = sum(.otl==FALSE),
            `Gbif HQ` = sum(.gbf==FALSE),
            `Insitution` = sum(.inst==FALSE),
            `Summary` = sum(.summary==FALSE)))%>%
    kable_styling() %>%
  scroll_box(height = "300px")
```
Checking the distribution of the observations

```{r, warning=FALSE, error=FALSE, message=FALSE}

Sweden <- raster::getData("GADM", 
                  country = "SWE",
                  path = "./data/",
                  level = 0)
Sweden_sf <- st_as_sf(Sweden)

BolMar <- ggplot(Sweden_sf) +
  geom_sf()+
  geom_point(data = Carex %>%
                filter(SpeciesDyn == "Carex capillaris"),# %>%
                # filter(.con == FALSE),
             aes(x = longitude,
                 y = latitude)) +
  theme_cowplot() +
  ggtitle("Carex capillaris")

CarAct <- ggplot(Sweden_sf) +
  geom_sf()+
  geom_point(data = Carex %>%
              filter(SpeciesDyn == "Carex acuta"), # %>%
                 # filter(.otl == TRUE),
             aes(x = longitude,
                 y = latitude)) + #,
                 # colour = .otl))+
  theme_cowplot() +
  ggtitle("Carex acuta")

```



<!-- I think some form of leaflet plot would be great here so that it is interactive -->
```{r, warning=FALSE, error=FALSE, message=FALSE, eval=FALSE}

grid_Sweden <- makeGrid(Sweden, 25)

OB <- organiseBirds(Carex, 
                    idCols = c("locality"),
                    xyCols = c("longitude", "latitude"),
                    sppCol = "SpeciesDyn")

SB <- summariseBirds(OB, grid_Sweden)
grid_Sweden <- st_as_sf(grid_Sweden)
Sweden_sf <- st_as_sf(Sweden)

```

```{r, warning=FALSE, error=FALSE, message=FALSE}
library(colorRamps)
SB_SPat <- st_as_sf(SB$spatial)

nSpecies <- ggplot(data = SB_SPat,aes(fill = nSpp))+
  geom_sf()+
  theme_cowplot()+
  scale_fill_gradientn(colors = matlab.like2(100))

nVisits <- ggplot(data = SB_SPat,aes(fill = nVis))+
  geom_sf()+
  theme_cowplot()+
  scale_fill_gradientn(colors = matlab.like2(100))

nObs <- ggplot(data = SB_SPat,aes(fill = nObs))+
  geom_sf()+
  theme_cowplot()+
  scale_fill_gradientn(colors = matlab.like2(100))

plot_grid(nSpecies, nVisits, nObs, ncol = 1)
```

<!-- ## Example using <i>Sphagnum auriculatum</i> -->

<!-- In this example of running through the work flow I have downloaded data from GBIF on the species <i>S. auriculatum</i> and <i>S. denticulatum</i> in Europe. These are currently listed as separate species in the GBIF backbone but the European checklist and dyntaxa.se list them (correctly) as both being synonyms of <i>S. aurculatu</i>. This data set gives and opportunity to run through a simplified version of the BDcleanR scripts with data that are known to be incorrect at least in one dimension.  -->
<!-- ```{r setup1, include=FALSE} -->
<!-- ############################################ -->
<!-- ## Data 1. Botanical Garden Information -->
<!-- ## Methods: Automatic data collection -->
<!-- ## Important output: garden.inf data.frame -->
<!-- ############################################ -->
<!-- library(RCurl) -->
<!-- library(httr) -->
<!-- library(XML) -->
<!-- library(stringr) -->

<!-- # Automatic collection of botanical garden information in BGCI garden search -->
<!-- # Read Garden name and correspongding websites -->
<!-- url.list    <- 'https://tools.bgci.org/garden_search.php?action=Find&ftrCountry=All&ftrKeyword=&x=50&y=21' -->
<!-- html.use    <- GET(url.list) -->
<!-- doc1        <- htmlParse(html.use) -->
<!-- garden.inf  <- readHTMLTable(doc1)[[3]] -->
<!-- garden.link <- getHTMLLinks(doc1) -->
<!-- link.no     <- which(str_detect(garden.link,'garden.php?')) -->
<!-- link.use    <- paste0('http://www.bgci.org/',garden.link[link.no]) -->

<!-- # Read detail information of each botanical garden -->
<!-- path.list <- list() -->
<!-- for(i in 1:length(link.use)){ -->

<!--   url1        <- link.use[i] -->
<!--   doc         <- htmlParse(GET(url1)) -->

<!--   xpath1      <- xpathSApply(doc = doc,path = "//div[@class='meta-heading']//li",xmlValue) -->
<!--   save.no1    <- which(str_detect(xpath1,'Latitude')) # save latitude -->
<!--   save.no2    <- which(str_detect(xpath1,'Longitude')) # save longitude -->
<!--   save.no3    <- which(str_detect(xpath1,'Altitude')) # save altitude -->
<!--   save.no     <- unique(c(save.no1,save.no2,save.no3)) -->
<!--   save.chr    <- xpath1[save.no] -->

<!--   path.list[[i]] <- save.chr # save results in list -->
<!--   print(i) -->
<!-- } -->

<!-- # String processing -->
<!-- use.list <- list() -->
<!-- for(i in 1:length(link.use)) -->
<!-- { -->
<!--   trans1 <- str_trim(path.list[[i]]) -->
<!--   if(length(trans1) == 0) -->
<!--   { -->
<!--     use.list[[i]] <- NULL -->
<!--     next -->
<!--   } -->
<!--   trans2        <- str_split(trans1,pattern = '\t')[[1]] -->
<!--   use.vec       <- trans2[which(trans2 != "")] -->
<!--   use.list[[i]] <- use.vec -->
<!-- } -->


<!-- # Transform information format to data.frame -->
<!-- garden.inf$Latitude <- NA -->
<!-- garden.inf$Longitude <- NA -->

<!-- for(i in 1:length(link.use)) -->
<!-- { -->
<!--   if(is.null(use.list[[i]])) next -->
<!--   t1 <- str_split(use.list[[i]],':') -->
<!--   t2 <- lapply(t1, str_trim) -->

<!--   for(k in 1:length(t2)) -->
<!--   { -->
<!--     if(t2[[k]][1] != "Latitude" & t2[[k]][1] != "Longitude") -->
<!--     { -->
<!--       next -->
<!--     } else -->
<!--     { -->
<!--       if(t2[[k]][1] == "Latitude") -->
<!--       { -->
<!--         garden.inf$Latitude[i]  <- t2[[k]][2] -->
<!--       } else -->
<!--       { -->
<!--         garden.inf$Longitude[i] <- t2[[k]][2] -->
<!--       } -->
<!--     } -->
<!--   } -->

<!-- } -->

<!-- # write the BGCI garden information -->
<!-- write.csv(garden.inf,'./data/BDcleaner_Scripts/Example/GardenInformation.csv',row.names = F) -->

<!-- ``` -->



<!-- ```{r setup2, include=FALSE, echo=TRUE} -->
<!-- library(data.table) -->
<!-- library(taxize) -->
<!-- library(dplyr) -->
<!-- df.bio <- lapply(list.files("./data/BDcleaner_Scripts/Example/Downloads/", -->
<!--                         full.names = TRUE, -->
<!--                         pattern = ".csv"), -->
<!--              function(q){ -->
<!--                fread(q,encoding = "UTF-8") -->
<!--              }) -->

<!-- df.bio <- as.data.table(rbindlist(df.bio)) -->

<!-- df.bio <- df.bio %>% -->
<!--   unique() -->



<!-- df.bio <- subset(df.bio,select = c('verbatimScientificName','countryCode', -->
<!--                                     'decimalLatitude','decimalLongitude','year')) -->

<!-- df.bio$DataID <- 1:nrow(df.bio) -->


<!-- nam <- data.frame(fullnamewithauthors= unique(df.bio$verbatimScientificName)) -->

<!-- #write.table( nam, -->
<!-- #          "./BDcleaner_Scripts/Example/Agreed_taxonomy.csv", -->
<!-- #          row.names = FALSE, -->
<!-- #          sep = "\t") -->
<!-- nam <- read.csv("./data/BDcleaner_Scripts/Example/Agreed_taxonomy.csv") -->

<!-- df.bio <- merge(df.bio,  -->
<!--       nam, -->
<!--       by.x = "verbatimScientificName", -->
<!--       by.y = "fullnamewithauthors") -->

<!-- head(df.bio) -->
<!-- ``` -->



<!-- ```{r setup3, include=FALSE, echo=TRUE} -->
<!-- ########################################################################### -->
<!-- ## Part 2. Geographical cleaning -->
<!-- ## Dimension: Space -->
<!-- ## Important output variables -->
<!-- ##  (1)  ExGeo: the existence of geo-coordinates -->
<!-- ##  (2)  GeoPrecision: the number of decimal digits of coordinates -->
<!-- ##  (3)  TaxonType: match type of taxon name (with TPL) -->
<!-- ##  (4)  TaxonStatus: name status of taxon names (TPL) -->
<!-- ##  (5)  TxonComf: confidence level of taxon names (TPL) -->
<!-- ##  (6)  Adj.Coord: whether the error coordinate can be corrected -->
<!-- ##  (7)  Adj.TransTypr: type of coordinate transformation (correct) -->
<!-- ##  (8)  CountrySame: whether the coordinates match the political boundary? -->
<!-- ##  (9)  Overland: whether the coordinated located in terrsital area -->
<!-- ##  (10) GeoNotZero: Lat/Lon == 0? -->
<!-- ##  (11) FlagCent: centroids of administrative area -->
<!-- ##  (12) FlagIns: location of biodiversity institution -->
<!-- ##  (13) FlagBGCI: location of botanical garden -->
<!-- ##  (14) over.urban: whether located in urban area -->
<!-- ##  (15) overUrbanType: urban cluster of urban core or rural area -->
<!-- ############################################################################ -->

<!-- # Add new variable to df.bio dataframe -->
<!-- add.var <- c("ExGeo","GeoPrecision") -->
<!-- df.bio[,add.var] <- rep(NA,nrow(df.bio)) -->

<!-- ## ExGeo: existence of geo-coordinates -->
<!-- judge.geo         <- is.na(df.bio$decimalLongitude)+is.na(df.bio$decimalLatitude) # True: 0; False: 1 -->
<!-- no.geo            <- which(judge.geo==0) -->
<!-- le.geo            <- length(no.geo) -->
<!-- df.bio$ExGeo[no.geo]  <- 1 -->
<!-- df.bio$ExGeo[-no.geo] <- 0 -->

<!-- # GeoPrecision Use: number of decimal digits of coordinates -->
<!-- clat <- df.bio$decimalLatitude[no.geo] %>% as.character %>% str_split(pattern = '[.]') -->
<!-- clon <- df.bio$decimalLongitude[no.geo] %>% as.character %>% str_split(pattern = '[.]') -->

<!-- geop.use <- rep(0,length = length(no.geo)) -->
<!-- for(p in 1:length(no.geo)) -->
<!-- { -->
<!--   if(is.na(clat[[p]][2]) | is.na(clon[[p]][2])) -->
<!--   {next} else -->
<!--   { -->
<!--     geop.use[p] <- max(str_length(clat[[p]][2]),str_length(clon[[p]][2])) -->
<!--   } -->
<!-- } -->
<!-- df.bio$GeoPrecision[no.geo] <- geop.use -->

<!-- # remove useless variables -->
<!-- rm(list = setdiff(ls(),c('df.bio','add.var'))) -->
<!-- ``` -->




<!-- ```{r setup4, include=FALSE, echo=TRUE} -->
<!-- ################################## -->
<!-- ## Geographical error detection -->
<!-- #################################### -->
<!-- library(sp) -->
<!-- library(rgdal) -->
<!-- library(rworldxtra) -->
<!-- library(countrycode) -->
<!-- library(rworldmap) -->
<!-- library(biogeo) -->
<!-- library(maptools) -->
<!-- library(geonames) -->
<!-- library(data.table) -->
<!-- library(maptools) -->

<!-- # Read world map data -->

<!-- data(wrld_simpl) # world map in maptools R package (coarse) -->
<!-- worldmap2 <- readOGR('YourPath/worldmap2.shp') # Natural earth wold map (medium) -->
<!-- worldmap  <- readOGR('YourPath/gadm28_adm0.shp') # GADM world map (fine) -->

<!-- # remove useless variables -->
<!-- rm(list = setdiff(ls(),c('df.bio','add.var','taxonbind', -->
<!--                          'judge.taxon','flag.next','worldmap','worldmap2'))) -->

<!-- # Add filter variable names -->
<!-- add.name <- c('GeoNotZero','OverLand','CountrySame','Adj.Overland','Adj.Type', -->
<!--               'Adj.Correct','AdjLat','AdjLon') -->
<!-- source("./data/BDcleaner_Scripts/Script 4 Functions.R") -->
<!-- ############################ -->
<!-- # GeoNotZero: lat/lon = 0? -->
<!-- ############################ -->
<!-- GeoNotZero  <- Coord.Notzero(df.bio$decimalLatitude,df.bio$decimalLongitude) -->
<!-- df.bio$GeoNotZero <- GeoNotZero -->
<!-- rm(GeoNotZero) -->

<!-- country.inf <- wrld_simpl@data -->
<!-- country.inf$ISO2 <- as.character(country.inf$ISO2) -->

<!-- # Some special lower case -->
<!-- df.bio$countryCode <- toupper(df.bio$countryCode %>% as.character) -->

<!-- # "" not NA if countryCode is NA -->
<!-- ``` -->

<!-- ```{r setup5, include=FALSE, eval=FALSE} -->
<!-- ################################### -->
<!-- ## GeoFilter 2: On the Land? -->
<!-- ## GeoFilter 3: Country boundary -->
<!-- ## We used three world maps -->
<!-- ################################### -->
<!-- Line.use <- which(df.bio$ExGeo == 1) -->
<!-- df.use   <- df.bio[Line.use,] -->
<!-- L.line   <- length(Line.use) -->

<!-- pro.shp <- SpatialPointsDataFrame(df.use[,c('decimalLongitude','decimalLatitude')], -->
<!--                                   data = df.use[,c("scientificNamDe",'FlagCountry','countryCode')], -->
<!--                                   proj4string = CRS("+proj=longlat +datum=WGS84")) -->
<!-- rm(df.use) -->

<!-- overlay.land <- rep(0,L.line) -->
<!-- country.same <- rep('Simple',L.line) -->

<!-- # Simple world map (in maptools package) -->
<!-- pro.shp@proj4string <- wrld_simpl@proj4string -->

<!-- # split data to ensure enough computer memory -->
<!-- use.ci <- sp.mt(1,nrow(pro.shp),3000000) -->

<!-- # Initializing the loop process -->
<!-- o1 <- over(pro.shp[1,],wrld_simpl) -->
<!-- over.shp1 <- data.frame(array(NA,dim = c(nrow(pro.shp),ncol(o1)))) -->
<!-- names(over.shp1) <- names(o1) -->

<!-- ## Overlay analysis -->
<!-- for(i in 1:nrow(use.ci)) -->
<!-- { -->
<!--   over.pro <- pro.shp[use.ci$start[i]:use.ci$end[i],] -->
<!--   overuse  <- over(over.pro,wrld_simpl) -->

<!--   overuse$FIPS      <- as.character(overuse$FIPS) -->
<!--   overuse$ISO2      <- as.character(overuse$ISO2) -->
<!--   overuse$ISO3      <- as.character(overuse$ISO3) -->
<!--   overuse$UN        <- as.character(overuse$UN) -->
<!--   overuse$NAME      <- as.character(overuse$NAME) -->
<!--   overuse$REGION    <- as.character(overuse$REGION) -->
<!--   overuse$SUBREGION <- as.character(overuse$SUBREGION) -->

<!--   over.shp1[use.ci$start[i]:use.ci$end[i],] <- overuse -->

<!--   rm(overuse); rm(over.pro) -->
<!--   print(i) -->
<!-- } -->

<!-- # Flag "simple" when the points located on simple world map -->
<!-- land.notna               <- which(!is.na(over.shp1$ISO2)) -->
<!-- overlay.land[land.notna] <- 'Simple' -->


<!-- ## Country boundary detection -->
<!-- dif.ju    <- as.character(over.shp1$ISO2) == pro.shp@data$countryCode -->
<!-- NA.number <- which(is.na(dif.ju)) -->
<!-- same.diff <- sort(union(which(!dif.ju),NA.number)) -->
<!-- country.same[same.diff] <- 0 -->
<!-- #country.same[NA.number] <- NA -->

<!-- # Natural world map -->
<!-- pro.shp2  <- pro.shp[same.diff,] -->
<!-- pro.shp2@proj4string <- worldmap2@proj4string -->
<!-- overuse.shp2         <- over(pro.shp2,worldmap2) -->

<!-- overland2 <- which(!is.na(overuse.shp2$ISO_A2)) -->
<!-- overlay.land[same.diff[overland2]] <- str_replace_all(overlay.land[same.diff[overland2]],'0','NE') -->


<!-- dif.ju2    <- as.character(overuse.shp2$ISO_A2) == pro.shp2@data$countryCode -->
<!-- NA.number2 <- which(is.na(dif.ju2)) -->
<!-- same.diff2 <- sort(union(which(!dif.ju2),NA.number2)) -->
<!-- country.same[same.diff[which(dif.ju2)]] <- str_replace_all(country.same[same.diff[which(dif.ju2)]],'0','NE') -->

<!-- diff.use <- same.diff[same.diff2] -->

<!-- if(length(same.diff2) != 0) -->
<!-- { -->
<!--   # GADM worldmap overlay -->
<!--   pro.shp3 <- pro.shp2[same.diff2,] -->
<!--   pro.shp3@proj4string <- worldmap@proj4string -->

<!--   # overuse.shp3        <- over(pro.shp3,worldmap) -->

<!--   use.ci3 <- sp.mt(1,nrow(pro.shp3),20000) -->

<!--   o3 <- over(pro.shp3[1,],worldmap) -->
<!--   over.shp3 <- data.frame(array(NA,dim = c(nrow(pro.shp3),ncol(o3)))) -->
<!--   names(over.shp3) <- names(o3) -->


<!--   for(i in 1:nrow(use.ci3)) -->
<!--   { -->
<!--     over.pro <- pro.shp3[use.ci3$start[i]:use.ci3$end[i],] -->
<!--     overuse  <- over(over.pro,worldmap) -->

<!--     overuse$NAME_ENGLI <- as.character(overuse$NAME_ENGLI) -->
<!--     overuse$ISO2       <- as.character(overuse$ISO2) -->
<!--     overuse$ISO        <- as.character(overuse$ISO) -->


<!--     over.shp3[use.ci3$start[i]:use.ci3$end[i],] <- overuse -->

<!--     rm(overuse); rm(over.pro) -->
<!--     print(i) -->

<!--   } -->

<!--   # GADM world map results -->
<!--   overland3 <- which(!is.na(over.shp3$ISO2)) -->
<!--   overlay.land[same.diff[same.diff2[overland3]]] <- str_replace_all(overlay.land[same.diff[same.diff2[overland3]]],'0','GADM') -->
<!--   dif.ju3    <- as.character(over.shp3$ISO2) == pro.shp3@data$countryCode -->
<!--   NA.number3 <- which(is.na(dif.ju3)) -->
<!--   same.diff3 <- sort(union(which(!dif.ju3),NA.number3)) -->
<!--   country.same[same.diff[same.diff2[which(dif.ju3)]]] <- str_replace_all(country.same[same.diff[same.diff2[which(dif.ju3)]]],'0','GADM') -->
<!--   #################################### -->
<!--   diff.use <- same.diff[same.diff2[same.diff3]] -->
<!--   # save.image('file') -->
<!-- } -->

<!-- ################################################################ -->
<!-- ## Correct coordinates -->
<!-- ################################################################ -->
<!-- adj.type     <- rep(NA,L.line) # transform types of coordinates -->
<!-- adj.lat      <- rep(NA,L.line) # latitude after correction -->
<!-- adj.lon      <- rep(NA,L.line) # longitude after correction -->
<!-- adj.correct  <- rep(NA,L.line) # Whether the coordinate need correct -->
<!-- adj.over     <- rep(NA,L.line) # After the correction process, where the coordinate located in? -->


<!-- ## 7 types -->
<!-- coord.save   <- data.frame(x = rep(NA,length(diff.use)*7),y = rep(NA,length(diff.use)*7)) -->
<!-- country.save <- rep(NA,length(diff.use)*7) -->
<!-- diff.save    <- rep(NA,length(diff.use)*7) -->

<!-- # country.same[diff.use[which(df.use$countryCode[diff.use] == "")]] <- NA -->
<!-- diff.use <- setdiff(diff.use, diff.use[which(df.use$countryCode[diff.use] == "")]) -->

<!-- x <- df.use$decimalLongitude[diff.use] -->
<!-- y <- df.use$decimalLatitude[diff.use] -->
<!-- use.iso2 <- df.use$countryCode[diff.use] %>% as.character -->

<!-- # Transformation process and tests (Simple world map) -->
<!-- over.list1  <- Coord8.Out(coordx = x, coordy = y, worldmap_use = wrld_simpl) -->
<!-- match.trans <- Coord.match.trans(ISO.rec = use.iso2, match.overlist = over.list1, -->
<!--                                  NameVec = 'ISO2') -->

<!-- # If the correction process were successed, we flag some variables -->
<!-- adj.correct[diff.use[which(!is.na(match.trans))]] <- 1 -->
<!-- adj.over[diff.use[which(!is.na(match.trans))]] <- 'Simple' -->
<!-- adj.type[diff.use[which(!is.na(match.trans))]] <- match.trans[which(!is.na(match.trans))] -->



<!-- # Transformation process and tests (Natural Earth world map) -->
<!-- diff.use2 <- diff.use[which(is.na(match.trans))] -->
<!-- use.iso2.2 <- use.iso2[which(is.na(match.trans))] -->

<!-- x2 <- df.use$decimalLongitude[diff.use2] -->
<!-- y2 <- df.use$decimalLatitude[diff.use2] -->

<!-- over.list2 <- Coord8.Out(coordx = x2, coordy = y2,  -->
<!--                          worldmap_use = worldmap2) -->
<!-- match.trans2 <- Coord.match.trans(ISO.rec = use.iso2.2,  -->
<!--                                   match.overlist = over.list2, -->
<!--                                   NameVec = 'ISO_A2') -->

<!-- table(match.trans2) -->
<!-- adj.correct[diff.use2[which(!is.na(match.trans2))]] <- 1 -->
<!-- adj.over[diff.use2[which(!is.na(match.trans2))]] <- 'NE' -->
<!-- adj.type[diff.use2[which(!is.na(match.trans2))]] <- match.trans2[which(!is.na(match.trans2))] -->

<!-- # Transformation process and tests (GADM world map) -->
<!-- diff.use3 <- diff.use2[which(is.na(match.trans2))] -->
<!-- use.iso3 <- use.iso2.2[which(is.na(match.trans2))] -->

<!-- x3 <- df.use$decimalLongitude[diff.use3] -->
<!-- y3 <- df.use$decimalLatitude[diff.use3] -->

<!-- over.list3 <- Coord8.Out(coordx = x3, coordy = y3, worldmap_use = worldmap) -->
<!-- match.trans2 <- Coord.match.trans(ISO.rec = use.iso3, match.overlist = over.list3, -->
<!--                                   NameVec = 'ISO') -->

<!-- # Flag relative variables in df.bio data.frame -->
<!-- df.bio$CountrySame[Line.use]   <- country.same -->
<!-- df.bio$Overland[Line.use]      <- overlay.land -->
<!-- df.bio$Adj.Coord[Line.use]     <- adj.correct -->
<!-- df.bio$Adj.overMap[Line.use]   <- adj.over -->
<!-- df.bio$Adj.TransTypr[Line.use] <- adj.type -->

<!-- # remove useless variables -->
<!-- rm(list = setdiff(ls(),c('df.bio','add.var','taxonbind', -->
<!--                          'judge.taxon','flag.next','add.name'))) -->



<!-- ############################################ -->
<!-- ## Potential error test in space dimension -->
<!-- ############################################ -->
<!-- library(plyr) -->

<!-- # Read distribution data -->
<!-- tb.coord <- count(df.bio[,c('decimalLatitude','decimalLongitude')]) -->
<!-- tb.coord$sumcoord <- tb.coord$decimalLatitude+tb.coord$decimalLongitude -->

<!-- # Institution data processing -->
<!-- # Data source: Zizka et al. (2019) -->
<!-- use.ins <- fread('L:/dropbox/Dropbox/Dropbox/Dropbox/data/institutions.csv') -->
<!-- loop.i  <- setdiff(1:nrow(tb.coord),which(is.na(tb.coord$sumcoord))) -->
<!-- pro1    <- which((tb.coord$decimalLatitude %in% use.ins$Lat) &  -->
<!--                    (tb.coord$decimalLongitude %in% use.ins$Lon)) -->

<!-- tb.coord$flag <- NA -->

<!-- for(i in pro1) -->
<!-- { -->
<!--   sub1  <- subset(use.ins, Lat == tb.coord$decimalLatitude[i]) -->
<!--   j.num <- which(sub1$Lon == tb.coord$decimalLongitude[i])[1] -->

<!--   if(length(j.num) != 0) {tb.coord$flag[i] <- sub1$Name[j.num]} -->
<!-- } -->

<!-- table(tb.coord$flag) -->

<!-- # Centroid data processing (GeoUse) -->
<!-- use.cent <- fread('YourPath/GeoNames_centroid.csv') -->
<!-- pro2     <- which((tb.coord$decimalLatitude %in% use.cent$lat) &  -->
<!--                     (tb.coord$decimalLongitude %in% use.cent$long)) -->
<!-- tb.coord$flagCent <- NA -->

<!-- for(i in pro2) -->
<!-- { -->
<!--   sub1  <- subset(use.cent, lat == tb.coord$decimalLatitude[i]) -->
<!--   j.num <- which(sub1$long == tb.coord$decimalLongitude[i])[1] -->

<!--   if(length(j.num) != 0) {tb.coord$flagCent[i] <- sub1$name[j.num]} -->
<!-- } -->

<!-- table(tb.coord$flagCent) -->

<!-- # Country data processing (MEE use) -->
<!-- library(CoordinateCleaner) -->
<!-- coord.inf <- countryref -->
<!-- loop.i  <- setdiff(1:nrow(tb.coord),which(is.na(tb.coord$sumcoord))) -->
<!-- pro3.1  <- which((tb.coord$decimalLatitude %in% coord.inf$centroid.lat) &  -->
<!--                    (tb.coord$decimalLongitude %in% coord.inf$centroid.lon)) -->
<!-- pro3.2  <- which((tb.coord$decimalLatitude %in% coord.inf$capital.lat) &  -->
<!--                    (tb.coord$decimalLongitude %in% coord.inf$capital.lon)) -->


<!-- tb.coord$flagMeeRef1 <- NA -->
<!-- tb.coord$flagMeeRef2 <- NA -->

<!-- for(i in pro3.1) -->
<!-- { -->
<!--   sub1  <- subset(coord.inf, centroid.lat == tb.coord$decimalLatitude[i]) -->
<!--   j.num <- which(sub1$centroid.lon == tb.coord$decimalLongitude[i])[1] -->

<!--   if(length(j.num) != 0) {tb.coord$flagMeeRef1[i] <- sub1$name[j.num];print(i)} -->
<!-- } -->


<!-- for(i in pro3.2[-1]) -->
<!-- { -->
<!--   sub1  <- subset(coord.inf, capital.lat == tb.coord$decimalLatitude[i]) -->
<!--   j.num <- which(sub1$capital.lon == tb.coord$decimalLongitude[i])[1] -->

<!--   if(length(j.num) != 0) {tb.coord$flagMeeRef2[i] <- paste0(sub1$name[j.num],'_cap');print(i)} -->
<!-- } -->
<!-- table(tb.coord$flag) -->



<!-- # Institution and centroid data processing (to vector) -->
<!-- subcoord  <- subset(df.bio, select = c('scientificName','decimalLatitude', -->
<!--                                        'decimalLongitude','ExGeo')) -->
<!-- flag.ins  <- rep(NA,nrow(df.bio)) -->
<!-- flag.cent <- rep(NA,nrow(df.bio)) -->

<!-- process1  <- which(!is.na(tb.coord$flag)) -->
<!-- for(i in process1) -->
<!-- { -->
<!--   rep.no <- which(subcoord$decimalLatitude == tb.coord$decimalLatitude[i] &  -->
<!--                     subcoord$decimalLongitude == tb.coord$decimalLongitude[i]) -->
<!--   flag.ins[rep.no] <- tb.coord$flag[i] -->
<!--   rm(rep.no) -->
<!-- } -->

<!-- process2 <- which(!is.na(tb.coord$flagCent)) -->
<!-- for(i in process2) -->
<!-- { -->
<!--   rep.no2 <- which(subcoord$decimalLatitude == tb.coord$decimalLatitude[i] &  -->
<!--                      subcoord$decimalLongitude == tb.coord$decimalLongitude[i]) -->
<!--   flag.cent[rep.no2] <- tb.coord$flagCent[i] -->
<!--   rm(rep.no2) -->
<!-- } -->

<!-- flag.ref1 <- rep(NA,nrow(df.bio)) -->
<!-- process3.1 <- which(!is.na(tb.coord$flagMeeRef1)) -->
<!-- for(i in process3.1) -->
<!-- { -->
<!--   rep.no3.1 <- which(subcoord$decimalLatitude == tb.coord$decimalLatitude[i] &  -->
<!--                      subcoord$decimalLongitude == tb.coord$decimalLongitude[i]) -->
<!--   flag.ref1[rep.no3.1] <- tb.coord$flagMeeRef1[i] -->
<!--   rm(rep.no3.1) -->
<!-- } -->

<!-- flag.ref2 <- rep(NA,nrow(df.bio)) -->
<!-- process3.2 <- which(!is.na(tb.coord$flagMeeRef2)) -->
<!-- for(i in process3.2) -->
<!-- { -->
<!--   rep.no3.2 <- which(subcoord$decimalLatitude == tb.coord$decimalLatitude[i] &  -->
<!--                      subcoord$decimalLongitude == tb.coord$decimalLongitude[i]) -->
<!--   flag.ref2[rep.no3.2] <- tb.coord$flagMeeRef2[i] -->
<!--   rm(rep.noe.2) -->
<!-- } -->

<!-- # Save results -->
<!-- table(flag.cent) -->
<!-- table(flag.ins) -->
<!-- table(flag.ref1) -->
<!-- table(flag.ref2) -->

<!-- df.bio$FlagCent  <- flag.cent # centroid -->
<!-- df.bio$FlagIns   <- flag.ins  # institution -->
<!-- df.bio$FlagRef1  <- flag.ref1 # adjusted coordinates -->
<!-- df.bio$FlagRef2  <- flag.ref2 # adjusted coordinates -->


<!-- ####################################### -->
<!-- ## Botanic Garden Filter -->
<!-- ####################################### -->
<!-- library(data.table) -->
<!-- library(stringr) -->

<!-- garden.inf <- read.csv('YourPath/GardenInformation.csv', encongding='UTF-8') -->
<!-- NameGarden <- tolower(str_trim(as.character(garden.inf$`Institution Name`))) -->

<!-- ## flag variable -->
<!-- flag.garden         <- rep(NA,nrow(df.bio)) -->

<!-- # extract coordinates -->
<!-- garden.coord            <- garden.inf[,c("Latitude","Longitude")] -->
<!-- row.names(garden.coord) <- paste0('BGCI',1:nrow(garden.inf)) -->
<!-- garden.uni.coord        <- unique(garden.coord) -->

<!-- na.row <- NULL -->
<!-- for(i in 1:nrow(garden.uni.coord)) -->
<!-- { -->
<!--   if( is.na(garden.uni.coord$Latitude[i]) | is.na(garden.uni.coord$Longitude[i])) -->
<!--   { -->
<!--     na.row <- c(na.row,i) -->
<!--   } -->
<!-- } -->

<!-- garden.uni.coord <- garden.uni.coord[-na.row,] -->

<!-- # Adjusted coordinates results -->
<!-- num.adj <- which(df.bio$Adj.Coord == 1) -->
<!-- df.adj <- data.frame(num.row = num.adj, Type = df.bio$Adj.TransTypr[num.adj],  -->
<!--                      Lon = df.bio$decimalLongitude[num.adj],  -->
<!--                      Lat = df.bio$decimalLatitude[num.adj], -->
<!--                      AdjLon = NA, AdjLat = NA) -->

<!-- for(i in 1:nrow(df.adj)) -->
<!-- { -->
<!--   coord.adj <- Coord8.single(x = df.adj$Lon[i], y = df.adj$Lat[i],type = df.adj$Type[i]) -->
<!--   df.adj[i,5:6] <- coord.adj -->
<!-- } -->

<!-- ## Filter: coordinate, same with botanic garden in BGCI database -->
<!-- for(i in 1:nrow(garden.uni.coord)) -->
<!-- { -->
<!--   lati      <- garden.uni.coord$Latitude[i]; loni <- garden.uni.coord$Longitude[i] -->
<!--   flagname  <- row.names(garden.uni.coord)[i] -->

<!--   flag1.geo <- which(df.bio$decimalLatitude == lati & df.bio$decimalLongitude == loni & is.na(df.bio$Adj.Coord)) -->
<!--   flag2.geo <- df.adj$num.row[which(df.adj$AdjLat == lati & df.adj$AdjLon == loni)] -->
<!--   flag.geo  <- c(flag1.geo,flag2.geo) -->

<!--   if(length(flag.geo) != 0) -->
<!--   { -->
<!--     flag.garden[flag.geo] <- flagname -->
<!--     print(i) -->
<!--   } else {next} -->

<!-- } -->

<!-- table(flag.garden) -->

<!-- ###### -->
<!-- df.bio$FlagBGCI <- flag.garden -->
<!-- rm(list = setdiff(ls(),c('df.bio','add.var','taxonbind', -->
<!--                          'judge.taxon','flag.next','add.name'))) -->
<!-- # save.image('YourPath') -->



<!-- ``` -->

<!--chapter:end:02-cleaning.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth", 
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```
# Fitness for use

Knowledge of the quality of data available is very important. Quantifying gaps in data taxonomic, temporal and spatial is an important step. The R package **BIRDS** provides a resource to do all of these things. It can:

  - Summarise spatial distribution of records;
  - Summarise the temporal distribution of records; and
  - Thorugh the implementation of ignorance scores, summarise likely gaps in biodiversity knowledge.
  
  
```{r message=FALSE, warning=FALSE}
library(BIRDS)
data("bryophytaObs")
data("gotaland")

OB <- organiseBirds(bryophytaObs)

grid <- makeGrid(gotaland, gridSize = 10)

SB <- summariseBirds(OB, grid)

```

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(sf)
library(cowplot)
library(ggplot2)
library(colorRamps)

spatial_sf <- st_as_sf(SB$spatial)

obs <- ggplot(data = spatial_sf ,aes( fill = nObs))+
  geom_sf()+
  ggtitle("Observations")+
  scale_fill_gradientn(colors = matlab.like2(100))+
  theme_cowplot()

Sp <- ggplot(data = spatial_sf ,aes( fill = nSpp))+
  geom_sf()+
  ggtitle("Number of species")+
  scale_fill_gradientn(colors = matlab.like2(100))+
  theme_cowplot()


obs
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
Sp
```

## Bias

All data sets are biased to some degree. For field data, the structure of data gathering should be robust enough to reduce the effects of bias to a minimum. Data downloaded from biodiversity databases are by their nature from different sources. Such sources may have differing causes of bias and it is important to consider if and how such biases will affect research.

Sources of bias may include:

- accessibility of the landscape;
- human population density;
- distribution of expertise;
- "Charisma" of the taxa;

There is a large body of literature on different methods which may be employed. One of the simplest is the selection of background points with the same spatial bias as taxa which are searched for in the manner [@phillips_sample_2009]. Such a layer can be produced using 2-dimensional kernel estimation through the **MASS** package
```{r, warning=FALSE, error=FALSE, message=FALSE}
library(raster)
library(MASS)
data("bryophytaObs")

Sample_bias_layer <- raster( MASS::kde2d(
            x = bryophytaObs$decimalLongitude,
            y = bryophytaObs$decimalLatitude,
            h = c(100,200),
            lims = c(10,20,55,60)))
```
 
 More complex methods of accounting for bias can be created by more explicitly modeling accessibility or the behavior of recorders themselves. There are resources for doing this, including:
 
 - **recorderMetrics** - Data derived metrics of recorder behavior [@august_data-derived_2020]
 - **Sampbias** - Bayesian analysis to quantify the effects of accessibility on species occurrence sets [@zizka_sampbias_2020]

Spatial sorting or dissaggregating presence points can reduce the effects of bias:

 - **ENMeval** - Provides methods for quickly spatially aggregating presence points for model building;
 - **spThin** - Randomly thins the number of presences used for a model by an agreed distance. THis


<!--chapter:end:03-fitness.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth", 
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```
# Data Exploration and Transformation

There are many different ways of arranging data, especially within the R environment. Two of the most frequently used are the **wide** and **long** formats. Here we show what the wide and long formats are and ways to move between the two.

## Wide
Wide format is where each row represents a data point and each column an attribute of that data point. In the example below each data point is a site, and each column is a species percentage cover.
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(kableExtra)

df_wide <- data.frame(Site = paste0("Site_",1:5),
                 Species1 = sample(0:100,5,replace = TRUE),
                 Species2 = sample(0:100,5,replace = TRUE),
                 Species3 = sample(0:100,5,replace = TRUE),
                 Species4 = sample(0:100,5,replace = TRUE),
                 Species5 = sample(0:100,5,replace = TRUE))
kable(df_wide, booktabs = TRUE, caption = "Example of wide format")%>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = T)
```

## Long
Long format is where each row has a value of an attribute of the data point. Each row in the example below records what the value of a species is at a particular site.

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(reshape2)
df_long <- melt(df_wide,
             variable.name = "Species",
             value.name = "cover")
kable(head(df_long),caption = "Example of long format")%>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = T)
```

There are a number of ways of moving between the two:

## Wide to long
```{r eval = FALSE, warning=FALSE, message=FALSE}

library(reshape2)
df_long <- melt(data = df_wide,
             variable.name = "Species",
             value.name = "cover")

library(tidyr)

df_long <- df_wide %>%
  gather(key = Species,
         value = cover,
         Species1:Species5,  # vector of columns to gather
         factor_key = FALSE)

```

## Long to Wide
```{r eval = FALSE, warning=FALSE, message=FALSE}
library(reshape2)

df_wide <- dcast(data = df_long,
                 Site ~ Species, 
                 value.var = "cover")

library(tidyr)

df_wide <- df_long %>%
  spread(key = Species,
         value = cover,
         fill = 0)
```

## Spatial data

Spatial data have coordinates. IN the simplest form they are a set of points. They may be polygonal (both regular and irregular) depicting any sort of feature. They may have additional information attached to them such as a data frame. They may take the form of a grid and may be in the raster format. Very frequently they represent the land surface and as such may have coordinate reference information attached (eg. SWEREF 99 TM) There are several resources for handling such data:

 - in R the most frequent base libraries encountered are:
 
    - **sp** - Classes and methods for spatial data 
    - **raster** - Geographic data analysis and modeling 
    - **sf** - Simple features for R 
    - **rgdal** -  Bindings for the 'Geospatial' Data Abstraction Library 
  
 - There are a number of other programs that are used for spatial GIS data. THese include:
    - QGIS - Open source GIS 
    - ArcGIS - Esri ArcGIS

Provided are the libraries and the appropriate functions for exporting spatial data from R

### Spatial vector data
```{r eval=FALSE}
library(sf)
write_sf() #capable of writing multiple different file formats

library(rgdal)
writeOGR() #capable of writing multiple different file formats

library(raster)
shapefile() #writes ESRI shapefiles
```
### Raster
```{r eval=FALSE}
library(raster)
writeRaster() #capable of writing multiple different file formats

```

<!--chapter:end:04-explore-transform.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth", 
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```
# Essential Biodiversity Variables

## SDM 
[rspatial](https://rspatial.org/raster/sdm/index.html)

## Diversity 

Biological diversity analyses typically use multivariate techniques to assess variation in data sets comprising sampling events and cases. A sampling event can be across time and space. In biodiversity analysis three cases are the most common:

- Taxonomic diversity (species);
- Functional diversity (biological form); and

- Genetic diversity (allellic frequency, phylogeny etc).

Variation in these three dimensions can be directly compared both within and between these different dimensions of biological diversity. 

### Types of analyses - Patitioning $\alpha$, $\beta$, $\gamma$ *etc.* diversity

Two forms of diversity analyses are currently widely used; classic diversity measures (eg. species richness, Shannons diversity index *etc.*) and *numbers equivalents* representation of the underlying diversity distribution. This second form was first introduced in  Hill [-@hill_diversity_1973] and a number of resources are now available for computing both forms. See the forum piece *Forum: Partitioning diversity* in Ecology [-@ellison_partitioning_2010]  for a fuller discussion on the use of *numbers equivalents*. 

Both types of diversity partitioning are used for all types of analyses and we present resources which are available for both forms: 
 - Resources for classic diversity measures
 - Resources for *numbers equivalents*.

### Data form
Within the R environment both methods require data to be in the **wide format**. See [link to page explaining shift from long to wide].

```{r echo=FALSE}
library(kableExtra)

df <- data.frame(class_1 = sample(1:100,10,replace = TRUE),
                 class_2 = sample(1:100,10,replace = TRUE),
                 class_3 = sample(1:100,10,replace = TRUE),
                 class_4 = sample(1:100,10,replace = TRUE),
                 class_5 = sample(1:100,10,replace = TRUE),
                 class_6 = sample(1:100,10,replace = TRUE),
                 class_7 = sample(1:100,10,replace = TRUE),
                 class_8 = sample(1:100,10,replace = TRUE),
                 class_9 = sample(1:100,10,replace = TRUE),
                 class_10 = sample(1:100,10,replace = TRUE)
                 )
row.names(df) <- paste("Site",
                       1:nrow(df),sep="_")
kable(df, booktabs = TRUE, caption = "Example of wide format")%>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

```

### Classic diversity measures 

People have used many different indices to measure diversity. These include:

### $\alpha$ diversity

$\alpha$ diversity refers to the diversity at a single site. There are a number of different indices to caclculate the most common are:

- Species richness;
- Shannon/shannon weaver index;
- Simpson;
- Inverse Simpson; and
- Gini Simpson

The R libraries **vegan, adiv, abdiv** all provide methods to calculate these measures as well as a wealth of others. Within **abdiv** the funtion *alpha_diversity* lists the $\alpha$ diversity measures available within the package. Whilst not exhaustive it is a large list.

There are also ways of estimating $\alpha$ diversity through rarefaction as well as modeling and visualising its different aspects in both **vegan** and **adiv**.

### $\beta$ diversity and dissimilarities

The $\beta$ diversity is a measure of the change in composition and/or abundance between sites. There is a long history of methods to measure this particular aspect of diversity. This has resulted in multiple indices and dissimilarities. Commonly used indices include:

- Jaccard;
- S√∏rrensen;
- Bray-Curtis;
- Hellinger distance,
- Chord distance


An extensive list of $\beta$ diversities are available through the function *betadiver* in the **vegan** package as well as *beta_diversity* in the **abdiv** package. 

Methods for analysing $\beta$ diversity are included in the libraries **betapart, vegan, adiv and ade4**.

### Additional related diversities

Measures of dark diversity (those species that are absent from an ecosystem but which belong to its species pool, [@partel_dark_2011]) and $\zeta$ diversity (a measure of species pool overlap, [@hui_zeta_2014]) have been developed. These measures are instituted in the libraries:

- **DarkDiv** dark diversity; and
- **zetadiv** zeta diversity
 

### Numbers equivalents

Numbers equivalents account for the nested heirarchies in $\alpha$, $\beta$ and $\gamma$ diversity. A number of different libraries have now been developed:

- Hill numbers: @hill_diversity_1973
  - **adiv**
  - **HillR** 
  - **iNext**

- Tsallis entropy: @marcon_decomposing_2015
  - **adiv**
  - **entropart**
  
 - Crossing point theory: @patil_diversity_1982
    - **BioFTF**
    

### Example of analysis of diversity

In this example we use the sampling event "Vegetation data from sheep grazing experiment at alpine site in Hol, Norway" available [here](https://www.gbif.org/dataset/57d403f4-9d00-4651-8e26-1f4e81a21181). 

Downloading the Darwin core archive which contains a ... TO BE DONE...

```{r  echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
library(finch)
library(reshape2)
library(dplyr)

fileDirs <- list.dirs("./data/Downloads/")
vegData <-  finch::dwca_read( list.files( fileDirs,
                     full.names = TRUE), read = TRUE)
```


#### Event data

```{r , echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
library(kableExtra)

kable(head(vegData$data$event.txt)) %>%
  kable_styling()
```

#### Occurence data

```{r , echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
library(kableExtra)

kable(head(vegData$data$occurrence.txt)) %>%
  kable_styling()
```

<!--chapter:end:05-essential-biodiversity-variables.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth", 
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```
`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

