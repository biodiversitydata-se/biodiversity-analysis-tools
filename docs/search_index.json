[["index.html", " A general workflow for analysis of primary biodiversity data Chapter 1 Introduction", " A general workflow for analysis of primary biodiversity data Debora Arlt, Alejandro Ruete and Charles Campbell for the Swedish Biodiversity Data Infrastructure 2023-07-17 Chapter 1 Introduction Biodiversity resources are increasingly international. The SBDI has made an effort to canalize biodiversity data and resources to help the research community access and analyze Swedish primary biodiversity data. Each research question draws its own challenges which are unique in themselves. Our aim here is to provide a general workflow that hopefully answers or at least prompts the questions that should be asked at each stage of the process. We point to resources, methods and facilities that may be useful in answering a particular question. We assume some knowledge of statistical inference and its limitations. The validity and appropriateness of a particular method dependent on the individual researcher(s). This workflow is focused on the statistical programming language R, as an environment where the complete analysis workflow can be documented in a fully reproducible way. However, we also point to other tools that can be used at different stages, and ways to import and export the data from and to those tools. We focus on biodiversity data and resources from Sweden but our aim is to present considerations and methods that can be applied beyond Sweden’s borders. The general workflow proposed is: Data –&gt; Cleaning –&gt; Fitness evaluation –&gt; Analysis always exploring and filtering the data in light of the research question. "],["the-importance-of-questions-and-sources-of-data.html", "Chapter 2 The importance of questions and sources of data 2.1 Questions 2.2 Taxonomies 2.3 Data Sources", " Chapter 2 The importance of questions and sources of data 2.1 Questions Any question to be asked of biodiversity data should be put as simply and succinctly as possible. With the number of different subject areas and techniques used, analyses can quickly become complex. 2.2 Taxonomies It is important to be aware of likely taxonomic anomalies prior to working within a region. Check-lists are very important, especially if working over several regions / countries. Whilst there are many things that will automatically look for the validity of a name they do not check for the validity of that species occurrence. For example Sphagnum auriculatum and S. denticulatum are both valid names. S. auriculatum is the currently accepted species in Europe but in the British Isles, Ireland and the Netherlands s. denticulatum is the most recorded taxa. Using data from across the European region without acknowledging this disagreement would impact the results of any research undertaken. For taxa which are known to be capable of dispersing great distances (eg birds) this becomes even more difficult especially when using community sourced data. For Sweden there is an agreed taxonomy for species accessible through dyntaxa and the R library dyntaxa. 2.3 Data Sources Depending on what questions are being asked there are many different resources available. We focus on biodiversity data 2.3.1 Biodiversity record data There are a large number of available on-line resources. These include but are not limited to (specific R libraries that connect to these databases are provided in bold): Swedish Biodiversity Data Infrastructure - Sweden’s data portal for biodiversity data Global Biodiversity Information Facility - International organization aggregating biodiversity data. Contains data from a mixture of sources; curated collections, community science data, ecological research projects etc. rgbif, spocc BioCASE - A European transnational biodiversity repository eBird - American database of bird observations auk, rebird,spocc iNaturalist - International community science observation repository spocc Berkeley ecoengine - Access to UC Berkley’s Natural history data spocc VertNet - vertebrate biodiversity collections rvert, spocc iDigBio - Integrated digitise biodiversity collections ridigbio OBIS - Ocean biodiversity information system robis ALA - Atlas of living Australia ALA4r Neotoma Palaeoecology databas neotoma … 2.3.2 Taxonomic diversity To keep track of ever changing taxonomy of species there are different databases that follow different standars. dyntaxa GBIF backbone … 2.3.3 Functional diversity Very broadly functional diversity is the diversity of what organisms do (Petchey and Gaston 2006). Such diversity can be direct physical measurements of traits of the organisms involved and / or data summarized from published works. There are databases dedicated to the distribution of scientific data that may be used. Such resources include: Dryad TRY: Plant Trait Database Ecological indicator and traits values for Swedish vascular plants(Tyler et al. 2021) [https://doi.org/10.1016/j.ecolind.2020.106923] Vertnet … 2.3.4 Genetic data bases Genetic data may be related directly to the samples used, phylogenetic trees generated from some other data set set or some other genetic aspect. Such resources include: BOLD: Barcode Of Life Data system Repository of gene sequences Genbank A genetic sequence repository Treebase A data base of phylogenetic trees … 2.3.5 Environemntal data There are a number of environmental repositories available. Static data sets for global resources include: 2.3.5.1 Sweden Skogsstyrelsen - Open data supplied by Skogsstyrelsen Sveriges dataportal - Swedish open data provided by governmental bodies. Miljödata MVM - Environmental data from SLU for Sweden 2.3.5.2 Global and European wordclim - Global gridded climate data at various resolutions climond - Global gridded climate data at various resolutions soilgrids - Global gridded soil data Copernicus Land monitoring service - Land monitoring data from across the EU References "],["data-cleaning.html", "Chapter 3 Data cleaning 3.1 Resources 3.2 Taxonomies 3.3 Location data 3.4 Examples", " Chapter 3 Data cleaning Biodiversity data repositories work hard to maintain the accuracy of their holdings. When multiple sources are involved several problems may arise. Here we shall quickly outline what they are and possible solutions. 3.1 Resources There are a number of libraries, work flows and online resources for automating downloading and cleaning of data. THese include: The BDverse. A group of libraries for cleaning biodiversity data Kurator Wallace 3.2 Taxonomies It is important to be aware of likely taxonomic anomalies prior to working within a region. Checklists are very important, especially if working over several regions / countries. Whilst there are many things that will automatically look for the validity of a name they do not check for the validity of that species occurrence. For example Sphagnum auriculatum Schimp. and Sphagnum denticulatum Bridel, 1826 are both valid names. S. auriculatum is the currently accepted species in Europe but in the British Isles, Ireland and the Netherlands s. denticulatum is the most recorded taxa. Both are legitimate names but they are a synonymy. Both names have been used in Europe but in distinct countries. The current European checklist (Hodgetts et al. 2020) has Sphagnum auriculatum Schimp. as the accepted taxon occurring in Europe. Naive downloading would result in two taxa being present when in fact it is two interpretations of the same taxon. Using data from across the European region without acknowledging this disagreement would impact the results of any research undertaken. For taxa which are known to be capable of dispersing great distances (eg. birds) this becomes even more difficult especially when using community sourced data. The example above uses data downloaded from GBIF. Data for Sphagnum denticulatum doi: 10.15468/dl.rrp4p4 Data for Sphagnum auriculatum doi: 10.15468/dl.3yrtw7 Within Sweden there is an agreed taxonomy for all extant taxa accessible through Dyntaxa. Checking of species lists can be done by directly copying species names into a dialogue box or uploading an excel spreadsheet in the correct format. 3.3 Location data 3.3.1 Locality information Many records have locality information attached to them. Where there are no coordinates attached this information can be used to locate the record to within an area of where records most likely came from. There are functions for which geocoding can be done automatically within R. Geocoding is finding the coordinate for a known locality. These are included in the libraries: ggmap requires google API key tidygeocoder Many localities may not be included in the gazetteers associated with these libraries but may be located using online or printed maps. This takes time but may be useful. Unique localities may be extracted, geocoded and then merged back with the data set. This becomes especially important when extracting records across country boundaries as countries have different numbers of georeferenced observations. Table 3.1: The number and percentage of records of bryophyta per EU country with and without coordinates Country Total Records % Records with coordinate % Records without coordinates AL Albania 290 8.62 8.62 AD Andorra 2359 7.33 7.33 AM Armenia 406 47.04 47.04 AT Austria 63788 22.15 22.15 BY Belarus 1671 25.43 25.43 BE Belgium 456833 6.91 6.91 BA Bosnia &amp; Herzegovina 368 75.54 75.54 BG Bulgaria 1177 41.89 41.89 HR Croatia 882 56.92 56.92 CY Cyprus 578 67.30 67.30 CZ Czechia 18211 51.15 51.15 DK Denmark 899105 1.29 1.29 EE Estonia 103097 37.08 37.08 FO Faroe Islands 1596 47.68 47.68 FI Finland 228360 9.13 9.13 FR France 929930 8.52 8.52 GE Georgia 8278 39.74 39.74 DE Germany 197180 20.03 20.03 GI Gibraltar 22 100.00 100.00 GR Greece 2051 67.28 67.28 HU Hungary 3680 64.54 64.54 IS Iceland 42775 15.00 15.00 IE Ireland 131949 2.80 2.80 IT Italy 20679 53.45 53.45 LV Latvia 7264 44.29 44.29 LI Liechtenstein 585 3.42 3.42 LT Lithuania 4876 6.36 6.36 LU Luxembourg 23048 1.63 1.63 MT Malta 95 44.21 44.21 MC Monaco 16 87.50 87.50 NL Netherlands 857912 4.16 4.16 MK North Macedonia 273 22.71 22.71 NO Norway 509847 30.05 30.05 PL Poland 49629 31.62 31.62 PT Portugal 254056 9.98 9.98 RO Romania 1472 60.94 60.94 RU Russia 210147 16.12 16.12 SM San Marino 1 0.00 0.00 SK Slovakia 4449 59.05 59.05 SI Slovenia 3683 39.86 39.86 ES Spain 113478 23.78 23.78 SE Sweden 693897 16.50 16.50 CH Switzerland 279575 4.22 4.22 TR Turkey 9311 93.97 93.97 UA Ukraine 10493 10.51 10.51 GB United Kingdom 2965316 1.44 1.44 VA Vatican City 3 100.00 100.00 As can be seen from the above example for European Bryophyta (Hornworts, Liverworts and Mosses) the number of records with coordinates vastly varies between countries. For example of 959444 records in Sweden 88.24 % have coordinates in Switzerland of 117132 records only 2.29 % have coordinates. 3.3.2 Coordinate uncertainty In many cases there is now an abundance of biodiversity data with coordinates. As can be seen from a summary of the above Sphagnum data coordinate uncertainty can vary from less than 1 meter to multiple kilometers. Coordinate uncertainty &lt;1 m 1-10 m 10-100 m 100-500 m 500-1000 m 1-5 km 5-10 km 10-50 km &gt;50 km None Frequency 335 586 5320 376 4619 4980 4147 41 2 7506 In the above example it can be seen that two have uncertainties greater than 50 km and several thousand records that have no known error margin the location. It is important to consider what the error is and removing those records for which the uncertainty is too high. Where this point is will be dependent on the scale of the research. 3.3.3 Coordinate errors Besides the declared uncertainty of the the coordinates, coordinate errors may occur for a variety of reasons. The library coordinateCleaner is very useful for removing some of the most common errors. These include: 0 latitude, 0 longitude swapping of latitude and longitude The location of the institution holding a preserved sample rather than the location of origin of that sample 3.4 Examples 3.4.1 Data cleaning Cyperaceae in Sweden In this example I have downloaded the occurrences of the Cyperaceae family in Sweden from SBDI since 2000. This is a single data set from an area with an agreed taxonomy dyntaxa. Our aim here is to download a group of records and clean them to be in such a state than they reflect the question the researcher has in mind. First we give each record in the assembled data set a unique ID. In this case it could be possible to use the GBIFid. In data sets sourced from multiple sources it is a good idea to create a unique ID related to the data source. In cleaning data there are a number of dimensions of uncertainty we shall first check. 3.4.2 Taxonomy In the species column there are records with no Species given in the species column. We then check the scientificName for whether there is useful information there kable(Carex |&gt; filter(species == &quot;&quot;) |&gt; distinct(scientificName)) |&gt; kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;), full_width = F) scientificName Carex divulsa subsp. leersii (F.W.Schultz) W.Koch Carex L. Carex saxatilis var. saxatilis Carex fuliginosa subsp. misandra (R.Br.) Nyman Carex bigelowii subsp. rigida (Raf.) W.Schultze-Motel Carex buxbaumii subsp. mutica (Hartm.) Isov. Carex buxbaumii f. buxbaumii Carex norvegica subsp. inferalpina (Wahlenb.) Hultén Carex diandra var. major (W.D.J.Koch) Boott Carex buxbaumii var. mutica Hartm. Carex cognata var. congolensis (Turrill) Lye Carex saxatilis subsp. laxa (Trautv.) Kalela In this case there are only genre recorded. We then check for data about species in the locality information for which there are no species data given in the species column. Here there are enough unique values to be checked easily within R. It is possible that in large data sets that there may be very many records for which there is useful information There are no species data in the locality field and so we begin a vector of IDs that are not at the required data resolution. unUseful &lt;- Carex |&gt; filter(scientificName == &quot;&quot;) |&gt; select(id) We now need to check that the species names in the records that we are using are valid for the area which we are looking at i.e. Sweden. There are several resources out there that do this. As we are looking specifically and solely at Sweden there is Dyntaxa.se. There is also an R package as part of the Swedish Biodiversity Data Infrastructure with examples and documentation. For wider applicability we will export the unique names from the Carex data frame. These will then be copied to the Dyntaxa portal for matching multiple names. The results are checked against the Swedish lists and unknown species or uncertainties are flagged. Names can be copied directly into a box in Dyntaxa or may be imported directly as an xlsx file. Here we shall export just the species names. There are multiple options. SpeciesNames &lt;- Carex |&gt; filter(!id %in% unUseful) |&gt; select(&quot;scientificName&quot;) |&gt; distinct() # library(openxlsx) #write.xlsx(SpeciesNames,&quot;./data/BDcleaner_Scripts/Example/UniqueCarexTaxa.xlsx&quot;) Dyntaxa provides options for any taxononmic uncertainties and lists the species for which no match can be made. Having made selections it exports as a .xlslx file. We can then check what those species for which there is no information about what they are. In this case there are an number of species which are likely to be horticultural, a few taxa which are of hybrid origin which are in the Dyntaxa with the addition of ‘×’ between genus and species, and Carex utriculata a species from North America. This last could be a misidentification of C. rostrata. We will exclude the horticultural and uncertain species. We do this by adding a Species column in the Dyntaxa file and then load that file into R. library(openxlsx) SpeciesNames &lt;- read.xlsx(&quot;./data/matchCarex.xlsx&quot;) # From Dyntaxa Carex &lt;- merge(Carex, SpeciesNames[,c(&quot;Provided.string&quot;,&quot;SpeciesDyn&quot;)], by.x = &quot;species&quot;,by.y = &quot;Provided.string&quot;, all.x = TRUE) We then add IDs of the rows with taxa which are to be excluded to the unUseful vector. It is possible that there is overlap between these vectors. Rather than overwriting the vector we join the vectors together and use unique to get a vector with no duplications. unUseful &lt;- unique( c(unUseful, Carex |&gt; filter(is.na(SpeciesDyn)) |&gt; select(id) ) ) ###Coordinate Cleaning We need to extract locality data for rows where there is no lat/lon information to geocode it, producing a latitude and longitude for each locality. This can be carried out in a similar way as for species ie the merging of data frames. This can be also be done automatically in R using the geocode feature of the libraries tidygeocoder or ggmap. Some of these services require an API key (eg. Google). See the documentation for the link for more information Localities &lt;- Carex |&gt; filter(is.na(longitude)) |&gt; select(locality) |&gt; distinct() write.csv(Localities,&quot;./data/Localities.csv&quot;,row.names = FALSE) For brevity we will simply include the records with no lat/lon info in the unUseful vector. These will ultimately not be used. unUseful &lt;- unique(c(unUseful, Carex |&gt; filter(is.na(latitude)) |&gt; select(id) ) ) Before cleaning the coordinates we simplify the data retaining columns that have information directly related to the collection of the data. Carex &lt;- Carex |&gt; select(id, SpeciesDyn, longitude, latitude, coordinateUncertaintyInMetres, locality, # recordedBy, # countryCode, # eventDate, year, month, day, institutionCode, collectionCode, catalogueNumber) We then filter out the data rows for which we can not use ie those of too great taxonomic uncertainty. The data may then be cleaned. There are a number of things to consider when cleaning data: How precise are the locations? Coordinate uncertainty ranges in the Cyperaceae of Sweden from 1 m - 30.5 km. Are interpreted coordinates sufficient? These interpreted coordinates may imply a greater precision than is necessarily true Are the locations likely to be errors? Errors can relate to where a sample is (eg. herbarium location) rather than where it came from. Are the locations in the country claimed? This can arise through swapping of lat and lon; duplication of latitude numbers in the longitude; or simply incorrect coordinates being given. unUseful &lt;- unique( c(unUseful, Carex |&gt; filter(is.na(coordinateUncertaintyInMetres)) |&gt; select(id) ) ) # kable( t(table( round( Carex$coordinateUncertaintyInMetres, -2)))) We shall filter out all records with unknown coordinate uncertainty and all with a coordinate uncertainty more than 12.5 km. unUseful &lt;- unique( c(unUseful, Carex |&gt; filter(is.na(coordinateUncertaintyInMetres)) |&gt; select(id) ) ) unUseful &lt;- unique( c(unUseful, Carex |&gt; filter(coordinateUncertaintyInMetres &gt; 12500) |&gt; select(id) ) ) We now have an index of records we can’t use owing to incomplete taxonomy or incomplete location information. We save these before cleaning the coordinates. save(unUseful, file = &quot;./data/Cyperacea_SWE_unsuseful.rdata&quot;) We first remove the already labeled inaccurate data. Carex &lt;- Carex[-which(Carex$id %in% unUseful),] We then use the library CoordinateCleaner to automatically flag coordinates that may be errors. Outputs of this can be a cleaned data.frame or additional columns with doubtful records flagged. Carex$countryCode &lt;- countrycode::countrycode(Carex$countryCode,&quot;iso2c&quot;,&quot;iso3c&quot;) #converts ISO2 country codes to ISO3 Carex &lt;- Carex |&gt; filter(!id %in% unUseful) Carex &lt;- Carex[-which(is.na(Carex$latitude)),] Carex &lt;- clean_coordinates(Carex, lon = &quot;longitude&quot;, lat = &quot;latitude&quot;, species = &quot;SpeciesDyn&quot;, countries = &quot;countryCode&quot;, tests = c(&quot;capitals&quot;, &quot;centroids&quot;, &quot;equal&quot;, &quot;gbif&quot;, &quot;institutions&quot;, &quot;outliers&quot;, &quot;seas&quot;, &quot;zeros&quot;,&quot;countries&quot;) ) kable( Carex |&gt; summarise(`Invalid coords` = sum(.val==FALSE), `Equal coords` = sum(.equ==FALSE), `0 coords` = sum(.zer==FALSE), `capitals` = sum(.cap==FALSE), `country centre` = sum(.cen==FALSE), `Country Border` = sum(.con==FALSE), `outlier` = sum(.otl==FALSE), `Gbif HQ` = sum(.gbf==FALSE), `Insitution` = sum(.inst==FALSE), `Summary` = sum(.summary==FALSE)), col.names = c(&quot;Invalid coords&quot;,&quot;Equal coords&quot;,&quot;0 coords&quot;,&quot;capitals&quot;, &quot;country centre&quot;,&quot;Country Border&quot;,&quot;outlier&quot;,&quot;Gbif HQ&quot;, &quot;Insitution&quot;, &quot;Summary&quot;)) Checking the distribution of the observations Sweden &lt;- raster::getData(&quot;GADM&quot;, country = &quot;SWE&quot;, path = &quot;./data/&quot;, level = 0) Sweden_sf &lt;- st_as_sf(Sweden) BolMar &lt;- ggplot(Sweden_sf) + geom_sf() + geom_point(data = Carex |&gt; filter(SpeciesDyn == &quot;Carex capillaris&quot;),# |&gt; # filter(.con == FALSE), aes(x = longitude, y = latitude)) + theme_cowplot() + ggtitle(&quot;Carex capillaris&quot;) CarAct &lt;- ggplot(Sweden_sf) + geom_sf() + geom_point(data = Carex |&gt; filter(SpeciesDyn == &quot;Carex acuta&quot;), # |&gt; # filter(.otl == TRUE), aes(x = longitude, y = latitude)) + #, # colour = .otl))+ theme_cowplot() + ggtitle(&quot;Carex acuta&quot;) grid_Sweden &lt;- makeGrid(Sweden, 25) OB &lt;- organiseBirds(Carex, idCols = c(&quot;locality&quot;), xyCols = c(&quot;longitude&quot;, &quot;latitude&quot;), sppCol = &quot;SpeciesDyn&quot;) SB &lt;- summariseBirds(OB, grid_Sweden) grid_Sweden &lt;- st_as_sf(grid_Sweden) Sweden_sf &lt;- st_as_sf(Sweden) library(colorRamps) SB_SPat &lt;- st_as_sf(SB$spatial) nSpecies &lt;- ggplot(data = SB_SPat,aes(fill = nSpp)) + geom_sf() + theme_cowplot() + scale_fill_gradientn(colors = matlab.like2(100)) nVisits &lt;- ggplot(data = SB_SPat,aes(fill = nVis)) + geom_sf() + theme_cowplot() + scale_fill_gradientn(colors = matlab.like2(100)) nObs &lt;- ggplot(data = SB_SPat,aes(fill = nObs)) + geom_sf() + theme_cowplot() + scale_fill_gradientn(colors = matlab.like2(100)) plot_grid(nSpecies, nVisits, nObs, ncol = 1) References "],["fitness-for-use.html", "Chapter 4 Fitness for use 4.1 Bias", " Chapter 4 Fitness for use Knowledge of the quality of data available is very important. Quantifying gaps in data taxonomic, temporal and spatial is an important step. The R package BIRDS provides a resource to do all of these things. It can: Summarise spatial distribution of records; Summarise the temporal distribution of records; and Thorugh the implementation of ignorance scores, summarise likely gaps in biodiversity knowledge. library(BIRDS) data(&quot;bryophytaObs&quot;) data(&quot;gotaland&quot;) OB &lt;- organiseBirds(bryophytaObs) grid &lt;- makeGrid(gotaland, gridSize = 10) SB &lt;- summariseBirds(OB, grid) 4.1 Bias All data sets are biased to some degree. For field data, the structure of data gathering should be robust enough to reduce the effects of bias to a minimum. Data downloaded from biodiversity databases are by their nature from different sources. Such sources may have differing causes of bias and it is important to consider if and how such biases will affect research. Sources of bias may include: accessibility of the landscape; human population density; distribution of expertise; “Charisma” of the taxa; There is a large body of literature on different methods which may be employed. One of the simplest is the selection of background points with the same spatial bias as taxa which are searched for in the manner (Phillips et al. 2009). Such a layer can be produced using 2-dimensional kernel estimation through the MASS package library(raster) library(MASS) data(&quot;bryophytaObs&quot;) Sample_bias_layer &lt;- raster( MASS::kde2d( x = bryophytaObs$decimalLongitude, y = bryophytaObs$decimalLatitude, h = c(100,200), lims = c(10,20,55,60))) More complex methods of accounting for bias can be created by more explicitly modeling accessibility or the behavior of recorders themselves. There are resources for doing this, including: recorderMetrics - Data derived metrics of recorder behavior (August et al. 2020) Sampbias - Bayesian analysis to quantify the effects of accessibility on species occurrence sets (Zizka, Antonelli, and Silvestro 2020) Spatial sorting or dissaggregating presence points can reduce the effects of bias: ENMeval - Provides methods for quickly spatially aggregating presence points for model building; spThin - Randomly thins the number of presences used for a model by an agreed distance. References "],["data-exploration-and-transformation.html", "Chapter 5 Data Exploration and Transformation 5.1 Wide 5.2 Long 5.3 Wide to long 5.4 Long to Wide 5.5 Spatial data", " Chapter 5 Data Exploration and Transformation There are many different ways of arranging data, especially within the R environment. Two of the most frequently used are the wide and long formats. Here we show what the wide and long formats are and ways to move between the two. 5.1 Wide Wide format is where each row represents a data point and each column an attribute of that data point. In the example below each data point is a site, and each column is a species percentage cover. Table 5.1: Example of wide format Site Species1 Species2 Species3 Species4 Species5 Site_1 67 42 96 6 88 Site_2 38 13 84 72 100 Site_3 0 81 20 78 36 Site_4 33 58 53 84 33 Site_5 86 50 73 36 88 5.2 Long Long format is where each row has a value of an attribute of the data point. Each row in the example below records what the value of a species is at a particular site. Table 5.2: Example of long format Site Species cover Site_1 Species1 67 Site_2 Species1 38 Site_3 Species1 0 Site_4 Species1 33 Site_5 Species1 86 Site_1 Species2 42 There are a number of ways of moving between the two: 5.3 Wide to long # library(reshape2) df_long &lt;- melt(data = df_wide, variable.name = &quot;Species&quot;, value.name = &quot;cover&quot;) df_long &lt;- df_wide |&gt; gather(key = Species, value = cover, Species1:Species5, # vector of columns to gather factor_key = FALSE) 5.4 Long to Wide df_wide &lt;- dcast(data = df_long, Site ~ Species, value.var = &quot;cover&quot;) df_wide &lt;- df_long |&gt; spread(key = Species, value = cover, fill = 0) 5.5 Spatial data Spatial data have coordinates. IN the simplest form they are a set of points. They may be polygonal (both regular and irregular) depicting any sort of feature. They may have additional information attached to them such as a data frame. They may take the form of a grid and may be in the raster format. Very frequently they represent the land surface and as such may have coordinate reference information attached (eg. SWEREF 99 TM) There are several resources for handling such data: in R the most frequent base libraries encountered are: sp - Classes and methods for spatial data raster - Geographic data analysis and modeling sf - Simple features for R rgdal - Bindings for the ‘Geospatial’ Data Abstraction Library There are a number of other programs that are used for spatial GIS data. THese include: QGIS - Open source GIS ArcGIS - Esri ArcGIS Provided are the libraries and the appropriate functions for exporting spatial data from R 5.5.1 Spatial vector data library(sf) write_sf() #capable of writing multiple different file formats library(rgdal) writeOGR() #capable of writing multiple different file formats library(raster) shapefile() #writes ESRI shapefiles 5.5.2 Raster library(raster) writeRaster() #capable of writing multiple different file formats library(terra) writeRaster() #capable of writing multiple different file formats "],["essential-biodiversity-variables.html", "Chapter 6 Essential Biodiversity Variables 6.1 SDM 6.2 Diversity", " Chapter 6 Essential Biodiversity Variables 6.1 SDM rspatial 6.2 Diversity Biological diversity analyses typically use multivariate techniques to assess variation in data sets comprising sampling events and cases. A sampling event can be across time and space. In biodiversity analysis three cases are the most common: Taxonomic diversity (species); Functional diversity (biological form); and Genetic diversity (allellic frequency, phylogeny etc). Variation in these three dimensions can be directly compared both within and between these different dimensions of biological diversity. 6.2.1 Types of analyses - Patitioning \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) etc. diversity Two forms of diversity analyses are currently widely used; classic diversity measures (eg. species richness, Shannons diversity index etc.) and numbers equivalents representation of the underlying diversity distribution. This second form was first introduced in Hill (1973) and a number of resources are now available for computing both forms. See the forum piece Forum: Partitioning diversity in Ecology (2010) for a fuller discussion on the use of numbers equivalents. Both types of diversity partitioning are used for all types of analyses and we present resources which are available for both forms: - Resources for classic diversity measures - Resources for numbers equivalents. 6.2.2 Data form Within the R environment both methods require data to be in the wide format. See [link to page explaining shift from long to wide]. Table 6.1: Example of wide format class_1 class_2 class_3 class_4 class_5 class_6 class_7 class_8 class_9 class_10 Site_1 68 97 89 74 44 2 81 13 73 93 Site_2 39 85 37 42 25 45 100 22 87 34 Site_3 1 21 34 38 70 18 13 93 83 10 Site_4 34 54 89 20 39 22 40 28 90 1 Site_5 87 74 44 28 51 78 89 48 48 43 Site_6 43 7 79 20 42 65 48 33 64 59 Site_7 14 73 33 44 6 70 89 45 94 26 Site_8 82 79 84 87 24 87 23 21 96 15 Site_9 59 85 35 70 32 70 84 31 60 58 Site_10 51 37 70 40 14 75 29 17 51 29 6.2.3 Classic diversity measures People have used many different indices to measure diversity. These include: 6.2.4 \\(\\alpha\\) diversity \\(\\alpha\\) diversity refers to the diversity at a single site. There are a number of different indices to caclculate the most common are: Species richness; Shannon/shannon weaver index; Simpson; Inverse Simpson; and Gini Simpson The R libraries vegan, adiv, abdiv all provide methods to calculate these measures as well as a wealth of others. Within abdiv the funtion alpha_diversity lists the \\(\\alpha\\) diversity measures available within the package. Whilst not exhaustive it is a large list. There are also ways of estimating \\(\\alpha\\) diversity through rarefaction as well as modeling and visualising its different aspects in both vegan and adiv. 6.2.5 \\(\\beta\\) diversity and dissimilarities The \\(\\beta\\) diversity is a measure of the change in composition and/or abundance between sites. There is a long history of methods to measure this particular aspect of diversity. This has resulted in multiple indices and dissimilarities. Commonly used indices include: Jaccard; Sørrensen; Bray-Curtis; Hellinger distance, Chord distance An extensive list of \\(\\beta\\) diversities are available through the function betadiver in the vegan package as well as beta_diversity in the abdiv package. Methods for analysing \\(\\beta\\) diversity are included in the libraries betapart, vegan, adiv and ade4. 6.2.6 Additional related diversities Measures of dark diversity (those species that are absent from an ecosystem but which belong to its species pool, (Pärtel, Szava-Kovats, and Zobel 2011)) and \\(\\zeta\\) diversity (a measure of species pool overlap, (Hui and McGeoch 2014)) have been developed. These measures are instituted in the libraries: DarkDiv dark diversity; and zetadiv zeta diversity 6.2.7 Numbers equivalents Numbers equivalents account for the nested heirarchies in \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) diversity. A number of different libraries have now been developed: Hill numbers: Hill (1973) adiv HillR iNext Tsallis entropy: Marcon and Hérault (2015) adiv entropart Crossing point theory: Patil and Taillie (1982) BioFTF 6.2.8 Example of analysis of diversity In this example we use the sampling event “Vegetation data from sheep grazing experiment at alpine site in Hol, Norway” available here. Downloading the Darwin core archive which contains a … TO BE DONE… 6.2.8.1 Event data 6.2.8.2 Occurence data References "],["query-data-from-nils.html", "Chapter 7 Query data from NILS 7.1 A brief intro to NILS 7.2 Vegetation cover in two time periods 7.3 Blueberry and mountain violet occurrence in the alpine region", " Chapter 7 Query data from NILS 7.1 A brief intro to NILS On behalf of the Swedish Environmental Protection Agency, SLU documents, through NILS (National Inventory of the Landscape in Sweden), landscape and vegetation changes in Sweden and thus how the conditions for biological diversity look and change over time. NILS’ base inventory took place in the years 2003 to 2018 throughout Sweden and in the years 2019 to 2020 only in the mountains. The purpose of the inventory was to collect data in the field, analyze and present estimates of conditions and changes. The inventory is based on a random sample of permanent sample areas. It takes five years to complete an inventory lap; round 1 ran from 2003 to 2007, round 2 from 2008 to 2012, etc. The data is collected via field inventory in sample areas and transects. In the sample area inventory, data is partly collected in larger circular sample areas with a radius of 10 meters and in small sample areas with a radius of 0.28 m where mainly species, species groups, habitat types and other classifications are recorded. More information about design and methodology can be found here and here (in Swedish). The map shows the location of stratum 10 (gray), i.e., the Swedish mountain region, and the 145 systematically placed 5 × 5 km sample units (red). The 12 circular sample plots is in the center of the sample unit with a distance of 250 m between the centers of each plot and a distance of 2125 m between the centers of each plot and the edge of the 5 × 5 km2. Each plot consists of two concentric circular plots, and different variables are recorded in these plots. Canopy cover is estimated in the 20-m radius plot, and shrub cover, cover of field vegetation, and cover of the bottom layer are estimated in the 10-m radius plot. The sample plot is divided into subplots if the sample plot contains distinct areas of different types of land use or land cover etc. (Esseen et al. 2007). The figure also shows a list of the field variables used in this study sorted size of the circular sample plots. This inventory contributes with an environmental target indicator for monitoring the environmental target “A magnificent mountain environment” - monitoring of vegetation changes in the mountains. All inventory data can be downloaded without registering and is available at NILS datavärdskap. The data is also available via REST APIs (Application Program Interface). More documentation can be found at Swagger description of NILS’ APIs. APIs are suitable for a replicable and streamlined analysis sending queries directly to the database therefore avoiding the unnecessary download of complete data sets. REST APIs are URL (a web address) with endpoint paths to resources and parameters for filtering the query. REST API URLs are often composed as follows: Base URL: http://apiserver.com Endpoint Path: /homes for the resource ‘homes’ Query String: ?limit=5&amp;format=json Rest API URL: BaseURL + Endpoint path +Query string 7.2 Vegetation cover in two time periods Here and in the next chapter we show examples of how to integrate these APIs in your workflow. The first example replicates Fig.3 a and b from (Hedenås, Christensen, and Svensson 2016). # Set up library(jsonlite) library(dplyr) library(tidyr) library(conflicted) conflicts_prefer( dplyr::select, dplyr::filter) library(glue) library(ggplot2) library(sf) The endpoint ‘Klasser’ points to the table with all categorical data sampled within mountain areas, that is forest type, type of land cover. (Note: only for the mountain areas at the moment). urlApi &lt;- &#39;https://landskap.slu.se/api/nils/api/&#39; endpoint &lt;- &#39;Klasser&#39; klasserUrl &lt;- url(glue(&#39;{urlApi}{endpoint}&#39;)) klasserGet &lt;- fromJSON(klasserUrl) klasser &lt;- klasserGet$data This is exactly what we what to avoid. We want to filter the query to obtain a restricted data set only containing data collected between 2003 and 2012. urlApi &lt;- &#39;https://landskap.slu.se/api/nils/api/&#39; years &lt;- paste0(&#39;Ar=&#39;,c(2003:2012), collapse = &#39;&amp;&#39;) endpoint &lt;- &#39;Klasser&#39; klasserUrl &lt;- url(glue(&#39;{urlApi}{endpoint}?{years}&#39;)) klasserGet &lt;- fromJSON(klasserUrl) klasser &lt;- klasserGet$data We also want some data from the table ‘Vegetationstackning’ that contains the cover (%) of different vegetation layers. In this case, however, we don’t want to preserve all columns, just those that are unique in the classes table. endpoint &lt;- &#39;Vegetationstackning&#39; vegetationUrl &lt;- url(glue(&#39;{urlApi}{endpoint}?{years}&#39;)) vegetationGet &lt;- fromJSON(vegetationUrl) vegetation &lt;- vegetationGet$data |&gt; select(-c(&quot;koordNS&quot;,&quot;koordEW&quot;,&quot;lan&quot;,&quot;kommun&quot;,&quot;bioGeoRegion&quot;,&quot;stratum&quot;)) # Ta bort de kolumner som finns i bägge apierna och som inte behövs för att slå samman tabellerna Then, following (Hedenås, Christensen, and Svensson 2016) we proceed to: filter out the the irrelevant ‘lan’ and leave only the mountain types “Fjällbjörkskog”, “Område ovan SKOGSgränsen”; join these tables by those fields that they have in common: “ar”,“rutaNummer”, “provytaNummer”,“delytaNummer”; and generate a categorical variable for the inventory phase (in NILS, every sample area is visited once within five years) klassVegetation &lt;- klasser |&gt; filter(lan != &quot;Utlandet&quot;, fjalltyp %in% c(&quot;Fjällbjörkskog&quot;, &quot;Område ovan SKOGSgränsen&quot;)) |&gt; # left_join(vegetation, by = c(&quot;ar&quot;,&quot;rutaNummer&quot;, &quot;provytaNummer&quot;,&quot;delytaNummer&quot;)) |&gt; mutate(invPha = ifelse(ar &lt; 2007, &quot;2003-2007&quot;, &quot;2008-2012&quot;)) To be able to work further we need to transpose the ‘wide’ table into a ‘long’ table: klassVegetationPL &lt;- klassVegetation |&gt; select(fjalltyp, invPha, tradTackningTotal, buskTackningTotal, faltskiktTackningTotal) |&gt; pivot_longer(!c(fjalltyp, invPha), names_to = &quot;layer&quot;, values_to = &quot;cover&quot;) and finally summarize per group: kvSumm &lt;- klassVegetationPL |&gt; group_by(fjalltyp, invPha, layer) |&gt; summarise_at(.vars = vars(cover), .funs = list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE)), .grups = &quot;drop&quot;) |&gt; mutate_at(vars(invPha), list(factor)) |&gt; mutate(fjalltyp = factor(fjalltyp, levels = c(&quot;Område ovan SKOGSgränsen&quot;, &quot;Fjällbjörkskog&quot;)), layer = factor(layer, levels = c(&quot;tradTackningTotal&quot;, &quot;buskTackningTotal&quot;, &quot;faltskiktTackningTotal&quot;))) |&gt; # to reorder the classes as.data.frame() All it is left to do, is produce a nice plot. ggplot(kvSumm, aes(x = layer, y = mean, color = invPha)) + geom_point(position = position_dodge(0.2)) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = .1, position = position_dodge(0.2)) + facet_wrap(vars(fjalltyp), labeller = as_labeller(c(&quot;Fjällbjörkskog&quot; = &quot;Mountain birch forest&quot;, &quot;Område ovan SKOGSgränsen&quot; = &quot;Alpine&quot;))) + labs(y = &quot;Total cover (%)&quot;, x = &quot;Layer&quot;, color = &quot;Inventory phase&quot;) + scale_x_discrete(labels = c(&quot;Tree&quot;, &quot;Shrub&quot;, &quot;Field&quot;)) The resulting graph shows the estimated total canopy cover, total cover of shrubs, and total cover of field vegetation, during the initial inventory (2003– 2007) and the re-inventory (2008–2012) in the alpine area and in the mountain birch forest. Further refinement of this graph for the publication shows estimates of cover with a 95% confidence interval and a separation of cover types to unlink y-axis values in the figure. 7.3 Blueberry and mountain violet occurrence in the alpine region Following with NILS data we will now illustrate another example, this time from the report Skog &amp; Mark 2015 (Naturvårdsverket 2015). In this example we reproduce the analys done in the first chapter with the aim to compare presence of two species at different altitudes in the mountain range. In this case we query the endpoint ‘SmaprovytaArter’ that contains the occurrence of different species per sample area. urlApi &lt;- &#39;https://landskap.slu.se/api/nils/api/&#39; endpoint &lt;- &#39;SmaprovytaArter&#39; years &lt;- paste0(&#39;Ar=&#39;,c(2003:2012), collapse = &#39;&amp;&#39;) region &lt;- &#39;BioGeoRegion=Alpin&#39; pyUrl &lt;- url(glue(&#39;{urlApi}{endpoint}?{years}&amp;{region}&#39;)) pyGet &lt;- fromJSON(pyUrl) py &lt;- pyGet$data |&gt; mutate(uuid = glue(&#39;{rutaNummer}-{provytaNummer}&#39;)) |&gt; mutate_at(vars(uuid, vetenskapligtNamn), list(factor)) pySpp &lt;- py |&gt; group_by(uuid, ar) |&gt; reframe(coordNS = unique(koordNS), coordEW = unique(koordEW), presBB = ifelse(&quot;Vaccinium myrtillus&quot; %in% vetenskapligtNamn, 1, 0), presAV = ifelse(&quot;Viola biflora&quot; %in% vetenskapligtNamn, 1, 0)) |&gt; as.data.frame() In the same way we query NILS, there are many other sources of data. We query opentopodata.org the obtain the altitude at specific coordinates. But first we need to convert NILS sample areas coordinates into a coordinate system that this new service can understand (WGS84). And we want to make the query only one per sample area, as they wont go anywhere between sampling events. pySf &lt;- py |&gt; group_by(uuid) |&gt; reframe(coordNS = unique(koordNS), coordEW = unique(koordEW)) |&gt; rowwise() |&gt; mutate(geom = st_sfc(list(st_point(c(coordEW, coordNS))), crs = st_crs(3006))) |&gt; # SWEREF99 TM st_as_sf() |&gt; st_transform(st_crs(4326)) # WGS84 pySf$elevation &lt;- NA locations &lt;- st_coordinates(pySf) |&gt; as.data.frame() |&gt; rename(&quot;latitude&quot; = Y, &quot;longitude&quot; = X) We also want to iterate the query as this service has a quota of max 100 locations per call. nLocations &lt;- nrow(locations) queryLimit &lt;- 100 lChunks &lt;- split(seq(nLocations), ceiling(seq(nLocations)/queryLimit)) urlAlt &lt;- &quot;https://api.opentopodata.org/v1/&quot; endpoint &lt;- &quot;eudem25m&quot; for (l in seq(length(lChunks))) { loc &lt;- locations[lChunks[[l]],] |&gt; mutate(&quot;xy&quot; = glue(&quot;{round(latitude,5)},{round(longitude,5)}&quot;)) loc &lt;- paste0(loc$xy, collapse = &quot;|&quot;) elevUrl &lt;- url(glue(&quot;{urlAlt}{endpoint}?locations={loc}&quot;)) elevGet &lt;- fromJSON(elevUrl) elevation &lt;- elevGet$results pySf$elevation[lChunks[[l]]] &lt;- elevation$elevation } We put everything together (join) and summarise the proportion of sample areas per inventory phase and elevation group. pySf$elevationBin &lt;- cut(pySf$elevation, breaks = c(seq(300,1000, 50),Inf), include.lowest = TRUE, right = FALSE) elevBinLabel &lt;- c(&quot;300-349&quot;,&quot;350-399&quot;,&quot;400-449&quot;,&quot;450-499&quot;,&quot;500-549&quot;,&quot;550-599&quot;, &quot;600-649&quot;,&quot;650-699&quot;,&quot;700-749&quot;,&quot;750-799&quot;,&quot;800-849&quot;,&quot;850-899&quot;, &quot;900-949&quot;,&quot;950-999&quot;,&quot;&gt;=1000&quot;) pySpp$invPha &lt;- ifelse(as.numeric(as.character(pySpp$ar)) &gt; 2007, &quot;2008-2012&quot;, &quot;2003-2007&quot;) pySum &lt;- pySpp |&gt; left_join(st_drop_geometry(pySf)) |&gt; group_by(invPha, elevationBin) |&gt; reframe(count = n(), sumPresBB = sum(presBB), sumPresAV = sum(presAV)) |&gt; mutate(occBB = sumPresBB/count*100, occAV = sumPresAV/count*100) and finally, we create a beautiful plot. ggplot(pySum, aes(x = elevationBin, y = variable)) + geom_point(aes(y = occBB, col = invPha), position = position_dodge(0.1), shape = 19) + geom_point(aes(y = occAV, col = invPha), position = position_dodge(0.1), shape = 15) + labs(title = &quot;Occurrence of two species in the mountains&quot;, y = &quot;Occurrence (%)&quot;, x = &quot;Altitude (m.a.s.l.)&quot;, color = &quot;Inventory phase&quot;) + ### add a dummy dataset for the shape legend geom_point(aes(shape = rep(c(&#39;Blueberry&#39;,&#39;Alpine yellow-violet&#39;), 15), y = occBB), colour = NA) + guides(color = guide_legend(override.aes = list(shape = 15, size = 2)), shape = guide_legend(&quot;Species&quot;, override.aes = list(shape = c(15,19), color = &quot;grey&quot;, size = 2))) + ylim(0, 100) + scale_x_discrete(labels = elevBinLabel) + theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) The resulting graphs show the percentage of NILS sample areas with blueberries (Vaccinium myrtillus) or alpine yellow-violet (Viola biflora) in the mountains at different altitudes for both inventory phases (2003–2007, 2008–2012). References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
