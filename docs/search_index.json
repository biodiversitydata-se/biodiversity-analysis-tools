[["index.html", " A general workflow for analysis of primary biodiversity data Chapter 1 Introduction", " A general workflow for analysis of primary biodiversity data Debora Arlt, Alejandro Ruete and Charles Campbell for the Swedish Biodiversity Data Infrastructure 2024-03-28 Chapter 1 Introduction Biodiversity resources are increasingly international. The SBDI has made an effort to canalize biodiversity data and resources to help the research community access and analyze Swedish primary biodiversity data. Each research question draws its own challenges which are unique in themselves. Our aim here is to provide a general workflow that hopefully answers or at least prompts the questions that should be asked at each stage of the process. We point to resources, methods and facilities that may be useful in answering a particular question. We assume some knowledge of statistical inference and its limitations. The validity and appropriateness of a particular method dependent on the individual researcher(s). This workflow is focused on the statistical programming language R, as an environment where the complete analysis workflow can be documented in a fully reproducible way. However, we also point to other tools that can be used at different stages, and ways to import and export the data from and to those tools. We focus on biodiversity data and resources from Sweden but our aim is to present considerations and methods that can be applied beyond Sweden’s borders. The general workflow proposed is: Data –&gt; Cleaning –&gt; Fitness evaluation –&gt; Analysis always exploring and filtering the data in light of the research question. "],["the-importance-of-questions-and-sources-of-data.html", "Chapter 2 The importance of questions and sources of data 2.1 Questions 2.2 Taxonomies 2.3 Data Sources", " Chapter 2 The importance of questions and sources of data 2.1 Questions Any question to be asked of biodiversity data should be put as simply and succinctly as possible. With the number of different subject areas and techniques used, analyses can quickly become complex. 2.2 Taxonomies It is important to be aware of likely taxonomic anomalies prior to working within a region. Check-lists are very important, especially if working over several regions / countries. Whilst there are many things that will automatically look for the validity of a name they do not check for the validity of that species occurrence. For example Sphagnum auriculatum and S. denticulatum are both valid names. S. auriculatum is the currently accepted species in Europe but in the British Isles, Ireland and the Netherlands s. denticulatum is the most recorded taxa. Using data from across the European region without acknowledging this disagreement would impact the results of any research undertaken. For taxa which are known to be capable of dispersing great distances (eg birds) this becomes even more difficult especially when using community sourced data. For Sweden there is an agreed taxonomy for species accessible through dyntaxa and the R library dyntaxa. 2.3 Data Sources Depending on what questions are being asked there are many different resources available. We focus on biodiversity data 2.3.1 Biodiversity record data There are a large number of available on-line resources. These include but are not limited to (specific R libraries that connect to these databases are provided in bold): Swedish Biodiversity Data Infrastructure - Sweden’s data portal for biodiversity data Global Biodiversity Information Facility - International organization aggregating biodiversity data. Contains data from a mixture of sources; curated collections, community science data, ecological research projects etc. rgbif, spocc BioCASE - A European transnational biodiversity repository eBird - American database of bird observations auk, rebird,spocc iNaturalist - International community science observation repository spocc Berkeley ecoengine - Access to UC Berkley’s Natural history data spocc VertNet - vertebrate biodiversity collections rvert, spocc iDigBio - Integrated digitise biodiversity collections ridigbio OBIS - Ocean biodiversity information system robis ALA - Atlas of living Australia galah Neotoma Palaeoecology databas neotoma … 2.3.2 Taxonomic diversity To keep track of ever changing taxonomy of species there are different databases that follow different standard. dyntaxa GBIF backbone … 2.3.3 Functional diversity Very broadly functional diversity is the diversity of what organisms do (Petchey and Gaston 2006). Such diversity can be direct physical measurements of traits of the organisms involved and / or data summarized from published works. There are databases dedicated to the distribution of scientific data that may be used. Such resources include: Dryad TRY: Plant Trait Database Ecological indicator and traits values for Swedish vascular plants(Tyler et al. 2021) [https://doi.org/10.1016/j.ecolind.2020.106923] Vertnet … 2.3.4 Genetic data bases Genetic data may be related directly to the samples used, phylogenetic trees generated from some other data set set or some other genetic aspect. Such resources include: BOLD: Barcode Of Life Data system Repository of gene sequences Genbank A genetic sequence repository Treebase A data base of phylogenetic trees … 2.3.5 Environemntal data There are a number of environmental repositories available. Static data sets for global resources include: 2.3.5.1 Sweden Skogsstyrelsen - Open data supplied by Skogsstyrelsen Sveriges dataportal - Swedish open data provided by governmental bodies. Miljödata MVM - Environmental data from SLU for Sweden 2.3.5.2 Global and European wordclim - Global gridded climate data at various resolutions climond - Global gridded climate data at various resolutions soilgrids - Global gridded soil data Copernicus Land monitoring service - Land monitoring data from across the EU References "],["data-cleaning.html", "Chapter 3 Data cleaning 3.1 Resources 3.2 Taxonomies 3.3 Location data 3.4 Examples", " Chapter 3 Data cleaning Biodiversity data repositories work hard to maintain the accuracy of their holdings. When multiple sources are involved several problems may arise. Here we shall quickly outline what they are and possible solutions. 3.1 Resources There are a number of libraries, work flows and online resources for automating downloading and cleaning of data. THese include: The BDverse. A group of libraries for cleaning biodiversity data Kurator Wallace 3.2 Taxonomies It is important to be aware of likely taxonomic anomalies prior to working within a region. Checklists are very important, especially if working over several regions / countries. Whilst there are many things that will automatically look for the validity of a name they do not check for the validity of that species occurrence. For example Sphagnum auriculatum Schimp. and Sphagnum denticulatum Bridel, 1826 are both valid names. S. auriculatum is the currently accepted species in Europe but in the British Isles, Ireland and the Netherlands s. denticulatum is the most recorded taxa. Both are legitimate names but they are a synonymy. Both names have been used in Europe but in distinct countries. The current European checklist (Hodgetts et al. 2020) has Sphagnum auriculatum Schimp. as the accepted taxon occurring in Europe. Naive downloading would result in two taxa being present when in fact it is two interpretations of the same taxon. Using data from across the European region without acknowledging this disagreement would impact the results of any research undertaken. For taxa which are known to be capable of dispersing great distances (eg. birds) this becomes even more difficult especially when using community sourced data. The example above uses data downloaded from GBIF. Data for Sphagnum denticulatum doi: 10.15468/dl.rrp4p4 Data for Sphagnum auriculatum doi: 10.15468/dl.3yrtw7 Within Sweden there is an agreed taxonomy for all extant taxa accessible through Dyntaxa. Checking of species lists can be done by directly copying species names into a dialogue box or uploading an excel spreadsheet in the correct format. 3.3 Location data 3.3.1 Locality information Many records have locality information attached to them. Where there are no coordinates attached this information can be used to locate the record to within an area of where records most likely came from. There are functions for which geocoding can be done automatically within R. Geocoding is finding the coordinate for a known locality. These are included in the libraries: ggmap requires google API key tidygeocoder Many localities may not be included in the gazetteers associated with these libraries but may be located using online or printed maps. This takes time but may be useful. Unique localities may be extracted, geocoded and then merged back with the data set. This becomes especially important when extracting records across country boundaries as countries have different numbers of georeferenced observations. Table 3.1: Table 3.2: The number and percentage of records of bryophyta per EU country with and without coordinates Country Total Records % Records with coordinate % Records without coordinates AL Albania 385 6.23 6.23 AD Andorra 2408 6.85 6.85 AM Armenia 503 54.67 54.67 AT Austria 68000 20.78 20.78 BY Belarus 2102 20.65 20.65 BE Belgium 470192 6.73 6.73 BA Bosnia &amp; Herzegovina 415 71.33 71.33 BG Bulgaria 1349 37.21 37.21 HR Croatia 1060 49.81 49.81 CY Cyprus 595 61.18 61.18 CZ Czechia 19910 48.49 48.49 DK Denmark 949565 1.24 1.24 EE Estonia 109502 34.48 34.48 FO Faroe Islands 1618 47.10 47.10 FI Finland 256141 5.35 5.35 FR France 1298606 6.27 6.27 GE Georgia 9680 36.94 36.94 DE Germany 213246 18.91 18.91 GI Gibraltar 22 100.00 100.00 GR Greece 2183 65.23 65.23 HU Hungary 4115 58.13 58.13 IS Iceland 43033 15.07 15.07 IE Ireland 132611 2.77 2.77 IT Italy 22635 44.66 44.66 LV Latvia 7596 42.96 42.96 LI Liechtenstein 546 3.48 3.48 LT Lithuania 5431 6.52 6.52 LU Luxembourg 23640 1.61 1.61 MT Malta 95 40.00 40.00 MC Monaco 16 87.50 87.50 NL Netherlands 886075 4.02 4.02 MK North Macedonia 284 24.65 24.65 NO Norway 526305 29.36 29.36 PL Poland 52850 31.81 31.81 PT Portugal 256568 9.93 9.93 RO Romania 1501 58.69 58.69 RU Russia 217991 15.96 15.96 SM San Marino 1 0.00 0.00 SK Slovakia 5047 51.46 51.46 SI Slovenia 3991 37.31 37.31 ES Spain 130037 20.33 20.33 SE Sweden 1205171 10.79 10.79 CH Switzerland 267715 4.51 4.51 TR Turkey 9274 85.00 85.00 UA Ukraine 13225 8.81 8.81 GB United Kingdom 2976217 1.44 1.44 VA Vatican City 3 100.00 100.00 As can be seen from the above example for European Bryophyta (Hornworts, Liverworts and Mosses) the number of records with coordinates vastly varies between countries. For example of 959444 records in Sweden 88.24 % have coordinates in Switzerland of 117132 records only 2.29 % have coordinates. 3.3.2 Coordinate uncertainty In many cases there is now an abundance of biodiversity data with coordinates. As can be seen from a summary of the above Sphagnum data coordinate uncertainty can vary from less than 1 meter to multiple kilometers. Coordinate uncertainty &lt;1 m 1-10 m 10-100 m 100-500 m 500-1000 m 1-5 km 5-10 km 10-50 km &gt;50 km None Frequency 335 586 5320 376 4619 4980 4147 41 2 7506 In the above example it can be seen that two have uncertainties greater than 50 km and several thousand records that have no known error margin the location. It is important to consider what the error is and removing those records for which the uncertainty is too high. Where this point is will be dependent on the scale of the research. 3.3.3 Coordinate errors Besides the declared uncertainty of the the coordinates, coordinate errors may occur for a variety of reasons. The library coordinateCleaner is very useful for removing some of the most common errors. These include: 0 latitude, 0 longitude swapping of latitude and longitude The location of the institution holding a preserved sample rather than the location of origin of that sample 3.4 Examples 3.4.1 Data cleaning Cyperaceae in Sweden In this example I have downloaded the occurrences of the Cyperaceae family in Sweden from SBDI since 2000. This is a single data set from an area with an agreed taxonomy dyntaxa. Our aim here is to download a group of records and clean them to be in such a state than they reflect the question the researcher has in mind. First we give each record in the assembled data set a unique ID. In this case it could be possible to use the GBIFid. In data sets sourced from multiple sources it is a good idea to create a unique ID related to the data source. In cleaning data there are a number of dimensions of uncertainty we shall first check. 3.4.2 Taxonomy In the species column there are records with no Species given in the species column. We then check the scientificName for whether there is useful information there kable(Carex %&gt;% filter(species == &quot;&quot;) %&gt;% distinct(scientificName)) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;), full_width = F) scientificName Carex divulsa subsp. leersii (F.W.Schultz) W.Koch Carex L. Carex saxatilis var. saxatilis Carex fuliginosa subsp. misandra (R.Br.) Nyman Carex bigelowii subsp. rigida (Raf.) W.Schultze-Motel Carex buxbaumii subsp. mutica (Hartm.) Isov. Carex buxbaumii f. buxbaumii Carex norvegica subsp. inferalpina (Wahlenb.) Hultén Carex diandra var. major (W.D.J.Koch) Boott Carex buxbaumii var. mutica Hartm. Carex cognata var. congolensis (Turrill) Lye Carex saxatilis subsp. laxa (Trautv.) Kalela In this case there are only genre recorded. We then check for data about species in the locality information for which there are no species data given in the species column. Here there are enough unique values to be checked easily within R. It is possible that in large data sets that there may be very many records for which there is useful information There are no species data in the locality field and so we begin a vector of IDs that are not at the required data resolution. unUseful &lt;- Carex %&gt;% filter(Carex$scientificName == &quot;&quot;) %&gt;% .$id We now need to check that the species names in the records that we are using are valid for the area which we are looking at i.e. Sweden. There are several resources out there that do this. As we are looking specifically and solely at Sweden there is Dyntaxa.se. There is also an R package as part of the Swedish Biodiversity Data Infrastructure with examples and documentation. For wider applicability we will export the unique names from the Carex data frame. These will then be copied to the Dyntaxa portal for matching multiple names. The results are checked against the Swedish lists and unknown species or uncertainties are flagged. Names can be copied directly into a box in Dyntaxa or may be imported directly as an xlsx file. Here we shall export just the species names. There are multiple options. SpeciesNames &lt;- Carex %&gt;% filter(!id %in% unUseful) %&gt;% dplyr::select(&quot;scientificName&quot;) %&gt;% distinct() # library(openxlsx) #write.xlsx(SpeciesNames,&quot;./data/BDcleaner_Scripts/Example/UniqueCarexTaxa.xlsx&quot;) Dyntaxa provides options for any taxononmic uncertainties and lists the species for which no match can be made. Having made selections it exports as a .xlslx file. We can then check what those species for which there is no information about what they are. In this case there are an number of species which are likely to be horticultural, a few taxa which are of hybrid origin which are in the Dyntaxa with the addition of ‘×’ between genus and species, and Carex utriculata a species from North America. This last could be a misidentification of C. rostrata. We will exclude the horticultural and uncertain species. We do this by adding a Species column in the Dyntaxa file and then load that file into R. library(openxlsx) SpeciesNames &lt;- read.xlsx(&quot;./data/matchCarex.xlsx&quot;) # From Dyntaxa Carex &lt;- merge(Carex, SpeciesNames[,c(&quot;Provided.string&quot;,&quot;SpeciesDyn&quot;)], by.x = &quot;species&quot;,by.y = &quot;Provided.string&quot;, all.x = TRUE) We then add IDs of the rows with taxa which are to be excluded to the unUseful vector. It is possible that there is overlap between these vectors. Rather than overwriting the vector we join the vectors together and use unique to get a vector with no duplications. unUseful &lt;- unique( c(unUseful, Carex %&gt;% filter(is.na(SpeciesDyn)) %&gt;% .$id)) ###Coordinate Cleaning We need to extract locality data for rows where there is no lat/lon information to geocode it, producing a latitude and longitude for each locality. This can be carried out in a similar way as for species ie the merging of data frames. This can be also be done automatically in R using the geocode feature of the libraries tidygeocoder or ggmap. Some of these services require an API key (eg. Google). See the documentation for the link for more information Localities &lt;- Carex %&gt;% filter(is.na(longitude)) %&gt;% dplyr::select(locality) %&gt;% distinct() write.csv(Localities,&quot;./data/Localities.csv&quot;,row.names = FALSE) For brevity we will simply include the records with no lat/lon info in the unUseful vector. These will ultimately not be used. unUseful &lt;- unique(c(unUseful, Carex %&gt;% filter(is.na(latitude))%&gt;% .$id)) Before cleaning the coordinates we simplify the data retaining columns that have information directly related to the collection of the data. Carex &lt;- Carex %&gt;% dplyr::select(SpeciesDyn, longitude, latitude, coordinateUncertaintyInMetres, locality, # recordedBy, # countryCode, # eventDate, year, month, day, institutionCode, collectionCode, catalogueNumber) We then filter out the data rows for which we can not use ie those of too great taxonomic uncertainty. The data may then be cleaned. There are a number of things to consider when cleaning data: How precise are the locations? Coordinate uncertainty ranges in the Cyperaceae of Sweden from 1 m - 30.5 km. Are interpreted coordinates sufficient? These interpreted coordinates may imply a greater precision than is necessarily true Are the locations likely to be errors? Errors can relate to where a sample is (eg. herbarium location) rather than where it came from. Are the locations in the country claimed? This can arise through swapping of lat and lon; duplication of latitude numbers in the longitude; or simply incorrect coordinates being given. unUseful &lt;- unique( c(unUseful, Carex %&gt;% filter(is.na(coordinateUncertaintyInMetres)) %&gt;% .$id)) # kable( t(table( round( Carex$coordinateUncertaintyInMetres, -2)))) We shall filter out all records with unknown coordinate uncertainty and all with a coordinate uncertainty more than 12.5 km. unUseful &lt;- unique( c(unUseful, Carex %&gt;% filter(is.na(coordinateUncertaintyInMetres))%&gt;% .$id)) unUseful &lt;- unique( c(unUseful, Carex %&gt;% filter(coordinateUncertaintyInMetres &gt; 12500)%&gt;% .$id)) We now have an index of records we can’t use owing to incomplete taxonomy or incomplete location information. We save these before cleaning the coordinates. save(unUseful, file = &quot;./data/Cyperacea_SWE_unsuseful.rdata&quot;) We first remove the already labeled inaccurate data. Carex &lt;- Carex[-which(Carex$id %in% unUseful),] We then use the library CoordinateCleaner to automatically flag coordinates that may be errors. Outputs of this can be a cleaned data.frame or additional columns with doubtful records flagged. Carex$countryCode &lt;- countrycode::countrycode(Carex$countryCode,&quot;iso2c&quot;,&quot;iso3c&quot;) #converts ISO2 country codes to ISO3 Carex &lt;- Carex %&gt;% filter(!id %in% unUseful) Carex &lt;- Carex[-which(is.na(Carex$latitude)),] Carex &lt;- clean_coordinates(Carex, lon = &quot;longitude&quot;, lat = &quot;latitude&quot;, species = &quot;SpeciesDyn&quot;, countries = &quot;countryCode&quot;, tests = c(&quot;capitals&quot;, &quot;centroids&quot;, &quot;equal&quot;, &quot;gbif&quot;, &quot;institutions&quot;, &quot;outliers&quot;, &quot;seas&quot;, &quot;zeros&quot;,&quot;countries&quot;) ) kable( Carex %&gt;% summarise(`Invalid coords` = sum(.val==FALSE), `Equal coords` = sum(.equ==FALSE), `0 coords` = sum(.zer==FALSE), `capitals` = sum(.cap==FALSE), `country centre` = sum(.cen==FALSE), `Country Border` = sum(.con==FALSE), `outlier` = sum(.otl==FALSE), `Gbif HQ` = sum(.gbf==FALSE), `Insitution` = sum(.inst==FALSE), `Summary` = sum(.summary==FALSE)), col.names = c(&quot;Invalid coords&quot;,&quot;Equal coords&quot;,&quot;0 coords&quot;,&quot;capitals&quot;, &quot;country centre&quot;,&quot;Country Border&quot;,&quot;outlier&quot;,&quot;Gbif HQ&quot;, &quot;Insitution&quot;, &quot;Summary&quot;)) Checking the distribution of the observations Sweden &lt;- raster::getData(&quot;GADM&quot;, country = &quot;SWE&quot;, path = &quot;./data/&quot;, level = 0) Sweden_sf &lt;- st_as_sf(Sweden) BolMar &lt;- ggplot(Sweden_sf) + geom_sf()+ geom_point(data = Carex %&gt;% filter(SpeciesDyn == &quot;Carex capillaris&quot;),# %&gt;% # filter(.con == FALSE), aes(x = longitude, y = latitude)) + theme_cowplot() + ggtitle(&quot;Carex capillaris&quot;) CarAct &lt;- ggplot(Sweden_sf) + geom_sf()+ geom_point(data = Carex %&gt;% filter(SpeciesDyn == &quot;Carex acuta&quot;), # %&gt;% # filter(.otl == TRUE), aes(x = longitude, y = latitude)) + #, # colour = .otl))+ theme_cowplot() + ggtitle(&quot;Carex acuta&quot;) grid_Sweden &lt;- makeGrid(Sweden, 25) OB &lt;- organiseBirds(Carex, idCols = c(&quot;locality&quot;), xyCols = c(&quot;longitude&quot;, &quot;latitude&quot;), sppCol = &quot;SpeciesDyn&quot;) SB &lt;- summariseBirds(OB, grid_Sweden) grid_Sweden &lt;- st_as_sf(grid_Sweden) Sweden_sf &lt;- st_as_sf(Sweden) library(colorRamps) SB_SPat &lt;- st_as_sf(SB$spatial) nSpecies &lt;- ggplot(data = SB_SPat,aes(fill = nSpp))+ geom_sf()+ theme_cowplot()+ scale_fill_gradientn(colors = matlab.like2(100)) nVisits &lt;- ggplot(data = SB_SPat,aes(fill = nVis))+ geom_sf()+ theme_cowplot()+ scale_fill_gradientn(colors = matlab.like2(100)) nObs &lt;- ggplot(data = SB_SPat,aes(fill = nObs))+ geom_sf()+ theme_cowplot()+ scale_fill_gradientn(colors = matlab.like2(100)) plot_grid(nSpecies, nVisits, nObs, ncol = 1) References "],["fitness-for-use.html", "Chapter 4 Fitness for use 4.1 Bias", " Chapter 4 Fitness for use Knowledge of the quality of data available is very important. Quantifying gaps in data taxonomic, temporal and spatial is an important step. The R package BIRDS provides a resource to do all of these things. It can: Summarise spatial distribution of records; Summarise the temporal distribution of records; and Thorugh the implementation of ignorance scores, summarise likely gaps in biodiversity knowledge. library(BIRDS) data(&quot;bryophytaObs&quot;) data(&quot;gotaland&quot;) OB &lt;- organiseBirds(bryophytaObs) grid &lt;- makeGrid(gotaland, gridSize = 10) SB &lt;- summariseBirds(OB, grid) 4.1 Bias All data sets are biased to some degree. For field data, the structure of data gathering should be robust enough to reduce the effects of bias to a minimum. Data downloaded from biodiversity databases are by their nature from different sources. Such sources may have differing causes of bias and it is important to consider if and how such biases will affect research. Sources of bias may include: accessibility of the landscape; human population density; distribution of expertise; “Charisma” of the taxa; There is a large body of literature on different methods which may be employed. One of the simplest is the selection of background points with the same spatial bias as taxa which are searched for in the manner (Phillips et al. 2009). Such a layer can be produced using 2-dimensional kernel estimation through the MASS package library(raster) library(MASS) data(&quot;bryophytaObs&quot;) Sample_bias_layer &lt;- raster( MASS::kde2d( x = bryophytaObs$decimalLongitude, y = bryophytaObs$decimalLatitude, h = c(100,200), lims = c(10,20,55,60))) More complex methods of accounting for bias can be created by more explicitly modeling accessibility or the behavior of recorders themselves. There are resources for doing this, including: recorderMetrics - Data derived metrics of recorder behavior (August et al. 2020) Sampbias - Bayesian analysis to quantify the effects of accessibility on species occurrence sets (Zizka, Antonelli, and Silvestro 2020) Spatial sorting or dissaggregating presence points can reduce the effects of bias: ENMeval - Provides methods for quickly spatially aggregating presence points for model building; spThin - Randomly thins the number of presences used for a model by an agreed distance. THis References "],["data-exploration-and-transformation.html", "Chapter 5 Data Exploration and Transformation 5.1 Wide 5.2 Long 5.3 Wide to long 5.4 Long to Wide 5.5 Spatial data", " Chapter 5 Data Exploration and Transformation There are many different ways of arranging data, especially within the R environment. Two of the most frequently used are the wide and long formats. Here we show what the wide and long formats are and ways to move between the two. 5.1 Wide Wide format is where each row represents a data point and each column an attribute of that data point. In the example below each data point is a site, and each column is a species percentage cover. Table 5.1: Table 5.2: Example of wide format Site Species1 Species2 Species3 Species4 Species5 Site_1 67 42 96 6 88 Site_2 38 13 84 72 100 Site_3 0 81 20 78 36 Site_4 33 58 53 84 33 Site_5 86 50 73 36 88 5.2 Long Long format is where each row has a value of an attribute of the data point. Each row in the example below records what the value of a species is at a particular site. Table 5.3: Table 5.4: Example of long format Site Species cover Site_1 Species1 67 Site_2 Species1 38 Site_3 Species1 0 Site_4 Species1 33 Site_5 Species1 86 Site_1 Species2 42 There are a number of ways of moving between the two: 5.3 Wide to long library(reshape2) df_long &lt;- melt(data = df_wide, variable.name = &quot;Species&quot;, value.name = &quot;cover&quot;) library(tidyr) df_long &lt;- df_wide %&gt;% gather(key = Species, value = cover, Species1:Species5, # vector of columns to gather factor_key = FALSE) 5.4 Long to Wide library(reshape2) df_wide &lt;- dcast(data = df_long, Site ~ Species, value.var = &quot;cover&quot;) library(tidyr) df_wide &lt;- df_long %&gt;% spread(key = Species, value = cover, fill = 0) 5.5 Spatial data Spatial data have coordinates. IN the simplest form they are a set of points. They may be polygonal (both regular and irregular) depicting any sort of feature. They may have additional information attached to them such as a data frame. They may take the form of a grid and may be in the raster format. Very frequently they represent the land surface and as such may have coordinate reference information attached (eg. SWEREF 99 TM) There are several resources for handling such data: in R the most frequent base libraries encountered are: sp - Classes and methods for spatial data raster - Geographic data analysis and modeling sf - Simple features for R rgdal - Bindings for the ‘Geospatial’ Data Abstraction Library There are a number of other programs that are used for spatial GIS data. THese include: QGIS - Open source GIS ArcGIS - Esri ArcGIS Provided are the libraries and the appropriate functions for exporting spatial data from R 5.5.1 Spatial vector data library(sf) write_sf() #capable of writing multiple different file formats library(rgdal) writeOGR() #capable of writing multiple different file formats library(raster) shapefile() #writes ESRI shapefiles 5.5.2 Raster library(raster) writeRaster() #capable of writing multiple different file formats "],["essential-biodiversity-variables.html", "Chapter 6 Essential Biodiversity Variables 6.1 SDM 6.2 Diversity", " Chapter 6 Essential Biodiversity Variables 6.1 SDM rspatial 6.2 Diversity Biological diversity analyses typically use multivariate techniques to assess variation in data sets comprising sampling events and cases. A sampling event can be across time and space. In biodiversity analysis three cases are the most common: Taxonomic diversity (species); Functional diversity (biological form); and Genetic diversity (allellic frequency, phylogeny etc). Variation in these three dimensions can be directly compared both within and between these different dimensions of biological diversity. 6.2.1 Types of analyses - Patitioning \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) etc. diversity Two forms of diversity analyses are currently widely used; classic diversity measures (eg. species richness, Shannons diversity index etc.) and numbers equivalents representation of the underlying diversity distribution. This second form was first introduced in Hill (1973) and a number of resources are now available for computing both forms. See the forum piece Forum: Partitioning diversity in Ecology (2010) for a fuller discussion on the use of numbers equivalents. Both types of diversity partitioning are used for all types of analyses and we present resources which are available for both forms: - Resources for classic diversity measures - Resources for numbers equivalents. 6.2.2 Data form Within the R environment both methods require data to be in the wide format. See [link to page explaining shift from long to wide]. Table 6.1: Table 6.2: Example of wide format class_1 class_2 class_3 class_4 class_5 class_6 class_7 class_8 class_9 class_10 Site_1 68 97 89 74 44 2 81 13 73 93 Site_2 39 85 37 42 25 45 100 22 87 34 Site_3 1 21 34 38 70 18 13 93 83 10 Site_4 34 54 89 20 39 22 40 28 90 1 Site_5 87 74 44 28 51 78 89 48 48 43 Site_6 43 7 79 20 42 65 48 33 64 59 Site_7 14 73 33 44 6 70 89 45 94 26 Site_8 82 79 84 87 24 87 23 21 96 15 Site_9 59 85 35 70 32 70 84 31 60 58 Site_10 51 37 70 40 14 75 29 17 51 29 6.2.3 Classic diversity measures People have used many different indices to measure diversity. These include: 6.2.4 \\(\\alpha\\) diversity \\(\\alpha\\) diversity refers to the diversity at a single site. There are a number of different indices to caclculate the most common are: Species richness; Shannon/shannon weaver index; Simpson; Inverse Simpson; and Gini Simpson The R libraries vegan, adiv, abdiv all provide methods to calculate these measures as well as a wealth of others. Within abdiv the funtion alpha_diversity lists the \\(\\alpha\\) diversity measures available within the package. Whilst not exhaustive it is a large list. There are also ways of estimating \\(\\alpha\\) diversity through rarefaction as well as modeling and visualising its different aspects in both vegan and adiv. 6.2.5 \\(\\beta\\) diversity and dissimilarities The \\(\\beta\\) diversity is a measure of the change in composition and/or abundance between sites. There is a long history of methods to measure this particular aspect of diversity. This has resulted in multiple indices and dissimilarities. Commonly used indices include: Jaccard; Sørrensen; Bray-Curtis; Hellinger distance, Chord distance An extensive list of \\(\\beta\\) diversities are available through the function betadiver in the vegan package as well as beta_diversity in the abdiv package. Methods for analysing \\(\\beta\\) diversity are included in the libraries betapart, vegan, adiv and ade4. 6.2.6 Additional related diversities Measures of dark diversity (those species that are absent from an ecosystem but which belong to its species pool, (Pärtel, Szava-Kovats, and Zobel 2011)) and \\(\\zeta\\) diversity (a measure of species pool overlap, (Hui and McGeoch 2014)) have been developed. These measures are instituted in the libraries: DarkDiv dark diversity; and zetadiv zeta diversity 6.2.7 Numbers equivalents Numbers equivalents account for the nested heirarchies in \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) diversity. A number of different libraries have now been developed: Hill numbers: Hill (1973) adiv HillR iNext Tsallis entropy: Marcon and Hérault (2015) adiv entropart Crossing point theory: Patil and Taillie (1982) BioFTF 6.2.8 Example of analysis of diversity In this example we use the sampling event “Vegetation data from sheep grazing experiment at alpine site in Hol, Norway” available here. Downloading the Darwin core archive which contains a … TO BE DONE… 6.2.8.1 Event data 6.2.8.2 Occurence data References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
