"package","vignette","dependancies","updatesN","lastUpdate","description"
"hypervolume",0,4,23,"2018-07-16 00:20","Estimates the shape and volume of high-dimensional datasets and performs set operations: intersection / overlap, union, unique components, inclusion test, and hole detection. Uses stochastic geometry approach to high-dimensional kernel density estimation, support vector machine delineation, and convex hull generation. Applications include modeling trait and niche hypervolumes and species distribution modeling."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"EHRtemporalVariability",1,1,7,"2020-05-21 14:50","Functions to delineate temporal dataset shifts in Electronic Health              Records through the projection and visualization of dissimilarities              among data temporal batches. This is done through the estimation of              data statistical distributions over time and their projection in              non-parametric statistical manifolds, uncovering the patterns of the              data latent temporal variability. 'EHRtemporalVariability' is              particularly suitable for multi-modal data and categorical variables              with a high number of values, common features of biomedical data where              traditional statistical process control or time-series methods may not              be appropriate. 'EHRtemporalVariability' allows you to explore and              identify dataset shifts through visual analytics formats such as              Data Temporal heatmaps and Information Geometric Temporal (IGT) plots.              An additional 'EHRtemporalVariability' Shiny app can be used to load              and explore the package results and even to allow the use of these              functions to those users non-experienced in R coding. Preprint published              in medRxiv (Sáez et al. 2020) <doi:10.1101/2020.04.07.20056564>."
"diceR",1,1,11,"2019-07-25 22:30","Performs cluster analysis using an ensemble    clustering framework, Chiu & Talhouk (2018)    <doi:10.1186/s12859-017-1996-y>.  Results from a diverse set of    algorithms are pooled together using methods such as majority voting,    K-Modes, LinkCluE, and CSPA. There are options to compare cluster    assignments across algorithms using internal and external indices,    visualizations such as heatmaps, and significance testing for the    existence of clusters."
"stream",2,2,21,"2019-06-08 06:50","A framework for data stream modeling and associated data mining tasks such as clustering and classification. The development of this package was supported in part by NSF IIS-0948893 and NIH R21HG005912. Hahsler et al (2017) <doi:10.18637/jss.v076.i14>."
"ParBayesianOptimization",3,1,7,"2020-02-24 21:10","Fast, flexible framework for implementing Bayesian optimization of model 	hyperparameters according to the methods described in Snoek et al. <arXiv:1206.2944>.	The package allows the user to run scoring function in parallel, save intermediary 	results, and tweak other aspects of the process to fully utilize the computing resources	available to the user."
"funtimes",0,1,10,"2019-02-17 23:30","Includes non-parametric estimators and tests for time series analysis. The functions are to test for presence of possibly non-monotonic trends and for synchronism of trends in multiple time series, using modern bootstrap techniques and robust non-parametric difference-based estimators."
"eventstream",0,1,1,"2019-12-06","Implements event extraction and early classification of events in data streams in R.     It has the functionality to generate 2-dimensional data streams with events belonging to     2 classes. These events can be extracted and features computed. The event features extracted    from incomplete-events can be classified using a partial-observations-classifier     (Kandanaarachchi et al. 2018) <doi:10.13140/RG.2.2.10051.25129>. "
"dPCP",1,1,1,"2020-09-25","The automated clustering and quantification of the digital PCR            data is based on the combination of 'DBSCAN' (Hahsler et al. (2019)            <doi:10.18637/jss.v091.i01>) and 'c-means' (Bezdek et l. (1981)             <doi:10.1007/978-1-4757-0450-1>) algorithms.            The analysis is independent of multiplexing geometry, dPCR system,             and input amount.            The details about input data and parameters are available in the            vignette. "
"DDoutlier",0,1,1,"2018-05-30","Outlier detection in multidimensional domains. Implementation of notable distance and density-based outlier algorithms. Allows users to identify local outliers by comparing observations to their nearest neighbors, reverse nearest neighbors, shared neighbors or natural neighbors. For distance-based approaches, see Knorr, M., & Ng, R. T. (1997) <doi:10.1145/782010.782021>, Angiulli, F., & Pizzuti, C. (2002) <doi:10.1007/3-540-45681-3_2>, Hautamaki, V., & Ismo, K. (2004) <doi:10.1109/ICPR.2004.1334558> and Zhang, K., Hutter, M. & Jin, H. (2009) <doi:10.1007/978-3-642-01307-2_84>. For density-based approaches, see Tang, J., Chen, Z., Fu, A. W. C., & Cheung, D. W. (2002) <doi:10.1007/3-540-47887-6_53>, Jin, W., Tung, A. K. H., Han, J., & Wang, W. (2006) <doi:10.1007/11731139_68>, Schubert, E., Zimek, A. & Kriegel, H-P. (2014) <doi:10.1137/1.9781611973440.63>, Latecki, L., Lazarevic, A. & Prokrajac, D. (2007) <doi:10.1007/978-3-540-73499-4_6>, Papadimitriou, S., Gibbons, P. B., & Faloutsos, C. (2003) <doi:10.1109/ICDE.2003.1260802>, Breunig, M. M., Kriegel, H.-P., Ng, R. T., & Sander, J. (2000) <doi:10.1145/342009.335388>, Kriegel, H.-P., Kröger, P., Schubert, E., & Zimek, A. (2009) <doi:10.1145/1645953.1646195>, Zhu, Q., Feng, Ji. & Huang, J. (2016) <doi:10.1016/j.patrec.2016.05.007>, Huang, J., Zhu, Q., Yang, L. & Feng, J. (2015) <doi:10.1016/j.knosys.2015.10.014>, Tang, B. & Haibo, He. (2017) <doi:10.1016/j.neucom.2017.02.039> and Gao, J., Hu, W., Zhang, X. & Wu, Ou. (2011) <doi:10.1007/978-3-642-20847-8_23>."
"cordillera",0,1,2,"2017-07-25 00:07","Functions for calculating the OPTICS Cordillera. The OPTICS Cordillera measures the amount of 'clusteredness' in a numeric data matrix within a distance-density based framework for a given minimum number of points comprising a cluster, as described in Rusch, Hornik, Mair (2017) <doi:10.1080/10618600.2017.1349664>. There is an R native version and a version that uses 'ELKI', with methods for printing, summarizing, and plotting the result. There also is an interface to the reference implementation of OPTICS in 'ELKI'."
"Biopeak",1,1,1,"2019-08-21","Enables the user to systematically identify and visualize impulse-like gene expression changes within short genomic series experiments. In order to detect such activation peaks, the gene expression is treated as a signal that propagates along an experimental axis (time, temperature or other series conditions). Peaks are selected by exhaustive identification of local maximums and subsequent filtering based on a range of controllable parameters. Moreover, the 'Biopeak' package provides a series of data exploration tools including: expression profile plots, correlation heat maps and clustering functionalities."
"ktaucenters",0,1,1,"2019-08-03","A clustering algorithm similar to K-Means is implemented, it has two main advantages,     namely (a) The estimator is resistant to outliers, that means that results of estimator are still correct when    there are atypical values in the sample and (b) The estimator is efficient, roughly speaking,     if there are no outliers in the sample, results will be similar than those obtained by a classic algorithm (K-Means).    Clustering procedure is carried out by minimizing the overall robust scale so-called tau scale.    (see Gonzalez, Yohai and Zamar (2019) <arxiv:1906.08198>).  "
"dataone",7,1,8,"2020-01-31 14:20","Provides read and write access to data and metadata from    the DataONE network <https://www.dataone.org> of data repositories.      Each DataONE repository implements a consistent repository application     programming interface. Users call methods in R to access these remote     repository functions, such as methods to query the metadata catalog, get     access to metadata for particular data packages, and read the data objects     from the data repository. Users can also insert and update data objects on     repositories that support these methods."
"bsts",0,3,14,"2019-09-22 19:40","Time series regression using dynamic linear models fit using  MCMC. See Scott and Varian (2014) <doi:10.1504/IJMMNO.2014.059942>, among many  other sources."
"bsts",0,3,14,"2019-09-22 19:40","Time series regression using dynamic linear models fit using  MCMC. See Scott and Varian (2014) <doi:10.1504/IJMMNO.2014.059942>, among many  other sources."
"stagedtrees",0,1,3,"2020-01-12 16:40","Creates and fits staged event tree probability models.              Staged event trees are probabilistic graphical models capable of              representing asymmetric conditional independence statements             among categorical variables.              This package contains functions to create, plot and fit              staged event trees from data, moreover different structure learning              algorithms are available.             References:             Collazo R. A., Görgen C. and Smith J. Q.              (2018, ISBN:9781498729604).             Görgen C., Bigatti A., Riccomagno E. and Smith J. Q. (2018)              <arXiv:1705.09457>.             Thwaites P. A., Smith, J. Q. (2017) <arXiv:1510.00186>.             Barclay L. M., Hutton J. L. and Smith J. Q. (2013)              <doi:10.1016/j.ijar.2013.05.006>.             Smith J. Q. and Anderson P. E. (2008)              <doi:10.1016/j.artint.2007.05.004>."
"ParallelPC",0,1,3,"2015-10-03 01:08","Parallelise constraint based causality discovery and causal inference methods. The parallelised algorithms in the package will generate the same results as that of the 'pcalg' package but will be much more efficient. "
"OGI",0,1,1,"2017-12-20","Consider a data matrix of n individuals with p variates. The objective general index (OGI)    is a general index that combines the p variates into a univariate index in order to rank the n    individuals. The OGI is always positively correlated with each of the variates.    More details can be found in Sei (2016) <doi:10.1016/j.jmva.2016.02.005>."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"BNDataGenerator",0,1,1,"2014-12-28","Data generator based on Bayesian network model"
"StratifiedBalancing",0,1,3,"2016-07-22 14:31","Performs Stratified Covariate Balancing with Markov blanket feature selection and use of synthetic cases. See Alemi et al. (2016) <doi:10.1111/1475-6773.12628>."
"SELF",0,1,2,"2017-11-13 13:03","Provides the SELF criteria to learn causal structure. Please cite ""Ruichu Cai, Jie Qiao, Zhenjie Zhang, Zhifeng Hao. SELF: Structural Equational Embedded Likelihood Framework for Causal Discovery. AAAI. 2018."""
"sdglinkage",4,1,1,"2020-04-27","A tool for synthetic data generation that can be used for linkage method     development, with elements of i) gold standard file with complete and accurate     information and ii) linkage files that are corrupted as we often see in raw dataset."
"r.blip",0,1,1,"2019-02-27","Allows the user to learn Bayesian networks from datasets containing thousands of variables. It focuses on score-based learning, mainly the 'BIC' and the 'BDeu' score functions. It provides state-of-the-art algorithms for the following tasks: (1) parent set identification - Mauro Scanagatta (2015) <http://papers.nips.cc/paper/5803-learning-bayesian-networks-with-thousands-of-variables>; (2) general structure optimization - Mauro Scanagatta (2018) <doi:10.1007/s10994-018-5701-9>, Mauro Scanagatta (2018) <http://proceedings.mlr.press/v73/scanagatta17a.html>; (3) bounded treewidth structure optimization - Mauro Scanagatta (2016) <http://papers.nips.cc/paper/6232-learning-treewidth-bounded-bayesian-networks-with-thousands-of-variables>; (4) structure learning on incomplete data sets - Mauro Scanagatta (2018) <doi:10.1016/j.ijar.2018.02.004>. Distributed under the LGPL-3 by IDSIA."
"pchc",0,1,2,"2020-07-06 18:30","Bayesian network learning using the PCHC algorithm. PCHC stands for PC Hill-Climbing. It is a new hybrid algorithm that used PC to construct the skeleton of the BN and then utilizes the Hill-Climbing greedy search. The relevant paper has been submitted and is currently in revision."
"MRPC",0,1,4,"2019-05-09 23:30","A PC Algorithm with the Principle of Mendelian Randomization. This package implements the MRPC             (PC with the principle of Mendelian randomization) algorithm to infer causal graphs. It also             contains functions to simulate data under a certain topology, to visualize a graph in different             ways, and to compare graphs and quantify the differences.             See Badsha and Fu (2019) <doi.org/10.3389/fgene.2019.00460>,Badsha, Martin and Fu (2018) <arXiv:1806.01899>. "
"MoTBFs",0,1,5,"2020-04-06 12:12","Learning, manipulation and  evaluation of mixtures of  truncated basis  functions   (MoTBFs),  which include mixtures of  polynomials (MOPs) and  mixtures of truncated   exponentials (MTEs). MoTBFs are a flexible framework for modelling hybrid Bayesian  networks (I. Pérez-Bernabé, A. Salmerón, H. Langseth (2015) <doi:10.1007/978-3-319-20807-7_36>; H. Langseth, T.D. Nielsen, I. Pérez-Bernabé, A. Salmerón (2014) <doi:10.1016/j.ijar.2013.09.012>; I. Pérez-Bernabé, A. Fernández, R. Rumí, A. Salmerón (2016) <doi:10.1007/s10618-015-0429-7>). The  package provides  functionality for learning  univariate, multivariate and  conditional  densities, with the  possibility of incorporating prior  knowledge. Structural  learning of hybrid Bayesian  networks is also provided. A set of useful tools is provided,  including  plotting, printing  and likelihood  evaluation. This  package  makes use of  S3   objects, with two new classes called 'motbf' and 'jointmotbf'."
"mDAG",0,1,4,"2019-03-13 19:20","Learning a mixed directed acyclic graph based on both continuous and categorical data."
"hmma",1,1,2,"2020-04-08 18:00","Asymmetric hidden Markov model (HMM-A) learning.   HMM-As are similar to regular HMMs, but use Bayesian Networks (BNs) in    their emission distribution. The HMM-As can therefore offer more problem    insight [Bueno et al. 2017, <doi:10.1016/j.ijar.2017.05.011>]."
"dbnR",0,1,3,"2020-03-25 14:50","Learning and inference over dynamic Bayesian networks of arbitrary     Markovian order.  Extends some of the functionality offered by the 'bnlearn'     package to learn the networks from data and perform exact inference.     It offers a modification of Trabelsi (2013) <doi:10.1007/978-3-642-41398-8_34>     dynamic max-min hill climbing algorithm for structure learning and     the possibility to perform forecasts of arbitrary length. A tool for     visualizing the structure of the net is also provided via the 'visNetwork' package."
"CovSelHigh",0,1,3,"2017-05-04 17:40","Model-free selection of covariates in high dimensions under unconfoundedness for situations where the parameter of interest is an average causal effect. This package is based on  model-free backward elimination algorithms proposed in de Luna, Waernbaum and Richardson (2011) <doi:10.1093/biomet/asr041> and VanderWeele and Shpitser (2011) <doi:10.1111/j.1541-0420.2011.01619.x>. Confounder selection can be performed via either Markov/Bayesian networks, random forests or LASSO."
"bnpa",0,1,2,"2017-01-13 17:16","This project aims to enable the method of Path Analysis to infer causalities              from data. For this we propose a hybrid approach, which uses Bayesian network              structure learning algorithms from data to create the input file for creation of a              PA model. The process is performed in a semi-automatic way by our intermediate              algorithm, allowing novice researchers to create and evaluate their own PA models             from a data set. The references used for this project are:              Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press. <doi:10.1017/S0269888910000275>.              Nagarajan, R., Scutari, M., & Lèbre, S. (2013). Bayesian networks in r. Springer, 122, 125-127. Scutari, M., & Denis, J. B. <doi:10.1007/978-1-4614-6446-4>.             Scutari M (2010). Bayesian networks: with examples in R. Chapman and Hall/CRC. <doi:10.1201/b17065>.              Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software, 48(2), 1 - 36. <doi:10.18637/jss.v048.i02>."
"bayesvl",0,1,1,"2019-05-24","Provides users with its associated functions for pedagogical purposes in visually learning Bayesian networks and Markov chain Monte Carlo (MCMC) computations. It enables users to: a) Create and examine the (starting) graphical structure of Bayesian networks; b) Create random Bayesian networks using a dataset with customized constraints; c) Generate 'Stan' code for structures of Bayesian networks for sampling the data and learning parameters; d) Plot the network graphs; e) Perform Markov chain Monte Carlo computations and produce graphs for posteriors checks. The package refers to one reference item, which describes the methods and algorithms: Vuong, Quan-Hoang and La, Viet-Phuong (2019) <doi:10.31219/osf.io/w5dx6> The 'bayesvl' R package. Open Science Framework (May 18)."
"BayesianNetwork",1,1,4,"2017-07-12 03:51","A 'Shiny' web application for creating interactive Bayesian Network models,    learning the structure and parameters of Bayesian networks, and utilities for classic    network analysis."
"GroupBN",0,1,2,"2020-05-01 20:50","Learn group Bayesian Networks using hierarchical Clustering. This package implements the inference of group Bayesian networks based on hierarchical Clustering, and the adaptive refinement of the grouping regarding an outcome of interest."
"Infusion",0,1,6,"2018-09-24 12:10","Implements functions for simulation-based inference. In particular, implements functions to perform likelihood inference from data summaries whose distributions are simulated (Rousset et al. 2017 <doi:10.1111/1755-0998.12627>).  "
"vanddraabe",1,1,2,"2017-08-11 18:15","Identify and analyze conserved waters within crystallographic   protein structures and molecular dynamics simulation trajectories. Statistical   parameters for each water cluster, informative graphs, and a PyMOL session   file to visually explore the conserved waters and protein are returned.   Hydrophilicity is the propensity of waters to congregate near specific protein   atoms and is related to conserved waters. An informatics derived set of   hydrophilicity values are provided based on a large, high-quality X-ray   protein structure dataset."
"COUSCOus",0,1,1,"2016-02-28","Contact prediction using shrinked covariance (COUSCOus). COUSCOus is a residue-residue contact detecting method approaching the contact inference using the glassofast implementation of Matyas and Sustik (2012, The University of Texas at Austin UTCS Technical Report 2012:1-3. TR-12-29.) that solves the L_1 regularised Gaussian maximum likelihood estimation of the inverse of a covariance matrix. Prior to the inverse covariance matrix estimation we utilise a covariance matrix shrinkage approach, the empirical Bayes covariance estimator, which has been shown by Haff (1980) <doi:10.1214/aos/1176345010> to be the best estimator in a Bayesian framework, especially dominating estimators of the form aS, such as the smoothed covariance estimator applied in a related contact inference technique PSICOV."
"compas",0,1,1,"2018-12-02","Manipulate and analyze 3-D structural geometry of Protein Data Bank (PDB) files."
"aLFQ",0,1,10,"2014-02-12 11:52","Determination of absolute protein quantities is necessary for multiple applications, such as mechanistic modeling of biological systems. Quantitative liquid chromatography tandem mass spectrometry (LC-MS/MS) proteomics can measure relative protein abundance on a system-wide scale. To estimate absolute quantitative information using these relative abundance measurements requires additional information such as heavy-labeled references of known concentration. Multiple methods have been using different references and strategies; some are easily available whereas others require more effort on the users end. Hence, we believe the field might benefit from making some of these methods available under an automated framework, which also facilitates validation of the chosen strategy. We have implemented the most commonly used absolute label-free protein abundance estimation methods for LC-MS/MS modes quantifying on either MS1-, MS2-levels or spectral counts together with validation algorithms to enable automated data analysis and error estimation. Specifically, we used Monte-carlo cross-validation and bootstrapping for model selection and imputation of proteome-wide absolute protein quantity estimation. Our open-source software is written in the statistical programming language R and validated and demonstrated on a synthetic sample. "
"Rknots",1,1,7,"2015-12-23 11:24","Contains functions for the topological analysis of polymers, with a focus on protein structures."
"fPortfolio",0,2,18,"2017-11-16 22:55","Provides a collection	of functions to optimize portfolios and to analyze them from    different points of view."
"phenopix",0,1,5,"2020-01-28 16:00","A collection of functions to process digital images, depict greenness index trajectories and extract relevant phenological stages. "
"PCRedux",2,1,8,"2020-01-07 23:40","Extracts features from amplification curve data of quantitative Polymerase Chain Reactions (qPCR) (Pabinger S. et al. (2014) <doi:10.1016/j.bdq.2014.08.002>) for machine learning purposes. Helper functions prepare the amplification curve data for processing as functional data (e.g., Hausdorff distance) or enable the plotting of amplification curve classes (negative, ambiguous, positive). The hookreg() and hookregNL() functions (Burdukiewicz M. et al. (2018) <doi:10.1016/j.bdq.2018.08.001>) can be used to predict amplification curves with an hook effect-like curvature. The pcrfit_single() function can be used to extract features from an amplification curve."
"fPortfolio",0,2,18,"2017-11-16 22:55","Provides a collection	of functions to optimize portfolios and to analyze them from    different points of view."
"phenopix",0,1,5,"2020-01-28 16:00","A collection of functions to process digital images, depict greenness index trajectories and extract relevant phenological stages. "
"PCRedux",2,1,8,"2020-01-07 23:40","Extracts features from amplification curve data of quantitative Polymerase Chain Reactions (qPCR) (Pabinger S. et al. (2014) <doi:10.1016/j.bdq.2014.08.002>) for machine learning purposes. Helper functions prepare the amplification curve data for processing as functional data (e.g., Hausdorff distance) or enable the plotting of amplification curve classes (negative, ambiguous, positive). The hookreg() and hookregNL() functions (Burdukiewicz M. et al. (2018) <doi:10.1016/j.bdq.2018.08.001>) can be used to predict amplification curves with an hook effect-like curvature. The pcrfit_single() function can be used to extract features from an amplification curve."
"clue",1,47,69,"2018-09-10 09:06","CLUster Ensembles."
"warbleR",4,2,26,"2020-03-10 08:10","Functions aiming to facilitate the analysis of the structure of animal acoustic signals in 'R'. 'warbleR' makes use of the basic sound analysis tools from the package 'seewave', and offers new tools for acoustic structure analysis. The main features of the package are the use of loops to apply tasks through acoustic signals referenced in a selection (annotation) table and the production of spectrograms in image files that allow to organize data and verify acoustic analyzes. The package offers functions to explore, organize and manipulate multiple sound files, explore and download 'Xeno-Canto' recordings, detect signals automatically, create spectrograms of complete recordings or individual signals, run different measures of acoustic signal structure, evaluate the performance of measurement methods, catalog signals, characterize different structural levels in acoustic signals, run statistical analysis of duet coordination and consolidate databases and annotation tables, among others."
"vhica",0,1,2,"2016-04-05 10:22","The ""Vertical and Horizontal Inheritance Consistence Analysis"" method is described in the following publication: ""VHICA: a new method to discriminate between vertical and horizontal transposon transfer: application to the mariner family within Drosophila"" by G. Wallau. et al. (2016) <doi:10.1093/molbev/msv341>. The purpose of the method is to detect horizontal transfers of transposable elements, by contrasting the divergence of transposable element sequences with that of regular genes. "
"treeDA",1,1,3,"2018-03-15 00:31","Performs sparse discriminant analysis on a combination of    node and leaf predictors when the predictor variables are    structured according to a tree, as described in Fukuyama et    al. (2017) <doi:10.1371/journal.pcbi.1005706>."
"tracerer",1,2,4,"2019-12-02 13:10","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'Tracer' (<http://tree.bio.ed.ac.uk/software/tracer/>) is a GUI tool   to parse and analyze the files generated by 'BEAST2'.  This package provides a way to parse and analyze 'BEAST2' input  files without active user input, but using  R function calls instead."
"TotalCopheneticIndex",0,1,3,"2017-11-30 14:34","For a given phylogenetic tree, calculates the Total Cophenetic Index.  Reference: A. Mir, F. Rossello, L. A. Rotger (2013).  A new balance index for phylogenetic trees.  Math. Biosci. 241, 125-136 <doi:10.1016/j.mbs.2012.10.005>."
"symmoments",1,1,4,"2014-08-04 15:29","Symbolic central and non-central moments of the multivariate normal distribution. Computes a standard representation, LateX code, and values at specified mean and covariance matrices."
"subniche",0,1,11,"2020-04-03 18:00","Complementary indexes calculation to the Outlying Mean Index analysis to explore niche shift of a community and biological constraint within an Euclidean space, with graphical displays."
"scrm",2,2,10,"2018-06-30 23:10","A coalescent simulator that allows the rapid simulation of    biological sequences under neutral models of evolution. Different to other    coalescent based simulations, it has an optional approximation parameter that    allows for high accuracy while maintaining a linear run time cost for long    sequences. It is optimized for simulating massive data sets as produced by Next-    Generation Sequencing technologies for up to several thousand sequences."
"rncl",0,2,7,"2018-07-27 16:20","An interface to the Nexus Class Library which allows parsing    of NEXUS, Newick and other phylogenetic tree file formats. It provides    elements of the file that can be used to build phylogenetic objects    such as ape's 'phylo' or phylobase's 'phylo4(d)'. This functionality    is demonstrated with 'read_newick_phylo()' and 'read_nexus_phylo()'."
"rehh",2,1,15,"2020-06-19 12:00","Population genetic data such as 'Single Nucleotide        Polymorphisms' (SNPs) is often used to identify genomic regions        that have been under recent natural or artificial selection        and might provide clues about the molecular mechanisms of adaptation.         One approach, the concept of an 'Extended Haplotype Homozygosity' (EHH),         introduced by (Sabeti 2002) <doi:10.1038/nature01140>, has given rise to         several statistics designed for whole genome scans.         The package provides functions to compute three of these,        namely: 'iHS' (Voight 2006) <doi:10.1371/journal.pbio.0040072> for        detecting positive or 'Darwinian' selection within a single population as well as        'Rsb' (Tang 2007) <doi:10.1371/journal.pbio.0050171> and         'XP-EHH' (Sabeti 2007) <doi:10.1038/nature06250>, targeted        at differential selection between two populations.         Various plotting functions are also included to facilitate        visualization and interpretation of these statistics.         Due to changes in the API, albeit mostly minor,         versions 3.X are not compatible with versions 2.0.X.        Note: optionally, vcf files can be imported using package vcfR. That package        is currently removed from CRAN, but can still be installed from         <https://github.com/knausb/vcfR> following instructions there."
"rdiversity",0,1,4,"2018-06-12 16:23","Provides a framework for the measurement and partitioning of    the (similarity-sensitive) biodiversity of a metacommunity and its    constituent subcommunities. Richard Reeve, et al. (2016)     <arXiv:1404.6520v3>."
"rbiom",0,1,2,"2020-05-26 16:20","    A toolkit for working with Biological Observation Matrix ('BIOM') files.    Features include reading/writing all 'BIOM' formats, rarefaction, alpha    diversity, beta diversity (including 'UniFrac'), summarizing counts by     taxonomic level, and sample subsetting. Standalone functions for     reading, writing, and subsetting phylogenetic trees are also provided.     All CPU intensive operations are encoded in C with multi-thread support."
"phylotate",0,1,4,"2018-01-29 23:43","Functions to read and write APE-compatible phylogenetic  trees in NEXUS and Newick formats, while preserving annotations."
"phylocomr",1,1,6,"2019-11-25 18:00","Interface to 'Phylocom' (<http://phylodiversity.net/phylocom/>),    a library for analysis of 'phylogenetic' community structure and    character evolution. Includes low level methods for interacting with    the three executables, as well as higher level interfaces for methods    like 'aot', 'ecovolve', 'bladj', 'phylomatic', and more."
"outbreaks",0,8,6,"2019-01-21 11:00","Empirical or simulated disease outbreak data, provided either as    RData or as text files."
"ouch",0,2,24,"2017-07-19 00:42","Fit and compare Ornstein-Uhlenbeck models for evolution along a phylogenetic tree."
"MVA",1,1,7,"2014-03-12 16:30","Functions, data sets, analyses and examples from the book   ‘An Introduction to Applied Multivariate Analysis with R’   (Brian S. Everitt and Torsten Hothorn, Springer, 2011). "
"multicross",0,1,3,"2019-09-23 20:30","We introduce a nonparametric, graphical test based on optimal matching for assessing whether multiple unknown multivariate probability distributions are equal. This method is consistent, and does not make any distributional assumptions on the data. Our procedure combines data that belong to different classes or groups to create a graph on the pooled data, and then utilizes the number of edges connecting data points from different classes to examine equality of distributions among the classes. The functions available through this package implement the work described here: <arXiv:1906.04776>."
"metagear",1,1,6,"2020-03-20 16:50","Functionalities for facilitating systematic reviews, data    extractions, and meta-analyses. It includes a GUI (graphical user interface)    to help screen the abstracts and titles of bibliographic data; tools to assign    screening effort across multiple collaborators/reviewers and to assess inter-    reviewer reliability; tools to help automate the download and retrieval of    journal PDF articles from online databases; figure and image extractions     from PDFs; web scraping of citations; automated and manual data extraction     from scatter-plot and bar-plot images; PRISMA (Preferred Reporting Items for    Systematic Reviews and Meta-Analyses) flow diagrams; simple imputation tools    to fill gaps in incomplete or missing study parameters; generation of random    effects sizes for Hedges' d, log response ratio, odds ratio, and correlation    coefficients for Monte Carlo experiments; covariance equations for modelling    dependencies among multiple effect sizes (e.g., effect sizes with a common    control); and finally summaries that replicate analyses and outputs from     widely used but no longer updated meta-analysis software (i.e., metawin).	Funding for this package was supported by National Science Foundation (NSF) 	grants DBI-1262545 and DEB-1451031."
"kmeRs",1,1,1,"2018-11-03","Contains tools to calculate similarity score matrix for DNA k-mers. The pairwise            similarity score is calculated using PAM or BLOSUM substitution matrix. The             results are evaluated by similarity score calculated by Needleman-Wunsch             (1970) <doi:10.1016/0022-2836(70)90057-4> global or Smith-Waterman             (1981) <doi:10.1016/0022-2836(81)90087-5> local alignment. Higher similarity            score indicates more similar sequences for BLOSUM and less similar sequences            for PAM matrix; 30, 40, 70, 120, 250 and 62, 45, 50, 62, 80, 100 matrix             versions are available for PAM and BLOSUM, respectively. "
"kmer",1,3,6,"2019-03-15 08:10","Contains tools for rapidly computing distance matrices     and clustering large sequence datasets using fast alignment-free     k-mer counting and recursive k-means partitioning.     See Vinga and Almeida (2003) <doi:10.1093/bioinformatics/btg005>     for a review of k-mer counting methods and applications for     biological sequence analysis."
"HSAUR3",22,7,11,"2018-05-28 20:36","Functions, data sets, analyses and examples from the   third edition of the book   ”A Handbook of Statistical Analyses Using R” (Torsten Hothorn and Brian S.  Everitt, Chapman & Hall/CRC, 2014). The first chapter  of the book, which is entitled ”An Introduction to R”,   is completely included in this package, for all other chapters,  a vignette containing all data analyses is available. In addition,  Sweave source code for slides of selected chapters is included in   this package (see HSAUR3/inst/slides). The publishers web page is   '<http://www.crcpress.com/product/isbn/9781482204582>'."
"HSAUR2",19,3,26,"2017-08-20 16:22","Functions, data sets, analyses and examples from the   second edition of the book   ”A Handbook of Statistical Analyses Using R” (Brian S. Everitt and Torsten  Hothorn, Chapman & Hall/CRC, 2008). The first chapter  of the book, which is entitled ”An Introduction to R”,   is completely included in this package, for all other chapters,  a vignette containing all data analyses is available. In addition,  the package contains Sweave code for producing slides for selected  chapters (see HSAUR2/inst/slides)."
"HSAUR",17,1,26,"2017-06-21 15:00","Functions, data sets, analyses and examples from the book   ”A Handbook of Statistical Analyses Using R” (Brian S. Everitt and Torsten  Hothorn, Chapman & Hall/CRC, 2006). The first chapter  of the book, which is entitled ”An Introduction to R”,   is completely included in this package, for all other chapters,  a vignette containing all data analyses is available."
"harmonicmeanp",1,1,4,"2019-07-06 20:10","The harmonic mean p-value (HMP) test combines p-values and corrects for multiple testing while controlling the strong-sense family-wise error rate. It is more powerful than common alternatives including Bonferroni and Simes procedures when combining large proportions of all the p-values, at the cost of slightly lower power when combining small proportions of all the p-values. It is more stringent than controlling the false discovery rate, and possesses theoretical robustness to positive correlations between tests and unequal weights. It is a multi-level test in the sense that a superset of one or more significant tests is certain to be significant and conversely when the superset is non-significant, the constituent tests are certain to be non-significant. It is based on MAMML (model averaging by mean maximum likelihood), a frequentist analogue to Bayesian model averaging, and is theoretically grounded in generalized central limit theorem. For detailed examples type vignette(""harmonicmeanp"") after installation. Version 3.0 addresses errors in versions 1.0 and 2.0 that led function p.hmp to control the familywise error rate only in the weak sense, rather than the strong sense as intended."
"GLSME",0,1,6,"2014-05-20 18:30","Performs linear regression with correlated predictors, responses and correlated measurement errors in predictors and responses, correcting for biased caused by these."
"ggimage",0,8,20,"2020-01-09 09:00","Supports image files and graphic objects to be visualized in    'ggplot2' graphic system."
"FinePop",0,1,9,"2018-05-22 12:35","Statistical tool set for population genetics. The package provides following functions: 1) empirical Bayes estimator of Fst and other measures of genetic differentiation, 2) regression analysis of environmental effects on genetic differentiation using bootstrap method, 3) interfaces to read and manipulate 'GENEPOP' format data files and allele/haplotype frequency format files."
"dendextend",4,52,25,"2020-02-28 23:40","Offers a set of functions for extending    'dendrogram' objects in R, letting you visualize and compare trees of    'hierarchical clusterings'. You can (1) Adjust a tree's graphical parameters    - the color, size, type, etc of its branches, nodes and labels. (2)    Visually and statistically compare different 'dendrograms' to one another."
"DAISIE",3,1,7,"2020-08-14 12:12","Simulates and computes the (maximum) likelihood of a dynamical model of island biota assembly through speciation, immigration and extinction. See e.g. Valente et al. 2015. Ecology Letters 18: 844-852, <doi:10.1111/ele.12461>."
"CongreveLamsdell2016",3,1,3,"2019-02-07 15:13","Includes the 100 datasets simulated by Congreve and Lamsdell (2016)  <doi:10.1111/pala.12236>, and analyses of the partition and quartet distance of  reconstructed trees from the generative tree, as analysed by Smith (2019)  <doi:10.1098/rsbl.2018.0632>."
"brms",10,24,47,"2020-07-13 15:10","Fit Bayesian generalized (non-)linear multivariate multilevel models    using 'Stan' for full Bayesian inference. A wide range of distributions     and link functions are supported, allowing users to fit – among others –     linear, robust linear, count data, survival, response times, ordinal,     zero-inflated, hurdle, and even self-defined mixture models all in a     multilevel context. Further modeling options include non-linear and     smooth terms, auto-correlation structures, censored data, meta-analytic     standard errors, and quite a few more. In addition, all parameters of the     response distribution can be predicted in order to perform distributional     regression. Prior specifications are flexible and explicitly encourage     users to apply prior distributions that actually reflect their beliefs.    Model fit can easily be assessed and compared with posterior predictive     checks and leave-one-out cross-validation. References: Bürkner (2017)    <doi:10.18637/jss.v080.i01>; Bürkner (2018) <doi:10.32614/RJ-2018-017>;    Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>."
"babette",5,1,3,"2020-02-01 21:00","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is commonly accompanied by 'BEAUti 2', 'Tracer' and 'DensiTree'.    'babette' provides for an alternative workflow of using     all these tools separately. This allows doing complex Bayesian    phylogenetics easily and reproducibly from 'R'. "
"asnipe",0,1,18,"2016-11-28 13:06","Implements several tools that are used in animal social network analysis, as described in Whitehead (2007) Analyzing Animal Societies <University of Chicago Press> and Farine & Whitehead (2015) <doi:10.1111/1365-2656.12418>. In particular, this package provides the tools to infer groups and generate networks from observation data, perform permutation tests on the data, calculate lagged association rates, and performed multiple regression analysis on social network data."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"aphid",1,3,8,"2019-03-15 08:03","Designed for the development and application of    hidden Markov models and profile HMMs for biological sequence analysis.     Contains functions for multiple and pairwise sequence alignment,     model construction and parameter optimization, file import/export,    implementation of the forward, backward and Viterbi algorithms for     conditional sequence probabilities, tree-based sequence weighting,     and sequence simulation.     Features a wide variety of potential applications including     database searching, gene-finding and annotation, phylogenetic     analysis and sequence classification.    Based on the models and algorithms described in Durbin et     al (1998, ISBN: 9780521629713)."
"ade4",0,84,41,"2018-08-31 18:50","Tools for multivariate data analysis. Several methods are provided for the analysis (i.e., ordination) of one-table (e.g., principal component analysis, correspondence analysis), two-table (e.g., coinertia analysis, redundancy analysis), three-table (e.g., RLQ analysis) and K-table (e.g., STATIS, multiple coinertia analysis). The philosophy of the package is described in Dray and Dufour (2007) <doi:10.18637/jss.v022.i04>."
"yatah",1,1,2,"2019-06-28 15:00","Provides functions to manage taxonomy when lineages    are described with strings and ranks separated with special patterns    like ""|*__"" or "";*__""."
"treestructure",1,1,1,"2020-02-17","Algorithms for detecting population structure from the history of coalescent events recorded in phylogenetic trees. This method classifies each tip and internal node of a tree into disjoint sets characterized by similar coalescent patterns. The methods are described in Volz, E., Wiuf, C., Grad, Y., Frost, S., Dennis, A., & Didelot, X. (2020) <doi:10.1093/sysbio/syaa009>."
"TreeSimGM",1,1,7,"2017-09-25 20:40","Provides a flexible simulation tool for phylogenetic trees under a general model for speciation and extinction. Trees with a user-specified number of extant tips, or a user-specified stem age are simulated. It is possible to assume any probability distribution for the waiting time until speciation and extinction. Furthermore, the waiting times to speciation / extinction may be scaled in different parts of the tree, meaning we can simulate trees with clade-dependent diversification processes. At a speciation event, one species splits into two. We allow for two different modes at these splits: (i) symmetric, where for every speciation event new waiting times until speciation and extinction are drawn for both daughter lineages; and (ii) asymmetric, where a speciation event results in one species with new waiting times, and another that carries the extinction time and age of its ancestor. The symmetric mode can be seen as an vicariant or allopatric process where divided populations suffer equal evolutionary forces while the asymmetric mode could be seen as a peripatric speciation where a mother lineage continues to exist.  Reference: O. Hagen and T. Stadler (2017). TreeSimGM: Simulating phylogenetic trees under general Bellman Harris models with lineage-specific shifts of speciation and extinction in R. Methods in Ecology and Evolution. <doi:10.1111/2041-210X.12917>."
"TreeSearch",4,2,13,"2020-07-07 13:00","Searches for phylogenetic trees that are optimal using a   user-defined criterion.   Handles inapplicable data using the algorithm of   Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.  Implements Profile Parsimony (Faith and Trueman, 2001)   <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969)   <doi:10.2307/2412182>."
"treenomial",0,1,4,"2019-12-07 00:40","Provides functionality for creation and comparison of polynomials that uniquely  describe trees as introduced in Liu (2019, <arXiv:1904.03332>). The core method  converts rooted unlabeled phylo objects from 'ape' to the tree defining polynomials   described with coefficient matrices. Additionally, a conversion for rooted binary trees   with binary trait labels is also provided. Once the polynomials of trees are calculated   there are functions to calculate distances, distance matrices and plot different distance   trees from a target tree. Manipulation and conversion to the tree defining polynomials is   implemented in C++ with 'Rcpp' and 'RcppArmadillo'. Furthermore, parallel programming with   'RcppThread' is used to improve performance converting to polynomials and calculating distances. "
"treeman",0,1,6,"2017-06-22 17:18","S4 class and methods for intuitive and efficient phylogenetic tree manipulation."
"TreeDist",5,1,4,"2020-08-28 13:30","Implements measures of tree similarity, including   information-based generalized Robinson-Foulds distances  (Phylogenetic Information Distance, Clustering Information Distance,  Matching Split Information Distance; Smith, 2020)  <doi:10.1093/bioinformatics/btaa614>;   Jaccard-Robinson-Foulds distances (Bocker et al. 2013)  <doi:10.1007/978-3-642-40453-5_13>,   including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;  the Matching Split Distance (Bogdanowicz & Giaro 2012)  <doi:10.1109/TCBB.2011.48>;  Maximum Agreement Subtree distances;  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al.   (1996) <doi:10.1007/3-540-61332-3_168>.  Calculates the median of a set of trees under any distance metric."
"treedata.table",6,1,1,"2020-09-30","An implementation that combines trait data and a phylogenetic tree (or trees) into a     single object of class treedata.table. The resulting object can be easily     manipulated to simultaneously change the trait- and tree-level sampling.    Currently implemented functions allow users to use a 'data.table' syntax when    performing operations on the trait dataset within the treedata.table object."
"TransPhylo",4,1,2,"2020-05-13 18:00","Inference of transmission tree from a dated phylogeny.     Includes methods to simulate and analyse outbreaks.    The methodology is described in    Didelot et al. (2014) <doi:10.1093/molbev/msu121>,    Didelot et al. (2017) <doi:10.1093/molbev/msw275>."
"tidytree",1,6,18,"2020-03-12 07:30","Phylogenetic tree generally contains multiple components including node, edge, branch and associated data. 'tidytree' provides an approach to convert tree object to tidy data frame as well as provides tidy interfaces to manipulate tree data."
"TBRDist",1,1,3,"2020-07-07 18:00","Fast calculation of the Subtree Prune and Regraft (SPR),  Tree Bisection and Reconnection (TBR) and Replug distances between   unrooted trees, using the algorithms of Whidden and   Matsen (2017) <arxiv:1511.07529>."
"stylo",0,1,26,"2020-04-20 12:50","Supervised and unsupervised multivariate methods, supplemented by GUI and some visualizations, to perform various analyses in the field of computational stylistics, authorship attribution, etc. For further reference, see Eder et al. (2016), <https://journal.r-project.org/archive/2016/RJ-2016-007/index.html>. You are also encouraged to visit the Computational Stylistics Group's website <https://computationalstylistics.github.io/>, where a reasonable amount of information about the package and related projects are provided."
"slouch",1,1,4,"2019-03-21 12:40","An implementation of a phylogenetic comparative method. It can fit univariate among-species Ornstein-Uhlenbeck models of phenotypic trait evolution, where the trait evolves towards a primary optimum. The optimum can be modelled as a single parameter, as multiple discrete regimes on the phylogenetic tree, and/or with continuous covariates. See also Hansen (1997) <doi:10.2307/2411186>, Butler & King (2004) <doi:10.1086/426002>, Hansen et al. (2008) <doi:10.1111/j.1558-5646.2008.00412.x>."
"SeqFeatR",1,1,10,"2018-11-30 14:30","Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>."
"secsse",1,1,3,"2019-03-13 14:10","Simultaneously infers state-dependent diversification across two or more states of a single or multiple traits while accounting for the role of a possible concealed trait. See Herrera-Alsina et al. 2019 Systematic Biology 68: 317-328 <doi:10.1093/sysbio/syy057>.  "
"RRPP",2,2,12,"2020-05-28 18:40","Linear model calculations are made for many random versions of data.      Using residual randomization in a permutation procedure, sums of squares are     calculated over many permutations to generate empirical probability distributions     for evaluating model effects.  This packaged is described by     Collyer & Adams (2018) <doi:10.1111/2041-210X.13029>.  Additionally, coefficients, statistics, fitted values, and residuals generated over many     permutations can be used for various procedures including pairwise tests, prediction, classification, and    model comparison.  This package should provide most tools one could need for the analysis of    high-dimensional data, especially in ecology and evolutionary biology, but certainly other fields, as well."
"rr2",0,1,3,"2019-02-04 10:50","Three methods to calculate R2 for models with correlated errors,     including Phylogenetic GLS, Phylogenetic Logistic Regression, Linear Mixed     Models (LMMs), and Generalized Linear Mixed Models (GLMMs). See details in     Ives 2018 <doi:10.1093/sysbio/syy060>."
"rotl",3,1,11,"2019-06-14 17:50","An interface to the 'Open Tree of Life' API to retrieve    phylogenetic trees, information about studies used to assemble the    synthetic tree, and utilities to match taxonomic names to 'Open Tree    identifiers'. The 'Open Tree of Life' aims at assembling a    comprehensive phylogenetic tree for all named species."
"rhierbaps",1,1,4,"2019-05-10 00:50","Implements the hierarchical Bayesian analysis of populations structure (hierBAPS)   algorithm of Cheng et al. (2013) <doi:10.1093/molbev/mst028> for clustering DNA sequences   from multiple sequence alignments in FASTA format.   The implementation includes improved defaults and plotting capabilities   and unlike the original 'MATLAB' version removes singleton SNPs by default."
"ratematrix",3,1,5,"2019-05-06 23:00","Estimates the evolutionary rate matrix (R) using Markov chain Monte Carlo (MCMC) as described in Caetano and Harmon (2017) <doi:10.1111/2041-210X.12826>. The package has functions to run MCMC chains, plot results, evaluate convergence, and summarize posterior distributions."
"RAINBOWR",1,1,4,"2020-04-29 10:20","By using 'RAINBOWR' (Reliable Association INference By Optimizing Weights with R), users can test multiple SNPs (Single Nucleotide Polymorphisms) simultaneously by kernel-based (SNP-set) methods. This package can also be applied to haplotype-based GWAS (Genome-Wide Association Study). Users can test not only additive effects but also dominance and epistatic effects. In detail, please check our paper on PLOS Computational Biology: Kosuke Hamazaki and Hiroyoshi Iwata (2020) <doi:10.1371/journal.pcbi.1007663>."
"Quartet",2,2,5,"2019-12-30 18:30","Calculates the number of four-taxon subtrees consistent with a pair  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),  Reconstructing the shape of a tree from observed dissimilarity data,  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>,   and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for  computing the quartet and triplet distances between binary or general trees,  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>  for pairs of binary trees."
"PVR",0,1,4,"2012-10-05 17:20","Estimates (and controls for) phylogenetic signal through phylogenetic eigenvectors regression (PVR) and phylogenetic signal-representation (PSR) curve, along with some plot utilities."
"Plasmidprofiler",0,1,2,"2017-01-05 10:39","Contains functions developed to combine the results of querying a plasmid database using    short-read sequence typing with the results of a blast analysis against the query results."
"PIGShift",0,1,2,"2014-02-03 07:12","Fits models of gene expression evolution to expression data from    coregulated groups of genes, assuming inverse gamma distributed rate    variation."
"PhySortR",0,1,6,"2016-05-16 14:33","Screens and sorts phylogenetic trees in both traditional and    extended Newick format. Allows for the fast and flexible screening (within    a tree) of Exclusive clades that comprise only the target taxa and/or Non-    Exclusive clades that includes a defined portion of non-target taxa."
"PhyloMeasures",0,1,4,"2017-01-12 16:14","Given a phylogenetic tree T and an assemblage S of species represented as              a subset of tips in T, we want to compute a measure of the diversity              of the species in S with respect to T. The current package offers              efficient algorithms that can process large phylogenetic data for several such measures.              Most importantly, the package includes algorithms for computing              efficiently the standardized versions of phylogenetic measures and their p-values, which are              essential for null model comparisons. Among other functions,              the package provides efficient computation of richness-standardized versions              for indices such as the net relatedness index (NRI),              nearest taxon index (NTI), phylogenetic             diversity index (PDI), and the corresponding indices of two-sample measures.              The package also introduces a new             single-sample measure, the Core Ancestor Cost (CAC); the package provides             functions for computing the value and the standardised index of the CAC and,             more than that, there is an extra function available that can compute exactly              any statistical moment of the measure. The package supports computations             under different null models, including abundance-weighted models."
"phyloland",0,1,4,"2014-09-12 10:12","Phyloland package models a space colonization process mapped onto a phylogeny, it aims at estimating limited dispersal and ecological competitive exclusion in a Bayesian MCMC statistical phylogeographic framework (please refer to phyloland-package help for details.)"
"phylogram",1,4,5,"2018-04-13 10:44","Contains functions for developing phylogenetic trees as    deeply-nested lists (""dendrogram"" objects).    Enables bi-directional conversion between dendrogram and    ""phylo"" objects    (see Paradis et al (2004) <doi:10.1093/bioinformatics/btg412>),    and features several tools for command-line tree    manipulation and import/export via Newick parenthetic text."
"phylocanvas",2,1,2,"2017-02-03 14:33","Create and customize interactive phylogenetic trees using the 'phylocanvas' JavaScript library and the 'htmlwidgets' package. These trees can be used directly from the R console, from 'RStudio', in Shiny apps, and in R Markdown documents.  See <http://phylocanvas.org/>  for more information on the 'phylocanvas' library."
"phylobase",1,12,15,"2020-02-11 22:40","Provides a base S4 class for comparative methods, incorporating    one or more trees and trait data."
"phyext2",0,1,2,"2015-07-17 20:32","Based on (but not identical to) the no-longer-maintained package 'phyext', provides enhancements to 'phylobase' classes, specifically for use by package 'SigTree'; provides classes and methods which help users manipulate branch-annotated trees (as in 'SigTree'); also provides support for a few other extra features."
"perfectphyloR",1,1,2,"2019-07-18 08:36","Reconstructs perfect phylogeny at a user-given focal point and to depict and test association in a genomic region based on the reconstructed partitions. Charith B Karunarathna and Jinko Graham (2019) <bioRxiv:10.1101/674523>."
"PCMBase",4,2,4,"2019-09-11 18:30","Phylogenetic comparative methods represent models of continuous trait   data associated with the tips of a phylogenetic tree. Examples of such models   are Gaussian continuous time branching stochastic processes such as Brownian   motion (BM) and Ornstein-Uhlenbeck (OU) processes, which regard the data at the   tips of the tree as an observed (final) state of a Markov process starting from   an initial state at the root and evolving along the branches of the tree. The   PCMBase R package provides a general framework for manipulating such models.   This framework consists of an application programming interface for specifying   data and model parameters, and efficient algorithms for simulating trait evolution   under a model and calculating the likelihood of model parameters for an assumed  model and trait data. The package implements a growing collection of models,   which currently includes BM, OU, BM/OU with jumps, two-speed OU as well as mixed   Gaussian models, in which different types of the above models can be associated   with different branches of the tree. The PCMBase package is limited to   trait-simulation and likelihood calculation of (mixed) Gaussian phylogenetic   models. The PCMFit package provides functionality for ML and Bayesian fit of   these models to tree and trait data. The package web-site   <https://venelin.github.io/PCMBase/>  provides access to the documentation and other resources. "
"pcmabc",0,1,6,"2018-12-18 11:30","Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. Both distory and geiger are orphaned at the moment."
"PBD",1,1,7,"2017-04-29 14:58","Conducts maximum likelihood analysis and simulation of the    protracted birth-death model of diversification. See    Etienne, R.S. & J. Rosindell 2012 <doi:10.1093/sysbio/syr091>;    Lambert, A., H. Morlon & R.S. Etienne 2014, <doi:10.1007/s00285-014-0767-x>;    Etienne, R.S., H. Morlon & A. Lambert 2014, <doi:10.1111/evo.12433>."
"ouxy",0,1,2,"2019-09-20 10:50","Performs statistical inference on the models of adaptive trait evolution under approximate Bayesian computation. This can simulate traits from four models, compute trait data summary statistics. Parameters are estimated under Approximate Bayesian Computation, model selection as well as posterior parameter mean will be reported. Users need to enter a comparative dataset and a phylogenetic tree."
"oppr",1,1,7,"2020-03-11 12:20","A decision support tool for prioritizing conservation projects.    Prioritizations can be developed by maximizing expected feature richness,    expected phylogenetic diversity, the number of features that meet    persistence targets, or identifying a set of projects that meet persistence    targets for minimal cost. Constraints (e.g. lock in specific actions) and    feature weights can also be specified to further customize prioritizations.    After defining a project prioritization problem, solutions can be obtained    using exact algorithms, heuristic algorithms, or random processes. In    particular, it is recommended to install the 'Gurobi' optimizer (available    from <https://www.gurobi.com>) because it can identify optimal solutions    very quickly. Finally, methods are provided for comparing different    prioritizations and evaluating their benefits. For more information, see    Hanson et al. (2019) <doi:10.1111/2041-210X.13264>."
"nLTT",6,1,10,"2018-04-18 12:37","Provides functions to calculate the normalised Lineage-Through-    Time (nLTT) statistic, given two phylogenetic trees. The nLTT statistic measures    the difference between two Lineage-Through-Time curves, where each curve is    normalised both in time and in number of lineages."
"mvSLOUCH",0,3,29,"2020-02-24 15:10","Fits multivariate Ornstein-Uhlenbeck types of models to continues trait data from species related by a common evolutionary history. See K. Bartoszek, J, Pienaar, P. Mostad, S. Andersson, T. F.Hansen (2012) <doi:10.1016/j.jtbi.2012.08.005>. The suggested PCMBaseCpp package (which significantly speeds up the likelihood calculations) can be obtained from <https://github.com/venelin/PCMBaseCpp/>."
"msaR",1,1,3,"2017-02-24 23:57","Visualises multiple sequence alignments dynamically within the    Shiny web application framework."
"msap",1,1,8,"2013-03-05 07:01","Statistical Analyses of Methylation-sensitive Amplification Polymorphism (MSAP) assays."
"ML.MSBD",1,1,4,"2019-03-07 16:42","Inference of a multi-states birth-death model from a phylogeny, comprising a number of states N, birth and death rates for each state and on which edges each state appears. Inference is done using a hybrid approach: states are progressively added in a greedy approach. For a fixed number of states N the best model is selected via maximum likelihood. Reference: J. Barido-Sottani and T. Stadler (2017) <doi:10.1101/215491>."
"markophylo",1,1,7,"2019-03-22 16:00","Allows for fitting of maximum likelihood models using Markov chains    on phylogenetic trees for analysis of discrete character data. Examples of such    discrete character data include restriction sites, gene family presence/absence,    intron presence/absence, and gene family size data. Hypothesis-driven user-    specified substitution rate matrices can be estimated. Allows for biologically    realistic models combining constrained substitution rate matrices, site rate    variation, site partitioning, branch-specific rates, allowing for non-stationary    prior root probabilities, correcting for sampling bias, etc. See Dang and Golding     (2016) <doi:10.1093/bioinformatics/btv541> for more details."
"MAGNAMWAR",1,1,6,"2018-04-19 01:05","Correlates variation within the meta-genome to target species    phenotype variations in meta-genome with association studies. Follows    the pipeline described in Chaston, J.M. et al. (2014) <doi:10.1128/mBio.01631-14>."
"kdetrees",2,1,3,"2013-08-21 14:56","A non-parametric method for identifying potential    outlying observations in a collection of phylogenetic trees based    on the methods of Owen and Provan (2011). Such discordant trees    may indicate problems with sequence annotation or tree    reconstruction, or they may represent interesting biological    phenomena, such as horizontal gene transfers."
"jackalope",2,1,7,"2020-02-28 21:10","Simply and efficiently    simulates (i) variants from reference genomes and (ii) reads from both Illumina     <https://www.illumina.com/>    and Pacific Biosciences (PacBio) <https://www.pacb.com/> platforms.     It can either read reference genomes from FASTA files or simulate new ones.    Genomic variants can be simulated using summary statistics, phylogenies,     Variant Call Format (VCF) files, and coalescent simulations—the latter of which    can include selection, recombination, and demographic fluctuations.    'jackalope' can simulate single, paired-end, or mate-pair Illumina reads,     as well as PacBio reads.    These simulations include sequencing errors, mapping qualities, multiplexing,    and optical/polymerase chain reaction (PCR) duplicates.    Simulating Illumina sequencing is based on ART    by Huang et al. (2012) <doi:10.1093/bioinformatics/btr708>.    PacBio sequencing simulation is based on     SimLoRD  by Stöcker et al. (2016) <doi:10.1093/bioinformatics/btw286>.    All outputs can be written to standard file formats."
"ipADMIXTURE",1,1,1,"2020-03-26","A data clustering package based on admixture ratios (Q matrix) of population structure. The framework is based on iterative Pruning procedure that performs data clustering by splitting a given population into subclusters until meeting the condition of stopping criteria the same as ipPCA, iNJclust, and IPCAPS frameworks. The package also provides a function to retrieve phylogeny tree that construct a neighbor-joining tree based on a similar matrix between clusters. By given multiple Q matrices with varying a number of ancestors (K), the framework define a similar value between clusters i,j as a minimum number K* that makes majority of members of two clusters are in the different clusters. This K* reflexes a minimum number of ancestors we need to splitting cluster i,j into different clusters if we assign K* clusters based on maximum admixture ratio of individuals. The publication of this package is at Chainarong Amornbunchornvej, Pongsakorn Wangkumhang, and Sissades Tongsima (2020) <doi:10.1101/2020.03.21.001206>."
"insect",1,1,4,"2018-07-20 12:50","Provides tools for probabilistic taxon assignment with informatic sequence classification trees. See Wilkinson et al (2018) <doi:10.7287/peerj.preprints.26812v1>."
"indelmiss",0,1,3,"2018-01-30 19:41","Genome-wide gene insertion and deletion rates can be modelled in a maximum     likelihood framework with the additional flexibility of modelling potential missing     data using the models included within. These models simultaneously estimate insertion     and deletion (indel) rates of gene families and proportions of ""missing"" data for     (multiple) taxa of interest. The likelihood framework is utilized for parameter     estimation. A phylogenetic tree of the taxa and gene presence/absence patterns     (with data ordered by the tips of the tree) are required. See Dang et al.    (2016) <doi:10.1534/genetics.116.191973> for more details."
"homals",0,1,16,"2015-07-23 15:14","Performs a homogeneity analysis (multiple correspondence analysis) and various extensions. Rank restrictions on the category quantifications can be imposed (nonlinear PCA). The categories are transformed by means of optimal scaling with options for nominal, ordinal, and numerical scale levels (for rank-1 restrictions). Variables can be grouped into sets, in order to emulate regression analysis and canonical correlation analysis. "
"Hmsc",5,1,3,"2019-12-16 13:50","Hierarchical Modelling of Species Communities (HMSC) is   a model-based approach for analyzing community ecological data.    This package implements it in the Bayesian framework with Gibbs   Markov chain Monte Carlo (MCMC) sampling."
"harrietr",0,1,2,"2017-02-22 08:42","Harriet was Charles Darwin's pet tortoise (possibly). 'harrietr'    implements some function to manipulate distance matrices and phylogenetic trees    to make it easier to plot with 'ggplot2' and to manipulate using 'tidyverse'    tools."
"haplotypes",0,1,4,"2019-03-16 23:50","Provides S4 classes and methods for reading and manipulating aligned DNA sequences, supporting an indel coding methods (only simple indel coding method is available in the current version), showing base substitutions and indels, calculating absolute pairwise distances between DNA sequences, and collapses identical DNA sequences into haplotypes or inferring haplotypes using user provided absolute pairwise character difference matrix.  This package also includes S4 classes and methods for estimating genealogical relationships among haplotypes using statistical parsimony and plotting parsimony networks.  "
"HACSim",0,1,6,"2019-09-15 01:30","Performs iterative extrapolation of species' haplotype accumulation curves using a nonparametric stochastic (Monte Carlo) optimization method for assessment of specimen sampling completeness based on the approach of Phillips et al. (2015) <doi:10.1515/dna-2015-0008>, Phillips et al. (2019) <doi:10.1002/ece3.4757> and Phillips et al. (2020) <doi:10.7717/peerj-cs.243>. Any genomic marker can be targeted to assess likely required specimen sample sizes. The method is well-suited to assess sampling sufficiency for DNA barcoding initiatives."
"graphscan",0,1,3,"2015-09-21 17:58","Multiple scan statistic with variable window for one dimension data and scan statistic based on connected components in 2D or 3D."
"gquad",0,1,4,"2017-03-02 21:06","Genomic biology is not limited to the confines of the canonical B-    forming DNA duplex, but includes over ten different types of other secondary    structures that are collectively termed non-B DNA structures. Of these non-B    DNA structures, the G-quadruplexes are highly stable four-stranded structures    that are recognized by distinct subsets of nuclear factors. This package    provide functions for predicting intramolecular G quadruplexes. In addition,     functions for predicting other intramolecular nonB DNA structures are included."
"geomedb",0,1,5,"2019-08-29 01:30","The Genomic Observatory Metadatabase (GeOMe Database) is an open access repository for    geographic and ecological metadata associated with sequenced samples. This package is used to retrieve    GeOMe data for analysis. See <http://www.geome-db.org> for more information regarding GeOMe."
"FossilSim",6,1,3,"2018-11-16 10:20","Simulating taxonomy and fossil data on phylogenetic trees under mechanistic    models of speciation, preservation and sampling."
"expands",1,1,13,"2018-03-18 20:30","Expanding Ploidy and Allele Frequency on Nested Subpopulations (expands) characterizes coexisting subpopulations in a single tumor sample using copy number and allele frequencies derived from exome- or whole genome sequencing input data (<http://www.ncbi.nlm.nih.gov/pubmed/24177718>). The model detects coexisting genotypes by leveraging run-specific tradeoffs between depth of coverage and breadth of coverage. This package predicts the number of clonal expansions, the size of the resulting subpopulations in the tumor bulk, the mutations specific to each subpopulation, tumor purity and phylogeny. The main function runExPANdS() provides the complete functionality needed to predict coexisting subpopulations from single nucleotide variations (SNVs) and associated copy numbers.  The robustness of subpopulation predictions increases with the number of mutations provided. It is recommended that at least 200 mutations are used as input to obtain stable results. Updates in version 2.1 include: (i) new parameter ploidy in runExPANdS.R allows specification of non-diploid background ploidies (e.g. for near-triploid cell lines); (ii) parallel computing option is available. Further documentation and FAQ available at <http://dna-discovery.stanford.edu/software/expands>."
"evobiR",0,1,2,"2013-10-21 17:35","Comparative analysis of continuous traits influencing discrete states, and utility tools to facilitate comparative analyses.  Implementations of ABBA/BABA type statistics to test for introgression in genomic data. Wright-Fisher, phylogenetic tree, and statistical distribution Shiny interactive simulations for use in teaching."
"EpiModel",1,2,30,"2020-05-08 17:10","Tools for simulating mathematical models of infectious disease dynamics.     Epidemic model classes include deterministic compartmental models, stochastic     individual-contact models, and stochastic network models. Network models use the    robust statistical methods of exponential-family random graph models (ERGMs)     from the Statnet suite of software packages in R. Standard templates for epidemic     modeling include SI, SIR, and SIS disease types. EpiModel features     an API for extending these templates to address novel scientific research aims."
"DHARMa",1,4,19,"2020-06-18 23:20","The 'DHARMa' package uses a simulation-based approach to create    readily interpretable scaled (quantile) residuals for fitted (generalized) linear mixed    models. Currently supported are linear and generalized linear (mixed) models from 'lme4'    (classes 'lmerMod', 'glmerMod'), 'glmmTMB' and 'spaMM', generalized additive models ('gam' from    'mgcv'), 'glm' (including 'negbin' from 'MASS', but excluding quasi-distributions) and 'lm' model    classes. Moreover, externally created simulations, e.g. posterior predictive simulations    from Bayesian software such as 'JAGS', 'STAN', or 'BUGS' can be processed as well.    The resulting residuals are standardized to values between 0 and 1 and can be interpreted    as intuitively as residuals from a linear regression. The package also provides a number of    plot and test functions for typical model misspecification problems, such as    over/underdispersion, zero-inflation, and residual spatial and temporal autocorrelation."
"debar",2,1,1,"2019-12-22","The 'debar' sequence processing pipeline is designed for denoising high throughput     sequencing data for the animal DNA barcode marker cytochrome c oxidase I (COI). The package     is designed to detect and correct insertion and deletion errors within sequencer outputs.     This is accomplished through comparison of input sequences against a profile hidden Markov     model (PHMM) using the Viterbi algorithm (for algorithm details see Durbin et al. 1998,     ISBN: 9780521629713). Inserted base pairs are removed and deleted base pairs are accounted     for through the introduction of a placeholder character. Since the PHMM is a probabilistic     representation of the COI barcode, corrections are not always perfect. For this reason     'debar' censors base pairs adjacent to reported indel sites, turning them into placeholder     characters (default is 7 base pairs in either direction, this feature can be disabled).    Testing has shown that this censorship results in the correct sequence length being restored,     and erroneous base pairs being masked the vast majority of the time (>95%). "
"DDD",0,6,36,"2020-04-01 15:10"," Implements maximum likelihood and bootstrap methods based on the diversity-dependent birth-death process to test whether speciation or extinction are diversity-dependent, under various models including various types of key innovations. See Etienne et al. 2012, Proc. Roy. Soc. B 279: 1300-1309, <doi:10.1098/rspb.2011.1439>, Etienne & Haegeman 2012, Am. Nat. 180: E75-E89, <doi:10.1086/667574> and Etienne et al. 2016. Meth. Ecol. Evol. 7: 1092-1099, <doi:10.1111/2041-210X.12565>. Also contains functions to simulate the diversity-dependent process."
"CommT",0,1,1,"2015-06-16","Provides functions to measure the difference between constrained and unconstrained gene tree distributions using various tree distance metrics. Constraints are enforced prior to this analysis via the estimation of a tree under the community tree model."
"colordistance",5,1,3,"2018-06-27 21:55","Loads and displays images, selectively masks specified background    colors, bins pixels by color using either data-dependent or    automatically generated color bins, quantitatively measures color    similarity among images using one of several distance metrics for    comparing pixel color clusters, and clusters images by object color    similarity. Uses CIELAB, RGB, or HSV color spaces. Originally written    for use with organism coloration (reef fish color diversity, butterfly    mimicry, etc), but easily applicable for any image set."
"coil",1,1,4,"2019-12-05 21:00","Designed for the cleaning, contextualization and assessment of cytochrome c     oxidase I DNA barcode data (COI-5P, or the five prime portion of COI). It contains     functions for placing COI-5P barcode sequences into a common reading frame,     translating DNA sequences to amino acids and for assessing the likelihood that a     given barcode sequence includes an insertion or deletion error. The error assessment     relies on the comparison of input sequences against nucleotide and amino acid profile    hidden Markov models (PHMMs) (for details see Durbin et al. 1998, ISBN: 9780521629713)     trained on a taxonomically diverse set of reference sequences. The functions are     provided as a complete pipeline and are also available individually for efficient and    targeted analysis of barcode data."
"CNull",0,1,1,"2017-03-16","Efficient computations for null models that require shuffling columns on big matrix data.             This package provides functions for faster computation of diversity measure statistics             when independent random shuffling is applied to the columns of a given matrix.              Given a diversity measure f and a matrix M, the provided functions can generate random samples              (shuffled matrix rows of M), the mean and variance of f, and the p-values of this measure              for two different null models that involve independent random shuffling of the columns of M.             The package supports computations of alpha and beta diversity measures.  "
"brranching",1,1,6,"2019-07-27 06:30","Includes methods for fetching 'phylogenies' from a variety    of sources, including the 'Phylomatic' web service     (<http://phylodiversity.net/phylomatic>), and 'Phylocom'     (<https://github.com/phylocom/phylocom/>)."
"BoSSA",3,1,14,"2018-09-25 21:10","Reads and plots phylogenetic placements."
"bite",0,1,1,"2020-04-22","Contains the JIVE (joint inter and intra-specific model of variance evolution) model and other Bayesian models aimed at understanding trait evolution. The goal of the package is to join phylogenetic comparative models (PCM) that tend to integrate various type of data (individual observations, environmental data, fossil data) into a hierarchical Bayesian framework. It contains various PCMs as well as functions to join those models into a hierarchical Bayesian framework in a flexible and user friendly way. It contains various Markov chain Monte-Carlo (MCMC) algorithms, methods for model comparison and many plotting function for pre- and post-processing data visualization. Finally, this package integrates functions allowing bridges between 'R' and the 'BEAST2' implementations of PCMs. Kostikova A, Silvestro D, Pearman PB, Salamin N (2016) <doi:10.1093/sysbio/syw010>. Gaboriau T, Mendes FK, Joly S, Silvestro D, Salamin N (in prep)."
"bioseq",2,1,1,"2020-07-26","Classes and functions to work with biological sequences (DNA, RNA and amino acid sequences).    Implements S3 infrastructure to work with biological sequences.    Provides a collection of functions to perform biological conversion among classes    (transcription, translation) and basic operations on sequences    (detection, selection and replacement based on positions or patterns).    The package also provides functions to import and export sequences from and to other package formats."
"BIEN",2,2,6,"2018-05-09 19:02","Provides Tools for Accessing the Botanical Information and Ecology Network Database.  The BIEN database contains cleaned and standardized botanical data including occurrence, trait, plot and taxonomic data (See <http://Bien.nceas.ucsb.edu/bien/> for more Information).  This package provides functions that query the BIEN database by constructing and executing optimized SQL queries."
"beautier",2,3,6,"2020-05-06 14:50","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'BEAUti 2' (which is part of 'BEAST2') is a GUI tool   that allows users to specify the many possible setups  and generates the XML file 'BEAST2' needs to run.  This package provides a way to create 'BEAST2' input  files without active user input, but using  R function calls instead."
"beastier",1,2,4,"2019-12-02 13:50","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is a command-line tool.    This package provides a way to call 'BEAST2'     from an 'R' function call."
"AnnotationBustR",1,1,4,"2018-04-10 00:16","Extraction of subsequences into FASTA files from GenBank annotations where gene names may vary among accessions. Borstein & O'Meara (2018) <doi:10.7717/peerj.5179>."
"adiv",0,1,7,"2020-02-11 16:40","Functions, data sets and examples for the calculation of various indices of biodiversity including species, functional and phylogenetic diversity. Part of the indices are expressed in terms of equivalent numbers of species. The package also provides ways to partition biodiversity across spatial or temporal scales (alpha, beta, gamma diversities). In addition to the quantification of biodiversity, ordination approaches are available which rely on diversity indices and allow the detailed identification of species, functional or phylogenetic differences between communities."
"adaptiveGPCA",1,2,3,"2017-05-05 23:58","Implements adaptive gPCA, as described in: Fukuyama, J. (2017)    <arXiv:1702.00501>. The package also includes functionality for applying    the method to 'phyloseq' objects so that the method can be easily applied    to microbiome data and a 'shiny' app for interactive visualization. "
"windex",0,1,1,"2014-10-16","Analysing convergent evolution using the Wheatsheaf index."
"VDJgermlines",0,1,1,"2018-12-18","Contains variable, diversity, and joining sequences and accompanying functions that enable both the extraction of and comparison between immune V-D-J genomic segments from a variety of species. Sources include IMGT from MP Lefranc (2009) <doi:10.1093/nar/gkn838> and Vgenerepertoire from publication DN Olivieri (2014) <doi:10.1007/s00251-014-0784-3>. "
"TreeTools",3,4,8,"2020-09-22 18:00","Efficient implementations of functions for the creation,   modification and analysis of phylogenetic trees.  Applications include:  generation of trees with specified shapes;  analysis of tree shape;  rooting of trees and extraction of subtrees;  calculation and depiction of node support;  calculation of ancestor-descendant relationships;  import and export of trees from Newick, Nexus (Maddison et al. 1997)  <doi:10.1093/sysbio/46.4.590>,  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;  and analysis of splits and cladistic information."
"treestartr",0,1,1,"2019-01-09","Combine a list of taxa with a phylogeny to generate a starting tree for use in    total evidence dating analyses."
"TreeSim",0,8,15,"2017-03-20 11:52","Simulation methods for phylogenetic trees where (i) all tips are sampled at one time point or (ii) tips are sampled sequentially through time. (i) For sampling at one time point, simulations are performed under a constant rate birth-death process, conditioned on having a fixed number of final tips (sim.bd.taxa()), or a fixed age (sim.bd.age()), or a fixed age and number of tips (sim.bd.taxa.age()). When conditioning on the number of final tips, the method allows for shifts in rates and mass extinction events during the birth-death process (sim.rateshift.taxa()). The function sim.bd.age() (and sim.rateshift.taxa() without extinction) allow the speciation rate to change in a density-dependent way. The LTT plots of the simulations can be displayed using LTT.plot(), LTT.plot.gen() and LTT.average.root(). TreeSim further samples trees with n final tips from a set of trees generated by the common sampling algorithm stopping when a fixed number m>>n of tips is first reached (sim.gsa.taxa()). This latter method is appropriate for m-tip trees generated under a big class of models (details in the sim.gsa.taxa() man page). For incomplete phylogeny, the missing speciation events can be added through simulations (corsim()). (ii) sim.rateshifts.taxa() is generalized to sim.bdsky.stt() for serially sampled trees, where the trees are conditioned on either the number of sampled tips or the age. Furthermore, for a multitype-branching process with sequential sampling, trees on a fixed number of tips can be simulated using sim.bdtypes.stt.taxa(). This function further allows to simulate under epidemiological models with an exposed class. The function sim.genespeciestree() simulates coalescent gene trees within birth-death species trees, and sim.genetree() simulates coalescent gene trees."
"treeplyr",0,1,8,"2016-03-03 05:50","Matches phylogenetic trees and trait data, and    allows simultaneous manipulation of the tree and data using 'dplyr'."
"TreePar",0,1,14,"2014-11-08 17:10","(i) For a given species phylogeny on present day data which is calibrated to calendar-time, a method for estimating maximum likelihood speciation and extinction processes is provided. The method allows for non-constant rates. Rates may change (1) as a function of time, i.e. rate shifts at specified times or mass extinction events (likelihood implemented as LikShifts, optimization as bd.shifts.optim and visualized as bd.shifts.plot) or (2) as a function of the number of species, i.e. density-dependence (likelihood implemented as LikDD and optimization as bd.densdep.optim) or (3) extinction rate may be a function of species age (likelihood implemented as LikAge and optimization as bd.age.optim.matlab). Note that the methods take into account the whole phylogeny, in particular it accounts for the ""pull of the present"" effect. (1-3) can take into account incomplete species sampling, as long as each species has the same probability of being sampled. For a given phylogeny on higher taxa (i.e. all but one species per taxa are missing), where the number of species is known within each higher taxa, speciation and extinction rates can be estimated under model (1) (implemented within LikShifts and bd.shifts.optim with groups !=0). (ii) For a given phylogeny with sequentially sampled tips, e.g. a virus phylogeny, rates can be estimated under a model where rates vary across time using bdsky.stt.optim based on likelihood LikShiftsSTT (extending LikShifts and bd.shifts.optim). Furthermore, rates may vary as a function of host types using LikTypesSTT (multitype branching process extending functions in R package diversitree). This function can furthermore calculate the likelihood under an epidemiological model where infected individuals are first exposed and then infectious."
"treedater",1,1,3,"2019-01-16 20:50","Functions for estimating times of common ancestry and molecular clock rates of evolution using a variety of evolutionary models, parametric and nonparametric bootstrap confidence intervals, methods for detecting outlier lineages, root-to-tip regression, and a statistical test for selecting molecular clock models. The methods are described in Volz, E.M. and S.D.W. Frost (2017) <doi:10.1093/ve/vex025>."
"treebase",1,1,10,"2016-09-05 08:26","Interface to the API for 'TreeBASE' <http://treebase.org>    from 'R.' 'TreeBASE' is a repository of user-submitted phylogenetic    trees (of species, population, or genes) and the data used to create    them."
"TESS",1,2,6,"2015-06-18 08:44","Simulation of reconstructed phylogenetic trees under tree-wide time-heterogeneous birth-death processes and estimation of diversification parameters under the same model. Speciation and extinction rates can be any function of time and mass-extinction events at specific times can be provided. Trees can be simulated either conditioned on the number of species, the time of the process, or both. Additionally, the likelihood equations are implemented for convenience and can be used for Maximum Likelihood (ML) estimation and Bayesian inference."
"strap",0,1,5,"2014-08-08 19:33","Functions for the stratigraphic analysis of phylogenetic trees."
"sensiPhy",1,1,9,"2019-12-10 21:50","An implementation of sensitivity analysis for phylogenetic comparative methods. The package is an umbrella of statistical and graphical methods that  estimate and report different types of uncertainty in PCM: (i) Species Sampling uncertainty (sample size; influential species and clades). (ii) Phylogenetic uncertainty (different topologies and/or branch lengths). (iii) Data uncertainty (intraspecific variation and measurement error)."
"rwty",1,1,3,"2016-06-22 05:42","Implements various tests, visualizations, and metrics    for diagnosing convergence of MCMC chains in phylogenetics.  It implements    and automates many of the functions of the AWTY package in the R    environment."
"Rphylopars",0,1,8,"2019-12-12 06:40","Tools for performing phylogenetic comparative methods for datasets with with multiple observations per species (intraspecific variation or measurement error) and/or missing data. Performs ancestral state reconstruction and missing data imputation on the estimated evolutionary model, which can be specified as Brownian Motion, Ornstein-Uhlenbeck, Early-Burst, Pagel's lambda, kappa, or delta, or a star phylogeny."
"Rphylip",0,1,2,"2014-03-11 05:20","Rphylip provides an R interface for the PHYLIP package. All users    of Rphylip will thus first have to install the PHYLIP phylogeny methods    program package (Felsenstein 2013). See http://www.phylip.com for more 	information about installing PHYLIP."
"RNeXML",5,2,22,"2020-05-10 09:20","Provides access to phyloinformatic data in 'NeXML' format.  The    package should add new functionality to R such as the possibility to    manipulate 'NeXML' objects in more various and refined way and compatibility    with 'ape' objects."
"rase",0,1,5,"2017-03-22 15:11","Implements the Range Ancestral State Estimation for phylogeography described in Quintero, I., Keil, P., Jetz, W., & Crawford, F. W. (2015) <doi:10.1093/sysbio/syv057>. It also includes Bayesian inference of ancestral states under a Brownian Motion model of character evolution and Maximum Likelihood estimation of rase for n-dimensional data. Visualizing functions in 3D are implemented using the rgl package."
"phytools",0,40,40,"2020-06-01 23:00","A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research."
"phylolm",0,6,9,"2018-05-31 06:51","Provides functions for fitting phylogenetic linear models and phylogenetic generalized linear models. The computation uses an algorithm that is linear in the number of tips in the tree. The package also provides functions for simulating continuous or binary traits along the tree. Other tools include functions to test the adequacy of a population tree."
"PhylogeneticEM",2,1,6,"2019-10-03 06:20","    Implementation of the automatic shift detection method for    Brownian Motion (BM) or Ornstein–Uhlenbeck (OU) models of trait evolution on    phylogenies. Some tools to handle equivalent shifts configurations are also    available. See Bastide et al. (2017) <doi:10.1111/rssb.12206> and    Bastide et al. (2018) <doi:10.1093/sysbio/syy005>."
"phyloclim",0,1,7,"2013-07-13 16:34","Implements some methods in phyloclimatic modeling:   estimation of ancestral climatic niches, age-range-correlation,   niche equivalency test and background-similarity test."
"phyclust",1,3,29,"2019-12-08 11:30","Phylogenetic clustering (phyloclustering) is an evolutionary        Continuous Time Markov Chain model-based approach to identify        population structure from molecular data without assuming        linkage equilibrium. The package phyclust (Chen 2011) provides a        convenient implementation of phyloclustering for DNA and SNP data,        capable of clustering individuals into subpopulations and identifying        molecular sequences representative of those subpopulations. It is        designed in C for performance, interfaced with R for visualization,        and incorporates other popular open source programs including        ms (Hudson 2002) <doi:10.1093/bioinformatics/18.2.337>,        seq-gen (Rambaut and Grassly 1997)        <doi:10.1093/bioinformatics/13.3.235>,        Hap-Clustering (Tzeng 2005) <doi:10.1002/gepi.20063> and        PAML baseml (Yang 1997, 2007) <doi:10.1093/bioinformatics/13.5.555>,        <doi:10.1093/molbev/msm088>,        for simulating data, additional analyses, and searching the best tree.        See the phyclust website for more information, documentations and        examples."
"pastis",1,1,3,"2013-05-29 11:55","A pre-processor for mrBayes that assimilates sequences, taxonomic    information and tree constraints as per xxx.  The main functions of    interest for most users will be pastis_simple, pastis_main and conch. The    main analysis is conducted with pastis_simple or pastis_main followed by a    manual execution of mrBayes (>3.2). The placement of taxa not contained in    the tree constraint can be investigated using conch."
"paleotree",0,2,25,"2019-06-05 00:20","Provides tools for transforming, a posteriori time-scaling, and    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of    fossil taxa using stratigraphic data. This package also contains a large number    of likelihood functions for estimating sampling and diversification rates from    different types of data available from the fossil record (e.g. range data,    occurrence data, etc). paleotree users can also simulate diversification and    sampling in the fossil record using the function simFossilRecord, which is a    detailed simulator for branching birth-death-sampling processes composed of    discrete taxonomic units arranged in ancestor-descendant relationships. Users    can use simFossilRecord to simulate diversification in incompletely sampled    fossil records, under various models of morphological differentiation (i.e.    the various patterns by which morphotaxa originate from one another), and    with time-dependent, longevity-dependent and/or diversity-dependent rates of    diversification, extinction and sampling. Additional functions allow users to    translate simulated ancestor-descendant data from simFossilRecord into standard    time-scaled phylogenies or unscaled cladograms that reflect the relationships    among taxon units."
"nichevol",0,1,1,"2020-03-02","A collection of tools that allow users to perform critical steps     in the process of assessing ecological niche evolution over phylogenies, with    uncertainty incorporated explicitly in reconstructions. The method proposed    here for ancestral reconstruction of ecological niches characterizes species'    niches using a bin-based approach that incorporates uncertainty in estimations.    Compared to other existing methods, the approaches presented here reduce risk    of overestimation of amounts and rates of ecological niche evolution. The    main analyses include: initial exploration of environmental data in occurrence    records and accessible areas, preparation of data for phylogenetic analyses,    executing comparative phylogenetic analyses of ecological niches, and plotting    for interpretations. Details on the theoretical background and methods used     can be found in: Peterson et al. (1999) <doi:10.1126/science.285.5431.1265>,    Soberon and Peterson (2005) <doi:10.17161/bi.v2i0.4>,    Peterson (2011) <doi:10.1111/j.1365-2699.2010.02456.x>,    Barve et al. (2011) <doi:10.1111/ecog.02671>,     Owens et al. (2013) <doi:10.1016/j.ecolmodel.2013.04.011>, and    Saupe et al. (2018) <doi:10.1093/sysbio/syx084>."
"mvMORPH",2,4,14,"2020-04-17 15:40","Fits multivariate (Brownian Motion, Early Burst, ACDC, Ornstein-Uhlenbeck and Shifts) models of continuous traits evolution on trees and time series. 'mvMORPH' also proposes high-dimensional multivariate comparative tools (linear models using Generalized Least Squares and multivariate tests) based on penalized likelihood.  See    Clavel et al. (2015) <doi:10.1111/2041-210X.12420>, Clavel et al. (2019) <doi:10.1093/sysbio/syy045>, and Clavel & Morlon (2020) <doi:10.1093/sysbio/syaa010>."
"MSCquartets",0,1,1,"2019-12-16","Methods for analyzing and using quartets displayed on a collection of gene trees,       primarily to make inferences about the species tree or network under the multi-species       coalescent model. These include quartet hypothesis tests for the model, as developed by       Mitchell et al. (2019) <doi:10.1214/19-EJS1576>, the species tree inference routines       based on quartet distances of Rhodes (2019) <doi:10.1109/TCBB.2019.2917204> and       Yourdkhani and Rhodes (2019), and the NANUQ algorithm for inference of level-1       species networks of Allman et al. (2019) <arXiv:1905.07050>."
"MPSEM",1,1,10,"2018-05-16 23:25","Computational tools to represent phylogenetic signals using adapted eigenvector maps."
"motmot",1,1,4,"2019-05-28 23:01","Functions for fitting models of trait evolution        on phylogenies for continuous traits. The majority of functions        described in Thomas and Freckleton (2012) <doi:10.1111/j.2041-210X.2011.00132.x> 	and include functions that allow for tests of variation in the rates of trait evolution."
"metaboGSE",1,1,8,"2020-01-07 14:00","Integrates metabolic networks and RNA-seq data to construct condition-specific series of metabolic sub-networks and applies to gene set enrichment analysis (Tran et al. (2018) <doi:10.1093/bioinformatics/bty929>)."
"MCMCtreeR",2,1,3,"2019-05-20 14:00","Provides functions to prepare time priors for 'MCMCtree' analyses in the 'PAML' software from Yang (2007)<doi:10.1093/molbev/msm088> and plot time-scaled phylogenies from any Bayesian divergence time analysis. Most time-calibrated node prior distributions require user-specified parameters. The package provides functions to refine these parameters, so that the resulting prior distributions accurately reflect confidence in known, usually fossil, time information. These functions also enable users to visualise distributions and write 'MCMCtree' ready input files. Additionally, the package supplies flexible functions to visualise age uncertainty on a plotted tree with using node bars, using branch widths proportional to the age uncertainty, or by plotting the full posterior distributions on nodes. Time-scaled phylogenetic plots can be visualised with absolute and geological timescales . All plotting functions are applicable with output from any Bayesian software, not just 'MCMCtree'."
"MCMCglmm",2,24,45,"2019-02-08 18:13","MCMC Generalised Linear Mixed Models. "
"iteRates",0,1,4,"2013-03-16 07:19","Iterates through a phylogenetic tree to identify regions        of rate variation using the parametric rate comparison test."
"ips",0,1,2,"2014-11-10 00:38","Functions that wrap popular phylogenetic software for sequence    alignment, masking of sequence alignments, and estimation of phylogenies and    ancestral character states."
"HyPhy",0,1,1,"2012-07-30","A Bay Area high level phylogenetic analysis package mostly        using the birth-death process. Analysis of species tree        branching times and simulation of species trees under a number        of different time variable birth-death processes.  Analysis of        gene tree species tree reconciliations and simulations of gene        trees in species trees."
"hisse",6,1,23,"2019-12-06 19:00","Sets up and executes a HiSSE model (Hidden State Speciation and Extinction) on a phylogeny and character sets to test for hidden shifts in trait dependent rates of diversification. Beaulieu and O'Meara (2016) <doi:10.1093/sysbio/syw022>."
"geiger",0,29,24,"2014-02-10 21:06","Methods for fitting macroevolutionary models to phylogenetic trees 	Pennell (2014) <doi:10.1093/bioinformatics/btu181>."
"diversitree",0,2,15,"2020-01-16 14:20","Contains a number of comparative 'phylogenetic' methods,  mostly focusing on analysing diversification and character  evolution.  Contains implementations of 'BiSSE' (Binary State  'Speciation' and Extinction) and its unresolved tree extensions,  'MuSSE' (Multiple State 'Speciation' and Extinction), 'QuaSSE',  'GeoSSE', and 'BiSSE-ness' Other included methods include Markov  models of discrete and continuous trait evolution and constant rate  'speciation' and extinction."
"distory",0,3,5,"2012-12-10 11:59","Geodesic distance between phylogenetic trees and  associated functions. The theoretical background of 'distory'  is published in Billera et al. (2001) ""Geometry of the space of  phylogenetic trees."" <doi:10.1006/aama.2001.0759>."
"convevol",0,1,6,"2018-08-20 19:40","Quantifies and assesses the significance of convergent evolution using two different methods (and 5 different measures) as described in Stayton (2015) <doi:10.1111/evo.12729>.  Also displays results in a phylomorphospace framework."
"coalescentMCMC",2,1,5,"2013-12-04 09:41","Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters."
"Claddis",0,1,5,"2020-08-27 01:00","Measures morphological diversity from discrete character data and    estimates evolutionary tempo on phylogenetic trees. Imports morphological    data from #NEXUS (Maddison et al. (1997) <doi:10.1093/sysbio/46.4.590>)    format with read_nexus_matrix(), and writes to both #NEXUS and TNT format     (Goloboff et al. (2008) <doi:10.1111/j.1096-0031.2008.00217.x>). Main    functions are test_rates(), which implements AIC and likelihood    ratio tests for discrete character rates introduced across Lloyd et al.    (2012) <doi:10.1111/j.1558-5646.2011.01460.x>, Brusatte et al. (2014)    <doi:10.1016/j.cub.2014.08.034>, Close et al. (2015)    <doi:10.1016/j.cub.2015.06.047>, and Lloyd (2016) <doi:10.1111/bij.12746>,    and MatrixDistances(), which implements multiple discrete character    distance metrics from Gower (1971) <doi:10.2307/2528823>, Wills (1998)    <doi:10.1006/bijl.1998.0255>, Lloyd (2016) <doi:10.1111/bij.12746>, and    Hopkins and St John (2018) <doi:10.1098/rspb.2018.1784>. This also includes    the GED correction from Lehmann et al. (2019) <doi:10.1111/pala.12430>.    Multiple functions implement morphospace plots:    plot_chronophylomorphospace() implements Sakamoto and Ruta (2012)    <doi:10.1371/journal.pone.0039752>, plot_morphospace() implements Wills et    al. (1994) <doi:10.1017/S009483730001263X>, plot_changes_on_tree()    implements Wang and Lloyd (2016) <doi:10.1098/rspb.2016.0214>, and    plot_morphospace_stack() implements Foote (1993)    <doi:10.1017/S0094837300015864>. Other functions include    safe_taxonomic_reduction(), which implements Wilkinson (1995)    <doi:10.1093/sysbio/44.4.501>, map_dollo_changes() implements    the Dollo stochastic character mapping of Tarver et al. (2018)    <doi:10.1093/gbe/evy096>, and estimate_ancestral_states() implements    the ancestral state options of Lloyd (2018) <doi:10.1111/pala.12380>."
"Canopy",1,1,4,"2017-04-08 22:30","A statistical framework and computational procedure for identifying  the sub-populations within a tumor, determining the mutation profiles of each   subpopulation, and inferring the tumor's phylogenetic history. The input are   variant allele frequencies (VAFs) of somatic single nucleotide alterations   (SNAs) along with allele-specific coverage ratios between the tumor and matched  normal sample for somatic copy number alterations (CNAs). These quantities can  be directly taken from the output of existing software. Canopy provides a   general mathematical framework for pooling data across samples and sites to   infer the underlying parameters. For SNAs that fall within CNA regions, Canopy  infers their temporal ordering and resolves their phase.  When there are   multiple evolutionary configurations consistent with the data, Canopy outputs   all configurations along with their confidence assessment."
"BBMV",0,1,3,"2017-10-23 20:14","Provides a set of functions to fit general macroevolutionary models for continuous traits evolving in adaptive landscapes of any shape. This package implements the Fokker-Planck-Kolmogorov model (FPK), in which the trait evolves under random diffusion but is also subject to a force that pulls it towards specific values - this force can be of any shape. FPK has a version in which hard reflective bounds exist at the extremes of the trait interval: this second model is called BBMV. "
"BarcodingR",0,1,3,"2016-10-18 23:10","To perform species identification using DNA barcodes."
"BAMMtools",0,2,13,"2017-02-03 08:13","Provides functions for analyzing and visualizing complex    macroevolutionary dynamics on phylogenetic trees. It is a companion    package to the command line program BAMM (Bayesian Analysis of    Macroevolutionary Mixtures) and is entirely oriented towards the analysis,    interpretation, and visualization of evolutionary rates. Functionality    includes visualization of rate shifts on phylogenies, estimating    evolutionary rates through time, comparing posterior distributions of    evolutionary rates across clades, comparing diversification models using    Bayes factors, and more."
"adhoc",0,1,2,"2013-12-13 12:42","Two functions to calculate intra- and interspecific pairwise distances, evaluate DNA barcoding identification error and calculate an ad hoc distance threshold for each particular reference library of DNA barcodes. Specimen identification at this ad hoc distance threshold (using the best close match method) will produce identifications with an estimated relative error probability that can be fixed by the user (e.g. 5%)."
"AbSim",1,1,6,"2017-10-27 13:08","Simulation methods for the evolution of antibody repertoires. The heavy and light chain variable region of both human and C57BL/6 mice can be simulated in a time-dependent fashion. Both single lineages using one set of V-, D-, and J-genes or full repertoires can be simulated. The algorithm begins with an initial V-D-J recombination event, starting the first phylogenetic tree. Upon completion, the main loop of the algorithm begins, with each iteration representing one simulated time step. Various mutation events are possible at each time step, contributing to a diverse final repertoire."
"varTestnlme",1,1,2,"2020-01-10 12:30","An implementation of the Likelihood ratio Test (LRT) for testing that,    in a (non)linear mixed effects model, the variances of a subset of the random    effects are equal to zero. There is no restriction on the subset of variances    that can be tested: for example, it is possible to test that all the variances    are equal to zero. Note that the implemented test is asymptotic.    This package should be used on model fits from packages 'nlme', 'lmer', and 'saemix'.    Charlotte Baey, Paul-Henry Cournède and Estelle Kuhn (2019) <doi:10.1016/j.csda.2019.01.014>."
"CITAN",0,1,9,"2014-12-06 23:18","Supports quantitative    research in scientometrics and bibliometrics. Provides    various tools for preprocessing bibliographic    data retrieved, e.g., from Elsevier's SciVerse Scopus,    computing bibliometric impact of individuals,    or modeling many phenomena encountered in the social sciences."
"CITAN",0,1,9,"2014-12-06 23:18","Supports quantitative    research in scientometrics and bibliometrics. Provides    various tools for preprocessing bibliographic    data retrieved, e.g., from Elsevier's SciVerse Scopus,    computing bibliometric impact of individuals,    or modeling many phenomena encountered in the social sciences."
"sunburstR",1,1,14,"2019-11-05 17:30","Make interactive 'd3.js' sequence sunburst diagrams in R with the    convenience and infrastructure of an 'htmlwidget'."
"lar",0,1,1,"2014-04-30","This package is intended for researchers studying historical labour relations (see http://www.historyoflabourrelations.org). The package allows for easy access of excel files in the standard defined by the Global Collaboratory on the History of Labour Relations. The package also allows for visualisation of labour relations according to the Collaboratory's format."
"ipADMIXTURE",1,1,1,"2020-03-26","A data clustering package based on admixture ratios (Q matrix) of population structure. The framework is based on iterative Pruning procedure that performs data clustering by splitting a given population into subclusters until meeting the condition of stopping criteria the same as ipPCA, iNJclust, and IPCAPS frameworks. The package also provides a function to retrieve phylogeny tree that construct a neighbor-joining tree based on a similar matrix between clusters. By given multiple Q matrices with varying a number of ancestors (K), the framework define a similar value between clusters i,j as a minimum number K* that makes majority of members of two clusters are in the different clusters. This K* reflexes a minimum number of ancestors we need to splitting cluster i,j into different clusters if we assign K* clusters based on maximum admixture ratio of individuals. The publication of this package is at Chainarong Amornbunchornvej, Pongsakorn Wangkumhang, and Sissades Tongsima (2020) <doi:10.1101/2020.03.21.001206>."
"colorfindr",0,1,3,"2018-10-07 22:30","Extracts colors from various image types, returns customized reports and plots treemaps     and 3D scatterplots of image compositions. Color palettes can also be created. "
"bdvis",0,1,10,"2018-02-13 05:30","Provides a set of functions to create basic visualizations to quickly    preview different aspects of biodiversity information such as inventory     completeness, extent of coverage (taxonomic, temporal and geographic), gaps    and biases."
"sunburstR",1,1,14,"2019-11-05 17:30","Make interactive 'd3.js' sequence sunburst diagrams in R with the    convenience and infrastructure of an 'htmlwidget'."
"lar",0,1,1,"2014-04-30","This package is intended for researchers studying historical labour relations (see http://www.historyoflabourrelations.org). The package allows for easy access of excel files in the standard defined by the Global Collaboratory on the History of Labour Relations. The package also allows for visualisation of labour relations according to the Collaboratory's format."
"ipADMIXTURE",1,1,1,"2020-03-26","A data clustering package based on admixture ratios (Q matrix) of population structure. The framework is based on iterative Pruning procedure that performs data clustering by splitting a given population into subclusters until meeting the condition of stopping criteria the same as ipPCA, iNJclust, and IPCAPS frameworks. The package also provides a function to retrieve phylogeny tree that construct a neighbor-joining tree based on a similar matrix between clusters. By given multiple Q matrices with varying a number of ancestors (K), the framework define a similar value between clusters i,j as a minimum number K* that makes majority of members of two clusters are in the different clusters. This K* reflexes a minimum number of ancestors we need to splitting cluster i,j into different clusters if we assign K* clusters based on maximum admixture ratio of individuals. The publication of this package is at Chainarong Amornbunchornvej, Pongsakorn Wangkumhang, and Sissades Tongsima (2020) <doi:10.1101/2020.03.21.001206>."
"colorfindr",0,1,3,"2018-10-07 22:30","Extracts colors from various image types, returns customized reports and plots treemaps     and 3D scatterplots of image compositions. Color palettes can also be created. "
"bdvis",0,1,10,"2018-02-13 05:30","Provides a set of functions to create basic visualizations to quickly    preview different aspects of biodiversity information such as inventory     completeness, extent of coverage (taxonomic, temporal and geographic), gaps    and biases."
"sunburstR",1,1,14,"2019-11-05 17:30","Make interactive 'd3.js' sequence sunburst diagrams in R with the    convenience and infrastructure of an 'htmlwidget'."
"lar",0,1,1,"2014-04-30","This package is intended for researchers studying historical labour relations (see http://www.historyoflabourrelations.org). The package allows for easy access of excel files in the standard defined by the Global Collaboratory on the History of Labour Relations. The package also allows for visualisation of labour relations according to the Collaboratory's format."
"ipADMIXTURE",1,1,1,"2020-03-26","A data clustering package based on admixture ratios (Q matrix) of population structure. The framework is based on iterative Pruning procedure that performs data clustering by splitting a given population into subclusters until meeting the condition of stopping criteria the same as ipPCA, iNJclust, and IPCAPS frameworks. The package also provides a function to retrieve phylogeny tree that construct a neighbor-joining tree based on a similar matrix between clusters. By given multiple Q matrices with varying a number of ancestors (K), the framework define a similar value between clusters i,j as a minimum number K* that makes majority of members of two clusters are in the different clusters. This K* reflexes a minimum number of ancestors we need to splitting cluster i,j into different clusters if we assign K* clusters based on maximum admixture ratio of individuals. The publication of this package is at Chainarong Amornbunchornvej, Pongsakorn Wangkumhang, and Sissades Tongsima (2020) <doi:10.1101/2020.03.21.001206>."
"colorfindr",0,1,3,"2018-10-07 22:30","Extracts colors from various image types, returns customized reports and plots treemaps     and 3D scatterplots of image compositions. Color palettes can also be created. "
"bdvis",0,1,10,"2018-02-13 05:30","Provides a set of functions to create basic visualizations to quickly    preview different aspects of biodiversity information such as inventory     completeness, extent of coverage (taxonomic, temporal and geographic), gaps    and biases."
"sunburstR",1,1,14,"2019-11-05 17:30","Make interactive 'd3.js' sequence sunburst diagrams in R with the    convenience and infrastructure of an 'htmlwidget'."
"lar",0,1,1,"2014-04-30","This package is intended for researchers studying historical labour relations (see http://www.historyoflabourrelations.org). The package allows for easy access of excel files in the standard defined by the Global Collaboratory on the History of Labour Relations. The package also allows for visualisation of labour relations according to the Collaboratory's format."
"ipADMIXTURE",1,1,1,"2020-03-26","A data clustering package based on admixture ratios (Q matrix) of population structure. The framework is based on iterative Pruning procedure that performs data clustering by splitting a given population into subclusters until meeting the condition of stopping criteria the same as ipPCA, iNJclust, and IPCAPS frameworks. The package also provides a function to retrieve phylogeny tree that construct a neighbor-joining tree based on a similar matrix between clusters. By given multiple Q matrices with varying a number of ancestors (K), the framework define a similar value between clusters i,j as a minimum number K* that makes majority of members of two clusters are in the different clusters. This K* reflexes a minimum number of ancestors we need to splitting cluster i,j into different clusters if we assign K* clusters based on maximum admixture ratio of individuals. The publication of this package is at Chainarong Amornbunchornvej, Pongsakorn Wangkumhang, and Sissades Tongsima (2020) <doi:10.1101/2020.03.21.001206>."
"colorfindr",0,1,3,"2018-10-07 22:30","Extracts colors from various image types, returns customized reports and plots treemaps     and 3D scatterplots of image compositions. Color palettes can also be created. "
"bdvis",0,1,10,"2018-02-13 05:30","Provides a set of functions to create basic visualizations to quickly    preview different aspects of biodiversity information such as inventory     completeness, extent of coverage (taxonomic, temporal and geographic), gaps    and biases."
"topsa",0,1,1,"2020-09-24","Estimate geometric sensitivity indices reconstructing the embedding manifold of the data. The reconstruction is done via a Vietoris Rips with a fixed radius. With the homology of order 2 we estimate symmetric reflections of the those manifold to determine a sensitivity index on of the input model. Detailed information of the methods can be found in <https://www.dropbox.com/s/0kcrjhhkl7899n1/article-symmetric-reflection.pdf> (WIP preprint). "
"pterrace",0,1,1,"2018-11-16","Plot the summary graphic called the persistence terrace for topological inference. It also provides the aid tool called the terrace area plot for determining the number of significant topological features. Moon, C., Giansiracusa, N., and Lazar, N. A. (2018) <doi:10.1080/10618600.2017.1422432>."
"archiDART",0,1,9,"2018-04-03 23:34","Analysis of complex plant root system architectures (RSA) using the output files created by Data Analysis of Root Tracings (DART), an open-access software dedicated to the study of plant root architecture and development across time series (Le Bot et al (2010) ""DART: a software to analyse root system architecture and development from captured images"", Plant and Soil, <doi:10.1007/s11104-009-0005-2>), and RSA data encoded with the Root System Markup Language (RSML) (Lobet et al (2015) ""Root System Markup Language: toward a unified root architecture description language"", Plant Physiology, <doi:10.1104/pp.114.253625>). More information can be found in Delory et al (2016) ""archiDART: an R package for the automated computation of plant root architectural traits"", Plant and Soil, <doi:10.1007/s11104-015-2673-4>."
"topsa",0,1,1,"2020-09-24","Estimate geometric sensitivity indices reconstructing the embedding manifold of the data. The reconstruction is done via a Vietoris Rips with a fixed radius. With the homology of order 2 we estimate symmetric reflections of the those manifold to determine a sensitivity index on of the input model. Detailed information of the methods can be found in <https://www.dropbox.com/s/0kcrjhhkl7899n1/article-symmetric-reflection.pdf> (WIP preprint). "
"pterrace",0,1,1,"2018-11-16","Plot the summary graphic called the persistence terrace for topological inference. It also provides the aid tool called the terrace area plot for determining the number of significant topological features. Moon, C., Giansiracusa, N., and Lazar, N. A. (2018) <doi:10.1080/10618600.2017.1422432>."
"archiDART",0,1,9,"2018-04-03 23:34","Analysis of complex plant root system architectures (RSA) using the output files created by Data Analysis of Root Tracings (DART), an open-access software dedicated to the study of plant root architecture and development across time series (Le Bot et al (2010) ""DART: a software to analyse root system architecture and development from captured images"", Plant and Soil, <doi:10.1007/s11104-009-0005-2>), and RSA data encoded with the Root System Markup Language (RSML) (Lobet et al (2015) ""Root System Markup Language: toward a unified root architecture description language"", Plant Physiology, <doi:10.1104/pp.114.253625>). More information can be found in Delory et al (2016) ""archiDART: an R package for the automated computation of plant root architectural traits"", Plant and Soil, <doi:10.1007/s11104-015-2673-4>."
"cyclestreets",0,1,2,"2018-05-03 14:28","An interface to the cycle routing/data services provided by 'CycleStreets',  a not-for-profit social enterprise and advocacy organisation.  The application programming interfaces (APIs) provided by 'CycleStreets' are documented at (<https://www.cyclestreets.net/api/>).  The focus of this package is the journey planning API, which aims to emulate the routes taken by a knowledgeable cyclist.  An innovative feature of the routing service of its provision of fastest, quietest and balanced profiles.  These represent routes taken to minimise time, avoid traffic and compromise between the two, respectively."
"pct",4,2,15,"2020-06-15 11:50","Functions and example data to teach and  increase the reproducibility of the methods and code underlying   the Propensity to Cycle Tool (PCT), a research project and web application   hosted at <https://www.pct.bike/>.   For an academic paper on the methods,  see Lovelace et al (2017) <doi:10.5198/jtlu.2016.862>."
"cyclestreets",0,1,2,"2018-05-03 14:28","An interface to the cycle routing/data services provided by 'CycleStreets',  a not-for-profit social enterprise and advocacy organisation.  The application programming interfaces (APIs) provided by 'CycleStreets' are documented at (<https://www.cyclestreets.net/api/>).  The focus of this package is the journey planning API, which aims to emulate the routes taken by a knowledgeable cyclist.  An innovative feature of the routing service of its provision of fastest, quietest and balanced profiles.  These represent routes taken to minimise time, avoid traffic and compromise between the two, respectively."
"pct",4,2,15,"2020-06-15 11:50","Functions and example data to teach and  increase the reproducibility of the methods and code underlying   the Propensity to Cycle Tool (PCT), a research project and web application   hosted at <https://www.pct.bike/>.   For an academic paper on the methods,  see Lovelace et al (2017) <doi:10.5198/jtlu.2016.862>."
"cyclestreets",0,1,2,"2018-05-03 14:28","An interface to the cycle routing/data services provided by 'CycleStreets',  a not-for-profit social enterprise and advocacy organisation.  The application programming interfaces (APIs) provided by 'CycleStreets' are documented at (<https://www.cyclestreets.net/api/>).  The focus of this package is the journey planning API, which aims to emulate the routes taken by a knowledgeable cyclist.  An innovative feature of the routing service of its provision of fastest, quietest and balanced profiles.  These represent routes taken to minimise time, avoid traffic and compromise between the two, respectively."
"pct",4,2,15,"2020-06-15 11:50","Functions and example data to teach and  increase the reproducibility of the methods and code underlying   the Propensity to Cycle Tool (PCT), a research project and web application   hosted at <https://www.pct.bike/>.   For an academic paper on the methods,  see Lovelace et al (2017) <doi:10.5198/jtlu.2016.862>."
"cyclestreets",0,1,2,"2018-05-03 14:28","An interface to the cycle routing/data services provided by 'CycleStreets',  a not-for-profit social enterprise and advocacy organisation.  The application programming interfaces (APIs) provided by 'CycleStreets' are documented at (<https://www.cyclestreets.net/api/>).  The focus of this package is the journey planning API, which aims to emulate the routes taken by a knowledgeable cyclist.  An innovative feature of the routing service of its provision of fastest, quietest and balanced profiles.  These represent routes taken to minimise time, avoid traffic and compromise between the two, respectively."
"pct",4,2,15,"2020-06-15 11:50","Functions and example data to teach and  increase the reproducibility of the methods and code underlying   the Propensity to Cycle Tool (PCT), a research project and web application   hosted at <https://www.pct.bike/>.   For an academic paper on the methods,  see Lovelace et al (2017) <doi:10.5198/jtlu.2016.862>."
"cyclestreets",0,1,2,"2018-05-03 14:28","An interface to the cycle routing/data services provided by 'CycleStreets',  a not-for-profit social enterprise and advocacy organisation.  The application programming interfaces (APIs) provided by 'CycleStreets' are documented at (<https://www.cyclestreets.net/api/>).  The focus of this package is the journey planning API, which aims to emulate the routes taken by a knowledgeable cyclist.  An innovative feature of the routing service of its provision of fastest, quietest and balanced profiles.  These represent routes taken to minimise time, avoid traffic and compromise between the two, respectively."
"pct",4,2,15,"2020-06-15 11:50","Functions and example data to teach and  increase the reproducibility of the methods and code underlying   the Propensity to Cycle Tool (PCT), a research project and web application   hosted at <https://www.pct.bike/>.   For an academic paper on the methods,  see Lovelace et al (2017) <doi:10.5198/jtlu.2016.862>."
"openSTARS",1,1,3,"2018-05-11 12:12","An open source implementation of the 'STARS' toolbox    (Peterson & Ver Hoef, 2014, <doi:10.18637/jss.v056.i02>) using 'R' and 'GRASS GIS'.    It prepares the *.ssn object needed for the 'SSN' package.    A Digital Elevation Model (DEM) is used to derive stream networks     (in contrast to 'STARS' that can clean an existing stream network)."
"smnet",0,1,4,"2016-08-29 18:49","Fits flexible additive models to data on stream networks, taking account of flow-connectivity of the network.  Models are fitted using penalised least squares."
"openSTARS",1,1,3,"2018-05-11 12:12","An open source implementation of the 'STARS' toolbox    (Peterson & Ver Hoef, 2014, <doi:10.18637/jss.v056.i02>) using 'R' and 'GRASS GIS'.    It prepares the *.ssn object needed for the 'SSN' package.    A Digital Elevation Model (DEM) is used to derive stream networks     (in contrast to 'STARS' that can clean an existing stream network)."
"smnet",0,1,4,"2016-08-29 18:49","Fits flexible additive models to data on stream networks, taking account of flow-connectivity of the network.  Models are fitted using penalised least squares."
"openSTARS",1,1,3,"2018-05-11 12:12","An open source implementation of the 'STARS' toolbox    (Peterson & Ver Hoef, 2014, <doi:10.18637/jss.v056.i02>) using 'R' and 'GRASS GIS'.    It prepares the *.ssn object needed for the 'SSN' package.    A Digital Elevation Model (DEM) is used to derive stream networks     (in contrast to 'STARS' that can clean an existing stream network)."
"smnet",0,1,4,"2016-08-29 18:49","Fits flexible additive models to data on stream networks, taking account of flow-connectivity of the network.  Models are fitted using penalised least squares."
"openSTARS",1,1,3,"2018-05-11 12:12","An open source implementation of the 'STARS' toolbox    (Peterson & Ver Hoef, 2014, <doi:10.18637/jss.v056.i02>) using 'R' and 'GRASS GIS'.    It prepares the *.ssn object needed for the 'SSN' package.    A Digital Elevation Model (DEM) is used to derive stream networks     (in contrast to 'STARS' that can clean an existing stream network)."
"smnet",0,1,4,"2016-08-29 18:49","Fits flexible additive models to data on stream networks, taking account of flow-connectivity of the network.  Models are fitted using penalised least squares."
"NetLogoR",2,1,7,"2019-11-27 13:00","Build and run spatially explicit    agent-based models using only the R platform. 'NetLogoR' follows the same    framework as the 'NetLogo' software    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation    in R of the structure and functions of 'NetLogo'.    'NetLogoR' provides new R classes to define model agents and functions to    implement spatially explicit agent-based models in the R environment.    This package allows benefiting of the fast and easy coding phase from the    highly developed 'NetLogo' framework, coupled with the versatility, power    and massive resources of the R software.    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using    'NetLogoR' are available. The 'NetLogo' code of the original version of these    models is provided alongside.    A programming guide inspired from the 'NetLogo' Programming Guide    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)    equivalences are also available.    NOTE: To increment 'time', these functions can use a for loop or can be    integrated with a discrete event simulator, such as 'SpaDES'    (<https://cran.r-project.org/package=SpaDES>).    The suggested package 'fastshp' can be installed with    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'."
"SpaDES.addins",1,1,3,"2018-02-02 19:31","Provides 'RStudio' addins for 'SpaDES' packages and 'SpaDES' module    development. See '?SpaDES.addins' for an overview of the tools provided."
"SpaDES",1,1,14,"2019-02-03 17:53","Metapackage for implementing a variety of event-based models, with    a focus on spatially explicit models. These include raster-based,    event-based, and agent-based models. The core simulation components    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)    framework that facilitates modularity, and easily enables the user to    include additional functionality by running user-built simulation modules    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters    and other maps (via 'quickPlot'), and caching methods for reproducible    simulations (via 'reproducible'). Additional functionality is provided by    the 'SpaDES.addins' and 'SpaDES.shiny' packages."
"NetLogoR",2,1,7,"2019-11-27 13:00","Build and run spatially explicit    agent-based models using only the R platform. 'NetLogoR' follows the same    framework as the 'NetLogo' software    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation    in R of the structure and functions of 'NetLogo'.    'NetLogoR' provides new R classes to define model agents and functions to    implement spatially explicit agent-based models in the R environment.    This package allows benefiting of the fast and easy coding phase from the    highly developed 'NetLogo' framework, coupled with the versatility, power    and massive resources of the R software.    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using    'NetLogoR' are available. The 'NetLogo' code of the original version of these    models is provided alongside.    A programming guide inspired from the 'NetLogo' Programming Guide    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)    equivalences are also available.    NOTE: To increment 'time', these functions can use a for loop or can be    integrated with a discrete event simulator, such as 'SpaDES'    (<https://cran.r-project.org/package=SpaDES>).    The suggested package 'fastshp' can be installed with    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'."
"SpaDES.addins",1,1,3,"2018-02-02 19:31","Provides 'RStudio' addins for 'SpaDES' packages and 'SpaDES' module    development. See '?SpaDES.addins' for an overview of the tools provided."
"SpaDES",1,1,14,"2019-02-03 17:53","Metapackage for implementing a variety of event-based models, with    a focus on spatially explicit models. These include raster-based,    event-based, and agent-based models. The core simulation components    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)    framework that facilitates modularity, and easily enables the user to    include additional functionality by running user-built simulation modules    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters    and other maps (via 'quickPlot'), and caching methods for reproducible    simulations (via 'reproducible'). Additional functionality is provided by    the 'SpaDES.addins' and 'SpaDES.shiny' packages."
"NetLogoR",2,1,7,"2019-11-27 13:00","Build and run spatially explicit    agent-based models using only the R platform. 'NetLogoR' follows the same    framework as the 'NetLogo' software    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation    in R of the structure and functions of 'NetLogo'.    'NetLogoR' provides new R classes to define model agents and functions to    implement spatially explicit agent-based models in the R environment.    This package allows benefiting of the fast and easy coding phase from the    highly developed 'NetLogo' framework, coupled with the versatility, power    and massive resources of the R software.    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using    'NetLogoR' are available. The 'NetLogo' code of the original version of these    models is provided alongside.    A programming guide inspired from the 'NetLogo' Programming Guide    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)    equivalences are also available.    NOTE: To increment 'time', these functions can use a for loop or can be    integrated with a discrete event simulator, such as 'SpaDES'    (<https://cran.r-project.org/package=SpaDES>).    The suggested package 'fastshp' can be installed with    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'."
"SpaDES.addins",1,1,3,"2018-02-02 19:31","Provides 'RStudio' addins for 'SpaDES' packages and 'SpaDES' module    development. See '?SpaDES.addins' for an overview of the tools provided."
"SpaDES",1,1,14,"2019-02-03 17:53","Metapackage for implementing a variety of event-based models, with    a focus on spatially explicit models. These include raster-based,    event-based, and agent-based models. The core simulation components    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)    framework that facilitates modularity, and easily enables the user to    include additional functionality by running user-built simulation modules    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters    and other maps (via 'quickPlot'), and caching methods for reproducible    simulations (via 'reproducible'). Additional functionality is provided by    the 'SpaDES.addins' and 'SpaDES.shiny' packages."
"NetLogoR",2,1,7,"2019-11-27 13:00","Build and run spatially explicit    agent-based models using only the R platform. 'NetLogoR' follows the same    framework as the 'NetLogo' software    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation    in R of the structure and functions of 'NetLogo'.    'NetLogoR' provides new R classes to define model agents and functions to    implement spatially explicit agent-based models in the R environment.    This package allows benefiting of the fast and easy coding phase from the    highly developed 'NetLogo' framework, coupled with the versatility, power    and massive resources of the R software.    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using    'NetLogoR' are available. The 'NetLogo' code of the original version of these    models is provided alongside.    A programming guide inspired from the 'NetLogo' Programming Guide    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)    equivalences are also available.    NOTE: To increment 'time', these functions can use a for loop or can be    integrated with a discrete event simulator, such as 'SpaDES'    (<https://cran.r-project.org/package=SpaDES>).    The suggested package 'fastshp' can be installed with    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'."
"SpaDES.addins",1,1,3,"2018-02-02 19:31","Provides 'RStudio' addins for 'SpaDES' packages and 'SpaDES' module    development. See '?SpaDES.addins' for an overview of the tools provided."
"SpaDES",1,1,14,"2019-02-03 17:53","Metapackage for implementing a variety of event-based models, with    a focus on spatially explicit models. These include raster-based,    event-based, and agent-based models. The core simulation components    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)    framework that facilitates modularity, and easily enables the user to    include additional functionality by running user-built simulation modules    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters    and other maps (via 'quickPlot'), and caching methods for reproducible    simulations (via 'reproducible'). Additional functionality is provided by    the 'SpaDES.addins' and 'SpaDES.shiny' packages."
"NetLogoR",2,1,7,"2019-11-27 13:00","Build and run spatially explicit    agent-based models using only the R platform. 'NetLogoR' follows the same    framework as the 'NetLogo' software    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation    in R of the structure and functions of 'NetLogo'.    'NetLogoR' provides new R classes to define model agents and functions to    implement spatially explicit agent-based models in the R environment.    This package allows benefiting of the fast and easy coding phase from the    highly developed 'NetLogo' framework, coupled with the versatility, power    and massive resources of the R software.    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using    'NetLogoR' are available. The 'NetLogo' code of the original version of these    models is provided alongside.    A programming guide inspired from the 'NetLogo' Programming Guide    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)    equivalences are also available.    NOTE: To increment 'time', these functions can use a for loop or can be    integrated with a discrete event simulator, such as 'SpaDES'    (<https://cran.r-project.org/package=SpaDES>).    The suggested package 'fastshp' can be installed with    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'."
"SpaDES.addins",1,1,3,"2018-02-02 19:31","Provides 'RStudio' addins for 'SpaDES' packages and 'SpaDES' module    development. See '?SpaDES.addins' for an overview of the tools provided."
"SpaDES",1,1,14,"2019-02-03 17:53","Metapackage for implementing a variety of event-based models, with    a focus on spatially explicit models. These include raster-based,    event-based, and agent-based models. The core simulation components    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)    framework that facilitates modularity, and easily enables the user to    include additional functionality by running user-built simulation modules    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters    and other maps (via 'quickPlot'), and caching methods for reproducible    simulations (via 'reproducible'). Additional functionality is provided by    the 'SpaDES.addins' and 'SpaDES.shiny' packages."
"fmf",0,1,1,"2020-09-03","A fast class noise detector which provides noise score for each observations. The package takes advantage of 'RcppArmadillo' to speed up the calculation of distances between observations."
"fmf",0,1,1,"2020-09-03","A fast class noise detector which provides noise score for each observations. The package takes advantage of 'RcppArmadillo' to speed up the calculation of distances between observations."
"CBDA",1,1,1,"2018-04-16","Classification performed on Big Data. It uses concepts from compressive sensing, and implements ensemble predictor (i.e., 'SuperLearner') and knockoff filtering as the main machine learning and feature mining engines."
"sambia",0,1,1,"2018-06-06","A collection of various techniques correcting statistical models for sample selection bias is provided. In particular, the resampling-based methods ""stochastic inverse-probability oversampling"" and ""parametric inverse-probability bagging"" are placed at the disposal which generate synthetic observations for correcting classifiers for biased samples resulting from stratified random sampling. For further information, see the article Krautenbacher, Theis, and Fuchs (2017) <doi:10.1155/2017/7847531>. The methods may be used for further purposes where weighting and generation of new observations is needed."
"CBDA",1,1,1,"2018-04-16","Classification performed on Big Data. It uses concepts from compressive sensing, and implements ensemble predictor (i.e., 'SuperLearner') and knockoff filtering as the main machine learning and feature mining engines."
"sambia",0,1,1,"2018-06-06","A collection of various techniques correcting statistical models for sample selection bias is provided. In particular, the resampling-based methods ""stochastic inverse-probability oversampling"" and ""parametric inverse-probability bagging"" are placed at the disposal which generate synthetic observations for correcting classifiers for biased samples resulting from stratified random sampling. For further information, see the article Krautenbacher, Theis, and Fuchs (2017) <doi:10.1155/2017/7847531>. The methods may be used for further purposes where weighting and generation of new observations is needed."
"CBDA",1,1,1,"2018-04-16","Classification performed on Big Data. It uses concepts from compressive sensing, and implements ensemble predictor (i.e., 'SuperLearner') and knockoff filtering as the main machine learning and feature mining engines."
"sambia",0,1,1,"2018-06-06","A collection of various techniques correcting statistical models for sample selection bias is provided. In particular, the resampling-based methods ""stochastic inverse-probability oversampling"" and ""parametric inverse-probability bagging"" are placed at the disposal which generate synthetic observations for correcting classifiers for biased samples resulting from stratified random sampling. For further information, see the article Krautenbacher, Theis, and Fuchs (2017) <doi:10.1155/2017/7847531>. The methods may be used for further purposes where weighting and generation of new observations is needed."
"CBDA",1,1,1,"2018-04-16","Classification performed on Big Data. It uses concepts from compressive sensing, and implements ensemble predictor (i.e., 'SuperLearner') and knockoff filtering as the main machine learning and feature mining engines."
"sambia",0,1,1,"2018-06-06","A collection of various techniques correcting statistical models for sample selection bias is provided. In particular, the resampling-based methods ""stochastic inverse-probability oversampling"" and ""parametric inverse-probability bagging"" are placed at the disposal which generate synthetic observations for correcting classifiers for biased samples resulting from stratified random sampling. For further information, see the article Krautenbacher, Theis, and Fuchs (2017) <doi:10.1155/2017/7847531>. The methods may be used for further purposes where weighting and generation of new observations is needed."
"tigger",1,1,10,"2019-07-19 07:40","Infers the V genotype of an individual from immunoglobulin (Ig)    repertoire sequencing data (AIRR-Seq, Rep-Seq). Includes detection of     any novel alleles. This information is then used to correct existing V     allele calls from among the sample sequences.    Citations:    Gadala-Maria, et al (2015) <doi:10.1073/pnas.1417683112>.    Gadala-Maria, et al (2019) <doi:10.3389/fimmu.2019.00129>."
"scoper",1,1,5,"2020-05-25 21:40","Provides a computational framework for identification of B cell clones from              Adaptive Immune Receptor Repertoire sequencing (AIRR-Seq) data. Three main              functions are included (identicalClones, hierarchicalClones, and spectralClones)              that perform clustering among sequences of BCRs/IGs (B cell receptors/immunoglobulins)              which share the same V gene, J gene and junction length.             Nouri N and Kleinstein SH (2018) <doi:10.1093/bioinformatics/bty235>.             Nouri N and Kleinstein SH (2019) <doi:10.1101/788620>.             Gupta NT, et al. (2017) <doi:10.4049/jimmunol.1601850>."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"singleCellHaystack",1,1,2,"2020-07-01 13:10","Identification of differentially expressed genes (DEGs) is a key step in single-cell     transcriptomics data analysis. 'singleCellHaystack' predicts DEGs without relying on     clustering of cells into arbitrary clusters. Single-cell RNA-seq (scRNA-seq) data is     often processed to fewer dimensions using Principal Component Analysis (PCA) and     represented in 2-dimensional plots (e.g. t-SNE or UMAP plots). 'singleCellHaystack'     uses Kullback-Leibler divergence to find genes that are expressed in subsets of cells     that are non-randomly positioned in a these multi-dimensional spaces or 2D representations.     For the theoretical background of 'singleCellHaystack' we refer to     Vandenbon and Diez (Nature Communications, 2020) <doi:10.1038/s41467-020-17900-3>."
"scSorter",1,1,1,"2020-09-01","Implements the algorithm described in	Guo, H., and Li, J. (Not yet published), ""scSorter: assigning cells to known cell types according to known marker genes"". 	Clusters cells to known cell types based on marker genes specified for each cell type."
"Rmagic",0,1,5,"2019-11-12 17:50","MAGIC (Markov affinity-based graph imputation of cells) is a method for addressing technical noise in single-cell data, including under-sampling of mRNA molecules, often termed ""dropout"" which can severely obscure important gene-gene relationships. MAGIC shares information across similar cells, via data diffusion, to denoise the cell count matrix and fill in missing transcripts. Read more: van Dijk et al. (2018) <doi:10.1016/j.cell.2018.05.061>."
"nanny",1,1,1,"2020-06-13","    Includes wrapper functions for the main high-level data analysis and manipulations,     such as cluster, dimensionality reduction, redundancy elimination, identify variable elements.     It operates on tidy data frames with element, feature and value column."
"BisqueRNA",1,1,5,"2019-06-19 11:30","Provides tools to accurately estimate cell type abundances     from heterogeneous bulk expression. A reference-based method utilizes    single-cell information to generate a signature matrix and transformation    of bulk expression for accurate regression based estimates. A marker-based    method utilizes known cell-specific marker genes to measure relative    abundances across samples.    For more details, see Jew and Alvarez et al (2019) <doi:10.1101/669911>."
"SoupX",1,1,1,"2020-05-26","Quantify, profile and remove ambient mRNA contamination (the ""soup"") from droplet based single cell RNA-seq experiments.  Implements the method described in Young et al. (2018) <doi:10.1101/303727>."
"Signac",0,1,3,"2020-04-16 11:10","A framework for the analysis and exploration of single-cell chromatin data.    The 'Signac' package contains functions for quantifying single-cell chromatin data,    computing per-cell quality control metrics, dimension reduction    and normalization, visualization, and DNA sequence motif analysis.    Reference: Stuart and Butler et al. (2019) <doi:10.1016/j.cell.2019.05.031>."
"scMappR",1,1,4,"2020-03-04 15:30","Description of scMappR R package adapted from pre-print. The single cell mapper (scMappR) R package contains a suite of bioinformatic tools that provide experimentally relevant cell-type specific information to a list of differentially expressed genes (DEG). The function ""scMappR_and_pathway_analysis"" reranks DEGs to generate cell-type specificity scores called cell-weighted fold-changes. Users input a list of DEGs, normalized counts, and a signature matrix into this function. scMappR then re-weights bulk DEGs by cell-type specific expression from the signature matrix, cell-type proportions from RNA-seq deconvolution and the ratio of cell-type proportions between the two conditions to account for changes in cell-type proportion. With cwFold-changes calculated, scMappR uses two approaches to utilize cwFold-changes to complete cell-type specific pathway analysis. The ""process_dgTMatrix_lists"" function in the scMappR package contains an automated scRNA-seq processing pipeline where users input scRNA-seq count data, which is made compatible for scMappR and other R packages that analyze scRNA-seq data. We further used this to store hundreds up regularly updating signature matrices. The functions ""tissue_by_celltype_enrichment"", ""tissue_scMappR_internal"", and ""tissue_scMappR_custom"" combine these consistently processed scRNAseq count data with gene-set enrichment tools to allow for cell-type marker enrichment of a generic gene list (e.g. GWAS hits). Reference: Sokolowski,D.J., Faykoo-Martinez,M., Erdman,L., Hou,H., Chan,C., Zhu,H., Holmes,M.M., Goldenberg,A. and Wilson,M.D. (2020) Single-cell mapper (scMappR): using scRNA-seq to infer cell-type specificities of differentially expressed genes. BioRxiv, 10.1101/2020.08.24.265298."
"regsem",1,1,21,"2020-02-19 13:00","Uses both ridge and lasso penalties (and extensions) to penalize    specific parameters in structural equation models. The package offers additional    cost functions, cross validation, and other extensions beyond traditional structural    equation models. Also contains a function to perform exploratory mediation (XMed). "
"psychonetrics",0,1,10,"2020-04-01 23:00","Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search."
"metaSEM",2,2,14,"2019-10-10 14:20","A collection of functions for conducting meta-analysis using a             structural equation modeling (SEM) approach via the 'OpenMx' and             'lavaan' packages. It also implements various procedures to			 perform meta-analytic structural equation modeling on the             correlation and covariance matrices."
"jmv",1,1,21,"2020-02-17 17:40","A suite of common statistical methods such as descriptives,    t-tests, ANOVAs, regression, correlation matrices, proportion tests,    contingency tables, and factor analysis. This package is also useable from    the 'jamovi' statistical spreadsheet (see <https://www.jamovi.org> for more    information)."
"vampyr",0,1,2,"2020-01-20 11:30","Vampirize the response biases from a dataset! Performs     factor analysis controlling the effects of social desirability    and acquiescence using the method described in Ferrando,     Lorenzo-Seva & Chico (2009) <doi:10.1080/10705510902751374>."
"semdrw",0,1,1,"2018-11-14","Interactive 'shiny' application for working with Structural Equation Modelling technique. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/semwebappk/> ."
"lvnet",0,1,7,"2016-11-25 14:14","Estimate, fit and compare Structural Equation Models (SEM) and network models (Gaussian Graphical Models; GGM) using OpenMx. Allows for two possible generalizations to include GGMs in SEM: GGMs can be used between latent variables (latent network modeling; LNM) or between residuals (residual network modeling; RNM). For details, see Epskamp, Rhemtulla and Borsboom (2017) <doi:10.1007/s11336-017-9557-x>."
"lcsm",5,1,2,"2020-06-05 12:40","Helper functions to implement univariate and bivariate latent change score models in R using the 'lavaan' package.  For details about Latent Change Score Modeling (LCSM) see McArdle (2009) <doi:10.1146/annurev.psych.60.110707.163612> and Grimm, An, McArdle, Zonderman and Resnick (2012) <doi:10.1080/10705511.2012.659627>.  The package automatically generates 'lavaan' syntax for different model specifications and varying timepoints.  The 'lavaan' syntax generated by this package can be returned and further specifications can be added manually.  Longitudinal plots as well as simplified path diagrams can be created to visualise data and model specifications.  Estimated model parameters and fit statistics can be extracted as data frames.  Data for different univariate and bivariate LCSM can be simulated by specifying estimates for model parameters to explore their effects.  This package combines the strengths of other R packages like 'lavaan', 'broom', and 'semPlot' by generating 'lavaan' syntax that helps these packages work together."
"bnpa",0,1,2,"2017-01-13 17:16","This project aims to enable the method of Path Analysis to infer causalities              from data. For this we propose a hybrid approach, which uses Bayesian network              structure learning algorithms from data to create the input file for creation of a              PA model. The process is performed in a semi-automatic way by our intermediate              algorithm, allowing novice researchers to create and evaluate their own PA models             from a data set. The references used for this project are:              Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press. <doi:10.1017/S0269888910000275>.              Nagarajan, R., Scutari, M., & Lèbre, S. (2013). Bayesian networks in r. Springer, 122, 125-127. Scutari, M., & Denis, J. B. <doi:10.1007/978-1-4614-6446-4>.             Scutari M (2010). Bayesian networks: with examples in R. Chapman and Hall/CRC. <doi:10.1201/b17065>.              Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software, 48(2), 1 - 36. <doi:10.18637/jss.v048.i02>."
"regsem",1,1,21,"2020-02-19 13:00","Uses both ridge and lasso penalties (and extensions) to penalize    specific parameters in structural equation models. The package offers additional    cost functions, cross validation, and other extensions beyond traditional structural    equation models. Also contains a function to perform exploratory mediation (XMed). "
"psychonetrics",0,1,10,"2020-04-01 23:00","Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search."
"metaSEM",2,2,14,"2019-10-10 14:20","A collection of functions for conducting meta-analysis using a             structural equation modeling (SEM) approach via the 'OpenMx' and             'lavaan' packages. It also implements various procedures to			 perform meta-analytic structural equation modeling on the             correlation and covariance matrices."
"jmv",1,1,21,"2020-02-17 17:40","A suite of common statistical methods such as descriptives,    t-tests, ANOVAs, regression, correlation matrices, proportion tests,    contingency tables, and factor analysis. This package is also useable from    the 'jamovi' statistical spreadsheet (see <https://www.jamovi.org> for more    information)."
"vampyr",0,1,2,"2020-01-20 11:30","Vampirize the response biases from a dataset! Performs     factor analysis controlling the effects of social desirability    and acquiescence using the method described in Ferrando,     Lorenzo-Seva & Chico (2009) <doi:10.1080/10705510902751374>."
"semdrw",0,1,1,"2018-11-14","Interactive 'shiny' application for working with Structural Equation Modelling technique. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/semwebappk/> ."
"lvnet",0,1,7,"2016-11-25 14:14","Estimate, fit and compare Structural Equation Models (SEM) and network models (Gaussian Graphical Models; GGM) using OpenMx. Allows for two possible generalizations to include GGMs in SEM: GGMs can be used between latent variables (latent network modeling; LNM) or between residuals (residual network modeling; RNM). For details, see Epskamp, Rhemtulla and Borsboom (2017) <doi:10.1007/s11336-017-9557-x>."
"lcsm",5,1,2,"2020-06-05 12:40","Helper functions to implement univariate and bivariate latent change score models in R using the 'lavaan' package.  For details about Latent Change Score Modeling (LCSM) see McArdle (2009) <doi:10.1146/annurev.psych.60.110707.163612> and Grimm, An, McArdle, Zonderman and Resnick (2012) <doi:10.1080/10705511.2012.659627>.  The package automatically generates 'lavaan' syntax for different model specifications and varying timepoints.  The 'lavaan' syntax generated by this package can be returned and further specifications can be added manually.  Longitudinal plots as well as simplified path diagrams can be created to visualise data and model specifications.  Estimated model parameters and fit statistics can be extracted as data frames.  Data for different univariate and bivariate LCSM can be simulated by specifying estimates for model parameters to explore their effects.  This package combines the strengths of other R packages like 'lavaan', 'broom', and 'semPlot' by generating 'lavaan' syntax that helps these packages work together."
"bnpa",0,1,2,"2017-01-13 17:16","This project aims to enable the method of Path Analysis to infer causalities              from data. For this we propose a hybrid approach, which uses Bayesian network              structure learning algorithms from data to create the input file for creation of a              PA model. The process is performed in a semi-automatic way by our intermediate              algorithm, allowing novice researchers to create and evaluate their own PA models             from a data set. The references used for this project are:              Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press. <doi:10.1017/S0269888910000275>.              Nagarajan, R., Scutari, M., & Lèbre, S. (2013). Bayesian networks in r. Springer, 122, 125-127. Scutari, M., & Denis, J. B. <doi:10.1007/978-1-4614-6446-4>.             Scutari M (2010). Bayesian networks: with examples in R. Chapman and Hall/CRC. <doi:10.1201/b17065>.              Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software, 48(2), 1 - 36. <doi:10.18637/jss.v048.i02>."
"regsem",1,1,21,"2020-02-19 13:00","Uses both ridge and lasso penalties (and extensions) to penalize    specific parameters in structural equation models. The package offers additional    cost functions, cross validation, and other extensions beyond traditional structural    equation models. Also contains a function to perform exploratory mediation (XMed). "
"psychonetrics",0,1,10,"2020-04-01 23:00","Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search."
"metaSEM",2,2,14,"2019-10-10 14:20","A collection of functions for conducting meta-analysis using a             structural equation modeling (SEM) approach via the 'OpenMx' and             'lavaan' packages. It also implements various procedures to			 perform meta-analytic structural equation modeling on the             correlation and covariance matrices."
"jmv",1,1,21,"2020-02-17 17:40","A suite of common statistical methods such as descriptives,    t-tests, ANOVAs, regression, correlation matrices, proportion tests,    contingency tables, and factor analysis. This package is also useable from    the 'jamovi' statistical spreadsheet (see <https://www.jamovi.org> for more    information)."
"vampyr",0,1,2,"2020-01-20 11:30","Vampirize the response biases from a dataset! Performs     factor analysis controlling the effects of social desirability    and acquiescence using the method described in Ferrando,     Lorenzo-Seva & Chico (2009) <doi:10.1080/10705510902751374>."
"semdrw",0,1,1,"2018-11-14","Interactive 'shiny' application for working with Structural Equation Modelling technique. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/semwebappk/> ."
"lvnet",0,1,7,"2016-11-25 14:14","Estimate, fit and compare Structural Equation Models (SEM) and network models (Gaussian Graphical Models; GGM) using OpenMx. Allows for two possible generalizations to include GGMs in SEM: GGMs can be used between latent variables (latent network modeling; LNM) or between residuals (residual network modeling; RNM). For details, see Epskamp, Rhemtulla and Borsboom (2017) <doi:10.1007/s11336-017-9557-x>."
"lcsm",5,1,2,"2020-06-05 12:40","Helper functions to implement univariate and bivariate latent change score models in R using the 'lavaan' package.  For details about Latent Change Score Modeling (LCSM) see McArdle (2009) <doi:10.1146/annurev.psych.60.110707.163612> and Grimm, An, McArdle, Zonderman and Resnick (2012) <doi:10.1080/10705511.2012.659627>.  The package automatically generates 'lavaan' syntax for different model specifications and varying timepoints.  The 'lavaan' syntax generated by this package can be returned and further specifications can be added manually.  Longitudinal plots as well as simplified path diagrams can be created to visualise data and model specifications.  Estimated model parameters and fit statistics can be extracted as data frames.  Data for different univariate and bivariate LCSM can be simulated by specifying estimates for model parameters to explore their effects.  This package combines the strengths of other R packages like 'lavaan', 'broom', and 'semPlot' by generating 'lavaan' syntax that helps these packages work together."
"bnpa",0,1,2,"2017-01-13 17:16","This project aims to enable the method of Path Analysis to infer causalities              from data. For this we propose a hybrid approach, which uses Bayesian network              structure learning algorithms from data to create the input file for creation of a              PA model. The process is performed in a semi-automatic way by our intermediate              algorithm, allowing novice researchers to create and evaluate their own PA models             from a data set. The references used for this project are:              Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press. <doi:10.1017/S0269888910000275>.              Nagarajan, R., Scutari, M., & Lèbre, S. (2013). Bayesian networks in r. Springer, 122, 125-127. Scutari, M., & Denis, J. B. <doi:10.1007/978-1-4614-6446-4>.             Scutari M (2010). Bayesian networks: with examples in R. Chapman and Hall/CRC. <doi:10.1201/b17065>.              Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software, 48(2), 1 - 36. <doi:10.18637/jss.v048.i02>."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"rFSA",0,1,4,"2017-12-21 18:36","Assists in statistical model building to find optimal and semi-optimal higher order interactions    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible     solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution     algorithm comes up with model forms of a specific type that can have fixed variables, higher order     interactions and their lower order terms."
"nsga3",0,1,1,"2019-02-18","An adaptation of Non-dominated Sorting Genetic Algorithm III for multi objective     feature selection tasks. Non-dominated Sorting Genetic Algorithm III is a genetic algorithm    that solves multiple optimization problems simultaneously by applying a non-dominated sorting     technique. It uses a reference points based selection operator to explore     solution space and preserve diversity. See the original paper by K. Deb and     H. Jain (2014) <doi:10.1109/TEVC.2013.2281534> for a detailed description."
"intePareto",1,1,1,"2019-10-09","Integrative analysis of gene expression (RNA-Seq data), and histone     modification data for user-defined sets of histone marks (ChIP-Seq data) to     discover consistent changes in genes between biological conditions.     Additionally, Pareto optimization is used to prioritize genes based on the     level of consistent changes in both RNA-Seq and ChIP-Seq data."
"trackr",2,1,8,"2018-11-07 01:10","Automatically annotates R-based artifacts	     with relevant descriptive and provenance-related    	     and provides a backend-agnostic storage and	     discoverability system for organizing, retrieving, and	     interrogating such artifacts."
"histry",2,1,4,"2018-06-25 06:42","Automatically tracks and makes programmatically available code    evaluation history in R sessions and dynamic documents."
"trackr",2,1,8,"2018-11-07 01:10","Automatically annotates R-based artifacts	     with relevant descriptive and provenance-related    	     and provides a backend-agnostic storage and	     discoverability system for organizing, retrieving, and	     interrogating such artifacts."
"histry",2,1,4,"2018-06-25 06:42","Automatically tracks and makes programmatically available code    evaluation history in R sessions and dynamic documents."
"SpaDES.tools",0,3,10,"2020-05-15 22:10","Provides GIS and map utilities, plus additional modeling tools for    developing cellular automata, dynamic raster models,  and agent based models    in 'SpaDES'.    Included are various methods for spatial spreading, spatial agents, GIS    operations, random map generation, and others.    See '?SpaDES.tools' for an categorized overview of these additional tools."
"SpaDES",1,1,14,"2019-02-03 17:53","Metapackage for implementing a variety of event-based models, with    a focus on spatially explicit models. These include raster-based,    event-based, and agent-based models. The core simulation components    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)    framework that facilitates modularity, and easily enables the user to    include additional functionality by running user-built simulation modules    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters    and other maps (via 'quickPlot'), and caching methods for reproducible    simulations (via 'reproducible'). Additional functionality is provided by    the 'SpaDES.addins' and 'SpaDES.shiny' packages."
"NetLogoR",2,1,7,"2019-11-27 13:00","Build and run spatially explicit    agent-based models using only the R platform. 'NetLogoR' follows the same    framework as the 'NetLogo' software    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation    in R of the structure and functions of 'NetLogo'.    'NetLogoR' provides new R classes to define model agents and functions to    implement spatially explicit agent-based models in the R environment.    This package allows benefiting of the fast and easy coding phase from the    highly developed 'NetLogo' framework, coupled with the versatility, power    and massive resources of the R software.    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using    'NetLogoR' are available. The 'NetLogo' code of the original version of these    models is provided alongside.    A programming guide inspired from the 'NetLogo' Programming Guide    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)    equivalences are also available.    NOTE: To increment 'time', these functions can use a for loop or can be    integrated with a discrete event simulator, such as 'SpaDES'    (<https://cran.r-project.org/package=SpaDES>).    The suggested package 'fastshp' can be installed with    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'."
"SpaDES.tools",0,3,10,"2020-05-15 22:10","Provides GIS and map utilities, plus additional modeling tools for    developing cellular automata, dynamic raster models,  and agent based models    in 'SpaDES'.    Included are various methods for spatial spreading, spatial agents, GIS    operations, random map generation, and others.    See '?SpaDES.tools' for an categorized overview of these additional tools."
"SpaDES",1,1,14,"2019-02-03 17:53","Metapackage for implementing a variety of event-based models, with    a focus on spatially explicit models. These include raster-based,    event-based, and agent-based models. The core simulation components    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)    framework that facilitates modularity, and easily enables the user to    include additional functionality by running user-built simulation modules    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters    and other maps (via 'quickPlot'), and caching methods for reproducible    simulations (via 'reproducible'). Additional functionality is provided by    the 'SpaDES.addins' and 'SpaDES.shiny' packages."
"NetLogoR",2,1,7,"2019-11-27 13:00","Build and run spatially explicit    agent-based models using only the R platform. 'NetLogoR' follows the same    framework as the 'NetLogo' software    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation    in R of the structure and functions of 'NetLogo'.    'NetLogoR' provides new R classes to define model agents and functions to    implement spatially explicit agent-based models in the R environment.    This package allows benefiting of the fast and easy coding phase from the    highly developed 'NetLogo' framework, coupled with the versatility, power    and massive resources of the R software.    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using    'NetLogoR' are available. The 'NetLogo' code of the original version of these    models is provided alongside.    A programming guide inspired from the 'NetLogo' Programming Guide    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)    equivalences are also available.    NOTE: To increment 'time', these functions can use a for loop or can be    integrated with a discrete event simulator, such as 'SpaDES'    (<https://cran.r-project.org/package=SpaDES>).    The suggested package 'fastshp' can be installed with    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'."
"SpaDES.tools",0,3,10,"2020-05-15 22:10","Provides GIS and map utilities, plus additional modeling tools for    developing cellular automata, dynamic raster models,  and agent based models    in 'SpaDES'.    Included are various methods for spatial spreading, spatial agents, GIS    operations, random map generation, and others.    See '?SpaDES.tools' for an categorized overview of these additional tools."
"SpaDES",1,1,14,"2019-02-03 17:53","Metapackage for implementing a variety of event-based models, with    a focus on spatially explicit models. These include raster-based,    event-based, and agent-based models. The core simulation components    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)    framework that facilitates modularity, and easily enables the user to    include additional functionality by running user-built simulation modules    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters    and other maps (via 'quickPlot'), and caching methods for reproducible    simulations (via 'reproducible'). Additional functionality is provided by    the 'SpaDES.addins' and 'SpaDES.shiny' packages."
"NetLogoR",2,1,7,"2019-11-27 13:00","Build and run spatially explicit    agent-based models using only the R platform. 'NetLogoR' follows the same    framework as the 'NetLogo' software    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation    in R of the structure and functions of 'NetLogo'.    'NetLogoR' provides new R classes to define model agents and functions to    implement spatially explicit agent-based models in the R environment.    This package allows benefiting of the fast and easy coding phase from the    highly developed 'NetLogo' framework, coupled with the versatility, power    and massive resources of the R software.    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using    'NetLogoR' are available. The 'NetLogo' code of the original version of these    models is provided alongside.    A programming guide inspired from the 'NetLogo' Programming Guide    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)    equivalences are also available.    NOTE: To increment 'time', these functions can use a for loop or can be    integrated with a discrete event simulator, such as 'SpaDES'    (<https://cran.r-project.org/package=SpaDES>).    The suggested package 'fastshp' can be installed with    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'."
"SpaDES.tools",0,3,10,"2020-05-15 22:10","Provides GIS and map utilities, plus additional modeling tools for    developing cellular automata, dynamic raster models,  and agent based models    in 'SpaDES'.    Included are various methods for spatial spreading, spatial agents, GIS    operations, random map generation, and others.    See '?SpaDES.tools' for an categorized overview of these additional tools."
"SpaDES",1,1,14,"2019-02-03 17:53","Metapackage for implementing a variety of event-based models, with    a focus on spatially explicit models. These include raster-based,    event-based, and agent-based models. The core simulation components    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)    framework that facilitates modularity, and easily enables the user to    include additional functionality by running user-built simulation modules    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters    and other maps (via 'quickPlot'), and caching methods for reproducible    simulations (via 'reproducible'). Additional functionality is provided by    the 'SpaDES.addins' and 'SpaDES.shiny' packages."
"NetLogoR",2,1,7,"2019-11-27 13:00","Build and run spatially explicit    agent-based models using only the R platform. 'NetLogoR' follows the same    framework as the 'NetLogo' software    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation    in R of the structure and functions of 'NetLogo'.    'NetLogoR' provides new R classes to define model agents and functions to    implement spatially explicit agent-based models in the R environment.    This package allows benefiting of the fast and easy coding phase from the    highly developed 'NetLogo' framework, coupled with the versatility, power    and massive resources of the R software.    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using    'NetLogoR' are available. The 'NetLogo' code of the original version of these    models is provided alongside.    A programming guide inspired from the 'NetLogo' Programming Guide    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)    equivalences are also available.    NOTE: To increment 'time', these functions can use a for loop or can be    integrated with a discrete event simulator, such as 'SpaDES'    (<https://cran.r-project.org/package=SpaDES>).    The suggested package 'fastshp' can be installed with    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'."
"RSA",0,1,16,"2020-04-29 11:40","Advanced response surface analysis. The main function RSA computes    and compares several nested polynomial regression models (full second- or     third-order polynomial, shifted and rotated squared difference model,     rising ridge surfaces, basic squared difference model, asymmetric or     level-dependent congruence effect models). The package provides plotting     functions for 3d wireframe surfaces, interactive 3d plots, and contour plots.     Calculates many surface parameters (a1 to a5, principal axes, stationary point,    eigenvalues) and provides standard, robust, or bootstrapped standard errors    and confidence intervals for them."
"RcmdrPlugin.pointG",0,1,3,"2012-02-17 08:38","This package provides a Rcmdr ""plug-in"" to analyze questionnaire data."
"MultiFit",1,1,5,"2019-03-17 19:43","Test for independence of two random vectors, learn and report the dependency structure. For more information, see Gorsky and Ma (2018) <arXiv:1806.06777>."
"GGMncv",0,1,2,"2020-07-06 14:40","Estimate Gaussian graphical models with non-convex penalties, including   the atan Wang and Zhu (2016) <doi:10.1155/2016/6495417>,   seamless L0 Dicker, Huang, and Lin (2013) <doi:10.5705/ss.2011.074>,  exponential Wang, Fan, and Zhu <doi:10.1007/s10463-016-0588-3>,   smooth integration of counting and absolute deviation Lv and Fan (2009) <doi:10.1214/09-AOS683>,  logarithm Mazumder, Friedman, and Hastie (2011) <doi:10.1198/jasa.2011.tm09738>,  Lq, smoothly clipped absolute deviation Fan and Li (2001) <doi:10.1198/016214501753382273>,  and minimax concave penalty Zhang (2010) <doi:10.1214/09-AOS729>. There are also extensions  for computing variable inclusion probabilities, multiple regression coefficients, and   statistical inference <doi:10.1214/15-EJS1031>."
"bmlm",0,1,13,"2018-11-09 16:10","Easy estimation of Bayesian multilevel mediation models with Stan."
"veccompare",0,1,1,"2017-09-15","Automates set operations (i.e., comparisons of overlap) between multiple vectors.    It also contains a function for automating reporting in 'RMarkdown', by generating markdown output for easy analysis, as well as an 'RMarkdown' template for use with 'RStudio'."
"psychonetrics",0,1,10,"2020-04-01 23:00","Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search."
"pompom",1,1,2,"2018-01-11 13:04","An implementation of a hybrid method of person-oriented method and perturbation on the model. Pompom is the initials of the two methods. The hybrid method will provide a multivariate intraindividual variability metric (iRAM). The person-oriented method used in this package refers to uSEM (unified structural equation modeling, see Kim et al., 2007, Gates et al., 2010 and Gates et al., 2012 for details). Perturbation on the model was conducted according to impulse response analysis introduced in Lutkepohl (2007).     Kim, J., Zhu, W., Chang, L., Bentler, P. M., & Ernst, T. (2007) <doi:10.1002/hbm.20259>.     Gates, K. M., Molenaar, P. C. M., Hillary, F. G., Ram, N., & Rovine, M. J. (2010) <doi:10.1016/j.neuroimage.2009.12.117>.     Gates, K. M., & Molenaar, P. C. M. (2012) <doi:10.1016/j.neuroimage.2012.06.026>.     Lutkepohl, H. (2007, ISBN:3540262393)."
"networktree",0,1,3,"2019-03-09 14:10","Methods to create tree models with correlation-based network models (multivariate normal distributions). "
"NetworkComparisonTest",0,1,3,"2016-10-29 00:25","This permutation based hypothesis test, suited for Gaussian and binary data,     assesses the difference between two networks based on several invariance measures     (e.g., network structure invariance, global strength invariance, edge invariance).     Network structures are estimated with l1-regularized partial correlations (Gaussian data)     or with l1-regularized logistic regression (eLasso, binary data). Suited for comparison     of independent and dependent samples. For dependent samples, only supported for data of     one group which is measured twice. See van Borkulo et al. (2017)     <doi:10.13140/RG.2.2.29455.38569>."
"mlVAR",0,1,10,"2017-09-02 18:22","Estimates the multi-level vector autoregression model on time-series data.             Three network structures are obtained: temporal networks, contemporaneous             networks and between-subjects networks."
"mgm",0,2,16,"2020-04-20 16:00","Estimation of k-Order time-varying Mixed Graphical Models and mixed VAR(p) models via elastic-net regularized neighborhood regression. For details see Haslbeck & Waldorp (2020) <doi:10.18637/jss.v093.i08>."
"lvnet",0,1,7,"2016-11-25 14:14","Estimate, fit and compare Structural Equation Models (SEM) and network models (Gaussian Graphical Models; GGM) using OpenMx. Allows for two possible generalizations to include GGMs in SEM: GGMs can be used between latent variables (latent network modeling; LNM) or between residuals (residual network modeling; RNM). For details, see Epskamp, Rhemtulla and Borsboom (2017) <doi:10.1007/s11336-017-9557-x>."
"IsingFit",0,4,4,"2014-10-23 11:45","This network estimation procedure eLasso, which is based on the Ising model, combines l1-regularized logistic regression with model selection based on the Extended Bayesian Information Criterion (EBIC). EBIC is a fit measure that identifies relevant relationships between variables. The resulting network consists of variables as nodes and relevant relationships as edges. Can deal with binary data."
"IATscores",0,1,5,"2019-12-18 17:50","Compute several variations of the Implicit Association Test (IAT) scores, including the D scores (Greenwald, Nosek, Banaji, 2003, <doi:10.1037/0022-3514.85.2.197>) and the new scores that were developed using robust statistics (Richetin, Costantini, Perugini, and Schonbrodt, 2015, <doi:10.1371/journal.pone.0129601>). "
"elasticIsing",0,1,1,"2016-09-11","Description: Uses k-fold cross-validation and elastic-net regularization to estimate the              Ising model on binary data. Produces 3D plots of the cost function as a function              of the tuning parameter in addition to the optimal network structure."
"diveRsity",1,3,17,"2016-12-09 08:03","Allows the calculation of both genetic diversity partition   statistics, genetic differentiation statistics, and locus informativeness   for ancestry assignment.   It also provides users with various option to calculate   bootstrapped 95\% confidence intervals both across loci,   for pairwise population comparisons, and to plot these results interactively.   Parallel computing capabilities and pairwise results without   bootstrapping are provided.   Also calculates F-statistics from Weir and Cockerham (1984).   Various plotting features are provided, as well as Chi-square tests of   genetic heterogeneity.   Functionality for the calculation of various diversity parameters is   possible for RAD-seq derived SNP data sets containing thousands of marker loci.  A shiny application for the development of microsatellite multiplexes is   also available."
"RSA",0,1,16,"2020-04-29 11:40","Advanced response surface analysis. The main function RSA computes    and compares several nested polynomial regression models (full second- or     third-order polynomial, shifted and rotated squared difference model,     rising ridge surfaces, basic squared difference model, asymmetric or     level-dependent congruence effect models). The package provides plotting     functions for 3d wireframe surfaces, interactive 3d plots, and contour plots.     Calculates many surface parameters (a1 to a5, principal axes, stationary point,    eigenvalues) and provides standard, robust, or bootstrapped standard errors    and confidence intervals for them."
"RcmdrPlugin.pointG",0,1,3,"2012-02-17 08:38","This package provides a Rcmdr ""plug-in"" to analyze questionnaire data."
"MultiFit",1,1,5,"2019-03-17 19:43","Test for independence of two random vectors, learn and report the dependency structure. For more information, see Gorsky and Ma (2018) <arXiv:1806.06777>."
"GGMncv",0,1,2,"2020-07-06 14:40","Estimate Gaussian graphical models with non-convex penalties, including   the atan Wang and Zhu (2016) <doi:10.1155/2016/6495417>,   seamless L0 Dicker, Huang, and Lin (2013) <doi:10.5705/ss.2011.074>,  exponential Wang, Fan, and Zhu <doi:10.1007/s10463-016-0588-3>,   smooth integration of counting and absolute deviation Lv and Fan (2009) <doi:10.1214/09-AOS683>,  logarithm Mazumder, Friedman, and Hastie (2011) <doi:10.1198/jasa.2011.tm09738>,  Lq, smoothly clipped absolute deviation Fan and Li (2001) <doi:10.1198/016214501753382273>,  and minimax concave penalty Zhang (2010) <doi:10.1214/09-AOS729>. There are also extensions  for computing variable inclusion probabilities, multiple regression coefficients, and   statistical inference <doi:10.1214/15-EJS1031>."
"bmlm",0,1,13,"2018-11-09 16:10","Easy estimation of Bayesian multilevel mediation models with Stan."
"veccompare",0,1,1,"2017-09-15","Automates set operations (i.e., comparisons of overlap) between multiple vectors.    It also contains a function for automating reporting in 'RMarkdown', by generating markdown output for easy analysis, as well as an 'RMarkdown' template for use with 'RStudio'."
"psychonetrics",0,1,10,"2020-04-01 23:00","Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search."
"pompom",1,1,2,"2018-01-11 13:04","An implementation of a hybrid method of person-oriented method and perturbation on the model. Pompom is the initials of the two methods. The hybrid method will provide a multivariate intraindividual variability metric (iRAM). The person-oriented method used in this package refers to uSEM (unified structural equation modeling, see Kim et al., 2007, Gates et al., 2010 and Gates et al., 2012 for details). Perturbation on the model was conducted according to impulse response analysis introduced in Lutkepohl (2007).     Kim, J., Zhu, W., Chang, L., Bentler, P. M., & Ernst, T. (2007) <doi:10.1002/hbm.20259>.     Gates, K. M., Molenaar, P. C. M., Hillary, F. G., Ram, N., & Rovine, M. J. (2010) <doi:10.1016/j.neuroimage.2009.12.117>.     Gates, K. M., & Molenaar, P. C. M. (2012) <doi:10.1016/j.neuroimage.2012.06.026>.     Lutkepohl, H. (2007, ISBN:3540262393)."
"networktree",0,1,3,"2019-03-09 14:10","Methods to create tree models with correlation-based network models (multivariate normal distributions). "
"NetworkComparisonTest",0,1,3,"2016-10-29 00:25","This permutation based hypothesis test, suited for Gaussian and binary data,     assesses the difference between two networks based on several invariance measures     (e.g., network structure invariance, global strength invariance, edge invariance).     Network structures are estimated with l1-regularized partial correlations (Gaussian data)     or with l1-regularized logistic regression (eLasso, binary data). Suited for comparison     of independent and dependent samples. For dependent samples, only supported for data of     one group which is measured twice. See van Borkulo et al. (2017)     <doi:10.13140/RG.2.2.29455.38569>."
"mlVAR",0,1,10,"2017-09-02 18:22","Estimates the multi-level vector autoregression model on time-series data.             Three network structures are obtained: temporal networks, contemporaneous             networks and between-subjects networks."
"mgm",0,2,16,"2020-04-20 16:00","Estimation of k-Order time-varying Mixed Graphical Models and mixed VAR(p) models via elastic-net regularized neighborhood regression. For details see Haslbeck & Waldorp (2020) <doi:10.18637/jss.v093.i08>."
"lvnet",0,1,7,"2016-11-25 14:14","Estimate, fit and compare Structural Equation Models (SEM) and network models (Gaussian Graphical Models; GGM) using OpenMx. Allows for two possible generalizations to include GGMs in SEM: GGMs can be used between latent variables (latent network modeling; LNM) or between residuals (residual network modeling; RNM). For details, see Epskamp, Rhemtulla and Borsboom (2017) <doi:10.1007/s11336-017-9557-x>."
"IsingFit",0,4,4,"2014-10-23 11:45","This network estimation procedure eLasso, which is based on the Ising model, combines l1-regularized logistic regression with model selection based on the Extended Bayesian Information Criterion (EBIC). EBIC is a fit measure that identifies relevant relationships between variables. The resulting network consists of variables as nodes and relevant relationships as edges. Can deal with binary data."
"IATscores",0,1,5,"2019-12-18 17:50","Compute several variations of the Implicit Association Test (IAT) scores, including the D scores (Greenwald, Nosek, Banaji, 2003, <doi:10.1037/0022-3514.85.2.197>) and the new scores that were developed using robust statistics (Richetin, Costantini, Perugini, and Schonbrodt, 2015, <doi:10.1371/journal.pone.0129601>). "
"elasticIsing",0,1,1,"2016-09-11","Description: Uses k-fold cross-validation and elastic-net regularization to estimate the              Ising model on binary data. Produces 3D plots of the cost function as a function              of the tuning parameter in addition to the optimal network structure."
"diveRsity",1,3,17,"2016-12-09 08:03","Allows the calculation of both genetic diversity partition   statistics, genetic differentiation statistics, and locus informativeness   for ancestry assignment.   It also provides users with various option to calculate   bootstrapped 95\% confidence intervals both across loci,   for pairwise population comparisons, and to plot these results interactively.   Parallel computing capabilities and pairwise results without   bootstrapping are provided.   Also calculates F-statistics from Weir and Cockerham (1984).   Various plotting features are provided, as well as Chi-square tests of   genetic heterogeneity.   Functionality for the calculation of various diversity parameters is   possible for RAD-seq derived SNP data sets containing thousands of marker loci.  A shiny application for the development of microsatellite multiplexes is   also available."
"mgsub",2,2,5,"2019-03-13 12:10","Designed to enable simultaneous substitution in strings in a safe fashion.    Safe means it does not rely on placeholders (which can cause errors in same length matches)."
"iemiscdata",2,2,5,"2016-07-14 07:26","Miscellaneous data sets [Engineering Economics, Environmental/    Water Resources Engineering, US Presidential Elections]."
"NLPutils",0,1,5,"2016-12-08 21:04","Utilities for Natural Language Processing."
"PLMIX",0,1,5,"2019-08-04 01:10","Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi.org/10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi/10.1002/sim.6224>."
"ClimMobTools",1,1,4,"2020-03-16 15:00","API client for 'ClimMob', an open source software for crowdsourcing     citizen science in agriculture under the 'tricot' method <https://climmob.net/>.     Developed by van Etten et al. (2019) <doi:10.1017/S0014479716000739>, it turns the     research paradigm on its head; instead of a few researchers designing complicated     trials to compare several technologies in search of the best solutions,     it enables many farmers to carry out reasonably simple experiments that     taken together can offer even more information. 'ClimMobTools' enables project     managers to deep explore and analyse their 'ClimMob' data in R."
"PLMIX",0,1,5,"2019-08-04 01:10","Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi.org/10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi/10.1002/sim.6224>."
"ClimMobTools",1,1,4,"2020-03-16 15:00","API client for 'ClimMob', an open source software for crowdsourcing     citizen science in agriculture under the 'tricot' method <https://climmob.net/>.     Developed by van Etten et al. (2019) <doi:10.1017/S0014479716000739>, it turns the     research paradigm on its head; instead of a few researchers designing complicated     trials to compare several technologies in search of the best solutions,     it enables many farmers to carry out reasonably simple experiments that     taken together can offer even more information. 'ClimMobTools' enables project     managers to deep explore and analyse their 'ClimMob' data in R."
"PLMIX",0,1,5,"2019-08-04 01:10","Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi.org/10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi/10.1002/sim.6224>."
"ClimMobTools",1,1,4,"2020-03-16 15:00","API client for 'ClimMob', an open source software for crowdsourcing     citizen science in agriculture under the 'tricot' method <https://climmob.net/>.     Developed by van Etten et al. (2019) <doi:10.1017/S0014479716000739>, it turns the     research paradigm on its head; instead of a few researchers designing complicated     trials to compare several technologies in search of the best solutions,     it enables many farmers to carry out reasonably simple experiments that     taken together can offer even more information. 'ClimMobTools' enables project     managers to deep explore and analyse their 'ClimMob' data in R."
"PLMIX",0,1,5,"2019-08-04 01:10","Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi.org/10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi/10.1002/sim.6224>."
"ClimMobTools",1,1,4,"2020-03-16 15:00","API client for 'ClimMob', an open source software for crowdsourcing     citizen science in agriculture under the 'tricot' method <https://climmob.net/>.     Developed by van Etten et al. (2019) <doi:10.1017/S0014479716000739>, it turns the     research paradigm on its head; instead of a few researchers designing complicated     trials to compare several technologies in search of the best solutions,     it enables many farmers to carry out reasonably simple experiments that     taken together can offer even more information. 'ClimMobTools' enables project     managers to deep explore and analyse their 'ClimMob' data in R."
"PLMIX",0,1,5,"2019-08-04 01:10","Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi.org/10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi/10.1002/sim.6224>."
"ClimMobTools",1,1,4,"2020-03-16 15:00","API client for 'ClimMob', an open source software for crowdsourcing     citizen science in agriculture under the 'tricot' method <https://climmob.net/>.     Developed by van Etten et al. (2019) <doi:10.1017/S0014479716000739>, it turns the     research paradigm on its head; instead of a few researchers designing complicated     trials to compare several technologies in search of the best solutions,     it enables many farmers to carry out reasonably simple experiments that     taken together can offer even more information. 'ClimMobTools' enables project     managers to deep explore and analyse their 'ClimMob' data in R."
"PLMIX",0,1,5,"2019-08-04 01:10","Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi.org/10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi/10.1002/sim.6224>."
"ClimMobTools",1,1,4,"2020-03-16 15:00","API client for 'ClimMob', an open source software for crowdsourcing     citizen science in agriculture under the 'tricot' method <https://climmob.net/>.     Developed by van Etten et al. (2019) <doi:10.1017/S0014479716000739>, it turns the     research paradigm on its head; instead of a few researchers designing complicated     trials to compare several technologies in search of the best solutions,     it enables many farmers to carry out reasonably simple experiments that     taken together can offer even more information. 'ClimMobTools' enables project     managers to deep explore and analyse their 'ClimMob' data in R."
"PLMIX",0,1,5,"2019-08-04 01:10","Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi.org/10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi/10.1002/sim.6224>."
"ClimMobTools",1,1,4,"2020-03-16 15:00","API client for 'ClimMob', an open source software for crowdsourcing     citizen science in agriculture under the 'tricot' method <https://climmob.net/>.     Developed by van Etten et al. (2019) <doi:10.1017/S0014479716000739>, it turns the     research paradigm on its head; instead of a few researchers designing complicated     trials to compare several technologies in search of the best solutions,     it enables many farmers to carry out reasonably simple experiments that     taken together can offer even more information. 'ClimMobTools' enables project     managers to deep explore and analyse their 'ClimMob' data in R."
"PLMIX",0,1,5,"2019-08-04 01:10","Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi.org/10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi/10.1002/sim.6224>."
"ClimMobTools",1,1,4,"2020-03-16 15:00","API client for 'ClimMob', an open source software for crowdsourcing     citizen science in agriculture under the 'tricot' method <https://climmob.net/>.     Developed by van Etten et al. (2019) <doi:10.1017/S0014479716000739>, it turns the     research paradigm on its head; instead of a few researchers designing complicated     trials to compare several technologies in search of the best solutions,     it enables many farmers to carry out reasonably simple experiments that     taken together can offer even more information. 'ClimMobTools' enables project     managers to deep explore and analyse their 'ClimMob' data in R."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"photosynthesis",2,1,3,"2020-07-01 11:40","Contains modeling and analytical tools for plant ecophysiology.    MODELING: Simulate C3 photosynthesis using the Farquhar, von Caemmerer,    Berry (1980) <doi:10.1007/BF00386231> model as described in Buckley and    Diaz-Espejo (2015) <doi:10.1111/pce.12459>. It uses units to ensure that    parameters are properly specified and transformed before calculations.    Temperature response functions get automatically ""baked"" into all    parameters based on leaf temperature following Bernacchi et al. (2002)    <doi:10.1104/pp.008250>. The package includes boundary layer, cuticular,    stomatal, and mesophyll conductances to CO2, which each can vary on the    upper and lower portions of the leaf. Use straightforward functions to    simulate photosynthesis over environmental gradients such as Photosynthetic    Photon Flux Density (PPFD) and leaf temperature, or over trait gradients    such as CO2 conductance or photochemistry.     ANALYTICAL TOOLS: Fit ACi (Farquhar et al. 1980 <doi:10.1007/BF00386231>)    and AQ curves (Marshall & Biscoe 1980 <doi:10.1093/jxb/31.1.29>),    temperature responses (Heskel et al. 2016 <doi:10.1073/pnas.1520282113>;    Kruse et al. 2008 <doi:10.1111/j.1365-3040.2008.01809.x>, Medlyn et al.    2002 <doi:10.1046/j.1365-3040.2002.00891.x>, Hobbs et al. 2013    <doi:10.1021/cb4005029>), respiration in the light (Kok 1956    <doi:10.1016/0006-3002(56)90003-8>, Walker & Ort 2015 <doi:10.1111/pce.12562>,    Yin et al. 2009 <doi:10.1111/j.1365-3040.2009.01934.x>, Yin et al. 2011    <doi:10.1093/jxb/err038>), mesophyll conductance (Harley et al. 1992    <doi:10.1104/pp.98.4.1429>), pressure-volume curves (Koide et al. 2000    <doi:10.1007/978-94-009-2221-1_9>, Sack et al. 2003    <doi:10.1046/j.0016-8025.2003.01058.x>, Tyree et al. 1972    <doi:10.1093/jxb/23.1.267>), hydraulic vulnerability curves (Ogle et al. 2009    <doi:10.1111/j.1469-8137.2008.02760.x>, Pammenter et al. 1998    <doi:10.1093/treephys/18.8-9.589>), and tools for running sensitivity    analyses particularly for variables with uncertainty (e.g. g_mc, gamma_star,    R_d)."
"tracerer",1,2,4,"2019-12-02 13:10","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'Tracer' (<http://tree.bio.ed.ac.uk/software/tracer/>) is a GUI tool   to parse and analyze the files generated by 'BEAST2'.  This package provides a way to parse and analyze 'BEAST2' input  files without active user input, but using  R function calls instead."
"Quartet",2,2,5,"2019-12-30 18:30","Calculates the number of four-taxon subtrees consistent with a pair  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),  Reconstructing the shape of a tree from observed dissimilarity data,  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>,   and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for  computing the quartet and triplet distances between binary or general trees,  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>  for pairs of binary trees."
"CongreveLamsdell2016",3,1,3,"2019-02-07 15:13","Includes the 100 datasets simulated by Congreve and Lamsdell (2016)  <doi:10.1111/pala.12236>, and analyses of the partition and quartet distance of  reconstructed trees from the generative tree, as analysed by Smith (2019)  <doi:10.1098/rsbl.2018.0632>."
"TreeTools",3,4,8,"2020-09-22 18:00","Efficient implementations of functions for the creation,   modification and analysis of phylogenetic trees.  Applications include:  generation of trees with specified shapes;  analysis of tree shape;  rooting of trees and extraction of subtrees;  calculation and depiction of node support;  calculation of ancestor-descendant relationships;  import and export of trees from Newick, Nexus (Maddison et al. 1997)  <doi:10.1093/sysbio/46.4.590>,  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;  and analysis of splits and cladistic information."
"TreeSearch",4,2,13,"2020-07-07 13:00","Searches for phylogenetic trees that are optimal using a   user-defined criterion.   Handles inapplicable data using the algorithm of   Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.  Implements Profile Parsimony (Faith and Trueman, 2001)   <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969)   <doi:10.2307/2412182>."
"TreeDist",5,1,4,"2020-08-28 13:30","Implements measures of tree similarity, including   information-based generalized Robinson-Foulds distances  (Phylogenetic Information Distance, Clustering Information Distance,  Matching Split Information Distance; Smith, 2020)  <doi:10.1093/bioinformatics/btaa614>;   Jaccard-Robinson-Foulds distances (Bocker et al. 2013)  <doi:10.1007/978-3-642-40453-5_13>,   including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;  the Matching Split Distance (Bogdanowicz & Giaro 2012)  <doi:10.1109/TCBB.2011.48>;  Maximum Agreement Subtree distances;  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al.   (1996) <doi:10.1007/3-540-61332-3_168>.  Calculates the median of a set of trees under any distance metric."
"timetree",0,1,1,"2015-04-28","A interface to the TimeTree of Life Webpage (www.timetree.org). TimeTree is a public database for information on the evolutionary timescale of life. This package includes functions for searching divergence time for  taxa or all nodes of a phylogeny."
"SeqFeatR",1,1,10,"2018-11-30 14:30","Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>."
"rwty",1,1,3,"2016-06-22 05:42","Implements various tests, visualizations, and metrics    for diagnosing convergence of MCMC chains in phylogenetics.  It implements    and automates many of the functions of the AWTY package in the R    environment."
"phytools",0,40,40,"2020-06-01 23:00","A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research."
"pcmabc",0,1,6,"2018-12-18 11:30","Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. Both distory and geiger are orphaned at the moment."
"paleotree",0,2,25,"2019-06-05 00:20","Provides tools for transforming, a posteriori time-scaling, and    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of    fossil taxa using stratigraphic data. This package also contains a large number    of likelihood functions for estimating sampling and diversification rates from    different types of data available from the fossil record (e.g. range data,    occurrence data, etc). paleotree users can also simulate diversification and    sampling in the fossil record using the function simFossilRecord, which is a    detailed simulator for branching birth-death-sampling processes composed of    discrete taxonomic units arranged in ancestor-descendant relationships. Users    can use simFossilRecord to simulate diversification in incompletely sampled    fossil records, under various models of morphological differentiation (i.e.    the various patterns by which morphotaxa originate from one another), and    with time-dependent, longevity-dependent and/or diversity-dependent rates of    diversification, extinction and sampling. Additional functions allow users to    translate simulated ancestor-descendant data from simFossilRecord into standard    time-scaled phylogenies or unscaled cladograms that reflect the relationships    among taxon units."
"markophylo",1,1,7,"2019-03-22 16:00","Allows for fitting of maximum likelihood models using Markov chains    on phylogenetic trees for analysis of discrete character data. Examples of such    discrete character data include restriction sites, gene family presence/absence,    intron presence/absence, and gene family size data. Hypothesis-driven user-    specified substitution rate matrices can be estimated. Allows for biologically    realistic models combining constrained substitution rate matrices, site rate    variation, site partitioning, branch-specific rates, allowing for non-stationary    prior root probabilities, correcting for sampling bias, etc. See Dang and Golding     (2016) <doi:10.1093/bioinformatics/btv541> for more details."
"ips",0,1,2,"2014-11-10 00:38","Functions that wrap popular phylogenetic software for sequence    alignment, masking of sequence alignments, and estimation of phylogenies and    ancestral character states."
"indelmiss",0,1,3,"2018-01-30 19:41","Genome-wide gene insertion and deletion rates can be modelled in a maximum     likelihood framework with the additional flexibility of modelling potential missing     data using the models included within. These models simultaneously estimate insertion     and deletion (indel) rates of gene families and proportions of ""missing"" data for     (multiple) taxa of interest. The likelihood framework is utilized for parameter     estimation. A phylogenetic tree of the taxa and gene presence/absence patterns     (with data ordered by the tips of the tree) are required. See Dang et al.    (2016) <doi:10.1534/genetics.116.191973> for more details."
"haplotypes",0,1,4,"2019-03-16 23:50","Provides S4 classes and methods for reading and manipulating aligned DNA sequences, supporting an indel coding methods (only simple indel coding method is available in the current version), showing base substitutions and indels, calculating absolute pairwise distances between DNA sequences, and collapses identical DNA sequences into haplotypes or inferring haplotypes using user provided absolute pairwise character difference matrix.  This package also includes S4 classes and methods for estimating genealogical relationships among haplotypes using statistical parsimony and plotting parsimony networks.  "
"CommT",0,1,1,"2015-06-16","Provides functions to measure the difference between constrained and unconstrained gene tree distributions using various tree distance metrics. Constraints are enforced prior to this analysis via the estimation of a tree under the community tree model."
"coalescentMCMC",2,1,5,"2013-12-04 09:41","Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters."
"BSS",0,1,1,"2020-06-24","Efficient simulation of Brownian semistationary (BSS) processes using the hybrid simulation scheme, as described in     Bennedsen, Lunde, Pakkannen (2017) <arXiv:1507.03004v4>, as well as functions to fit BSS processes    to data, and functions to estimate the stochastic volatility process of a BSS process."
"BoSSA",3,1,14,"2018-09-25 21:10","Reads and plots phylogenetic placements."
"beastier",1,2,4,"2019-12-02 13:50","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is a command-line tool.    This package provides a way to call 'BEAST2'     from an 'R' function call."
"babette",5,1,3,"2020-02-01 21:00","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is commonly accompanied by 'BEAUti 2', 'Tracer' and 'DensiTree'.    'babette' provides for an alternative workflow of using     all these tools separately. This allows doing complex Bayesian    phylogenetics easily and reproducibly from 'R'. "
"MSCquartets",0,1,1,"2019-12-16","Methods for analyzing and using quartets displayed on a collection of gene trees,       primarily to make inferences about the species tree or network under the multi-species       coalescent model. These include quartet hypothesis tests for the model, as developed by       Mitchell et al. (2019) <doi:10.1214/19-EJS1576>, the species tree inference routines       based on quartet distances of Rhodes (2019) <doi:10.1109/TCBB.2019.2917204> and       Yourdkhani and Rhodes (2019), and the NANUQ algorithm for inference of level-1       species networks of Allman et al. (2019) <arXiv:1905.07050>."
"tracerer",1,2,4,"2019-12-02 13:10","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'Tracer' (<http://tree.bio.ed.ac.uk/software/tracer/>) is a GUI tool   to parse and analyze the files generated by 'BEAST2'.  This package provides a way to parse and analyze 'BEAST2' input  files without active user input, but using  R function calls instead."
"Quartet",2,2,5,"2019-12-30 18:30","Calculates the number of four-taxon subtrees consistent with a pair  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),  Reconstructing the shape of a tree from observed dissimilarity data,  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>,   and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for  computing the quartet and triplet distances between binary or general trees,  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>  for pairs of binary trees."
"CongreveLamsdell2016",3,1,3,"2019-02-07 15:13","Includes the 100 datasets simulated by Congreve and Lamsdell (2016)  <doi:10.1111/pala.12236>, and analyses of the partition and quartet distance of  reconstructed trees from the generative tree, as analysed by Smith (2019)  <doi:10.1098/rsbl.2018.0632>."
"TreeTools",3,4,8,"2020-09-22 18:00","Efficient implementations of functions for the creation,   modification and analysis of phylogenetic trees.  Applications include:  generation of trees with specified shapes;  analysis of tree shape;  rooting of trees and extraction of subtrees;  calculation and depiction of node support;  calculation of ancestor-descendant relationships;  import and export of trees from Newick, Nexus (Maddison et al. 1997)  <doi:10.1093/sysbio/46.4.590>,  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;  and analysis of splits and cladistic information."
"TreeSearch",4,2,13,"2020-07-07 13:00","Searches for phylogenetic trees that are optimal using a   user-defined criterion.   Handles inapplicable data using the algorithm of   Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.  Implements Profile Parsimony (Faith and Trueman, 2001)   <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969)   <doi:10.2307/2412182>."
"TreeDist",5,1,4,"2020-08-28 13:30","Implements measures of tree similarity, including   information-based generalized Robinson-Foulds distances  (Phylogenetic Information Distance, Clustering Information Distance,  Matching Split Information Distance; Smith, 2020)  <doi:10.1093/bioinformatics/btaa614>;   Jaccard-Robinson-Foulds distances (Bocker et al. 2013)  <doi:10.1007/978-3-642-40453-5_13>,   including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;  the Matching Split Distance (Bogdanowicz & Giaro 2012)  <doi:10.1109/TCBB.2011.48>;  Maximum Agreement Subtree distances;  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al.   (1996) <doi:10.1007/3-540-61332-3_168>.  Calculates the median of a set of trees under any distance metric."
"timetree",0,1,1,"2015-04-28","A interface to the TimeTree of Life Webpage (www.timetree.org). TimeTree is a public database for information on the evolutionary timescale of life. This package includes functions for searching divergence time for  taxa or all nodes of a phylogeny."
"SeqFeatR",1,1,10,"2018-11-30 14:30","Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>."
"rwty",1,1,3,"2016-06-22 05:42","Implements various tests, visualizations, and metrics    for diagnosing convergence of MCMC chains in phylogenetics.  It implements    and automates many of the functions of the AWTY package in the R    environment."
"phytools",0,40,40,"2020-06-01 23:00","A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research."
"pcmabc",0,1,6,"2018-12-18 11:30","Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. Both distory and geiger are orphaned at the moment."
"paleotree",0,2,25,"2019-06-05 00:20","Provides tools for transforming, a posteriori time-scaling, and    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of    fossil taxa using stratigraphic data. This package also contains a large number    of likelihood functions for estimating sampling and diversification rates from    different types of data available from the fossil record (e.g. range data,    occurrence data, etc). paleotree users can also simulate diversification and    sampling in the fossil record using the function simFossilRecord, which is a    detailed simulator for branching birth-death-sampling processes composed of    discrete taxonomic units arranged in ancestor-descendant relationships. Users    can use simFossilRecord to simulate diversification in incompletely sampled    fossil records, under various models of morphological differentiation (i.e.    the various patterns by which morphotaxa originate from one another), and    with time-dependent, longevity-dependent and/or diversity-dependent rates of    diversification, extinction and sampling. Additional functions allow users to    translate simulated ancestor-descendant data from simFossilRecord into standard    time-scaled phylogenies or unscaled cladograms that reflect the relationships    among taxon units."
"markophylo",1,1,7,"2019-03-22 16:00","Allows for fitting of maximum likelihood models using Markov chains    on phylogenetic trees for analysis of discrete character data. Examples of such    discrete character data include restriction sites, gene family presence/absence,    intron presence/absence, and gene family size data. Hypothesis-driven user-    specified substitution rate matrices can be estimated. Allows for biologically    realistic models combining constrained substitution rate matrices, site rate    variation, site partitioning, branch-specific rates, allowing for non-stationary    prior root probabilities, correcting for sampling bias, etc. See Dang and Golding     (2016) <doi:10.1093/bioinformatics/btv541> for more details."
"ips",0,1,2,"2014-11-10 00:38","Functions that wrap popular phylogenetic software for sequence    alignment, masking of sequence alignments, and estimation of phylogenies and    ancestral character states."
"indelmiss",0,1,3,"2018-01-30 19:41","Genome-wide gene insertion and deletion rates can be modelled in a maximum     likelihood framework with the additional flexibility of modelling potential missing     data using the models included within. These models simultaneously estimate insertion     and deletion (indel) rates of gene families and proportions of ""missing"" data for     (multiple) taxa of interest. The likelihood framework is utilized for parameter     estimation. A phylogenetic tree of the taxa and gene presence/absence patterns     (with data ordered by the tips of the tree) are required. See Dang et al.    (2016) <doi:10.1534/genetics.116.191973> for more details."
"haplotypes",0,1,4,"2019-03-16 23:50","Provides S4 classes and methods for reading and manipulating aligned DNA sequences, supporting an indel coding methods (only simple indel coding method is available in the current version), showing base substitutions and indels, calculating absolute pairwise distances between DNA sequences, and collapses identical DNA sequences into haplotypes or inferring haplotypes using user provided absolute pairwise character difference matrix.  This package also includes S4 classes and methods for estimating genealogical relationships among haplotypes using statistical parsimony and plotting parsimony networks.  "
"CommT",0,1,1,"2015-06-16","Provides functions to measure the difference between constrained and unconstrained gene tree distributions using various tree distance metrics. Constraints are enforced prior to this analysis via the estimation of a tree under the community tree model."
"coalescentMCMC",2,1,5,"2013-12-04 09:41","Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters."
"BSS",0,1,1,"2020-06-24","Efficient simulation of Brownian semistationary (BSS) processes using the hybrid simulation scheme, as described in     Bennedsen, Lunde, Pakkannen (2017) <arXiv:1507.03004v4>, as well as functions to fit BSS processes    to data, and functions to estimate the stochastic volatility process of a BSS process."
"BoSSA",3,1,14,"2018-09-25 21:10","Reads and plots phylogenetic placements."
"beastier",1,2,4,"2019-12-02 13:50","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is a command-line tool.    This package provides a way to call 'BEAST2'     from an 'R' function call."
"babette",5,1,3,"2020-02-01 21:00","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is commonly accompanied by 'BEAUti 2', 'Tracer' and 'DensiTree'.    'babette' provides for an alternative workflow of using     all these tools separately. This allows doing complex Bayesian    phylogenetics easily and reproducibly from 'R'. "
"MSCquartets",0,1,1,"2019-12-16","Methods for analyzing and using quartets displayed on a collection of gene trees,       primarily to make inferences about the species tree or network under the multi-species       coalescent model. These include quartet hypothesis tests for the model, as developed by       Mitchell et al. (2019) <doi:10.1214/19-EJS1576>, the species tree inference routines       based on quartet distances of Rhodes (2019) <doi:10.1109/TCBB.2019.2917204> and       Yourdkhani and Rhodes (2019), and the NANUQ algorithm for inference of level-1       species networks of Allman et al. (2019) <arXiv:1905.07050>."
"tracerer",1,2,4,"2019-12-02 13:10","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'Tracer' (<http://tree.bio.ed.ac.uk/software/tracer/>) is a GUI tool   to parse and analyze the files generated by 'BEAST2'.  This package provides a way to parse and analyze 'BEAST2' input  files without active user input, but using  R function calls instead."
"Quartet",2,2,5,"2019-12-30 18:30","Calculates the number of four-taxon subtrees consistent with a pair  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),  Reconstructing the shape of a tree from observed dissimilarity data,  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>,   and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for  computing the quartet and triplet distances between binary or general trees,  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>  for pairs of binary trees."
"CongreveLamsdell2016",3,1,3,"2019-02-07 15:13","Includes the 100 datasets simulated by Congreve and Lamsdell (2016)  <doi:10.1111/pala.12236>, and analyses of the partition and quartet distance of  reconstructed trees from the generative tree, as analysed by Smith (2019)  <doi:10.1098/rsbl.2018.0632>."
"TreeTools",3,4,8,"2020-09-22 18:00","Efficient implementations of functions for the creation,   modification and analysis of phylogenetic trees.  Applications include:  generation of trees with specified shapes;  analysis of tree shape;  rooting of trees and extraction of subtrees;  calculation and depiction of node support;  calculation of ancestor-descendant relationships;  import and export of trees from Newick, Nexus (Maddison et al. 1997)  <doi:10.1093/sysbio/46.4.590>,  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;  and analysis of splits and cladistic information."
"TreeSearch",4,2,13,"2020-07-07 13:00","Searches for phylogenetic trees that are optimal using a   user-defined criterion.   Handles inapplicable data using the algorithm of   Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.  Implements Profile Parsimony (Faith and Trueman, 2001)   <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969)   <doi:10.2307/2412182>."
"TreeDist",5,1,4,"2020-08-28 13:30","Implements measures of tree similarity, including   information-based generalized Robinson-Foulds distances  (Phylogenetic Information Distance, Clustering Information Distance,  Matching Split Information Distance; Smith, 2020)  <doi:10.1093/bioinformatics/btaa614>;   Jaccard-Robinson-Foulds distances (Bocker et al. 2013)  <doi:10.1007/978-3-642-40453-5_13>,   including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;  the Matching Split Distance (Bogdanowicz & Giaro 2012)  <doi:10.1109/TCBB.2011.48>;  Maximum Agreement Subtree distances;  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al.   (1996) <doi:10.1007/3-540-61332-3_168>.  Calculates the median of a set of trees under any distance metric."
"timetree",0,1,1,"2015-04-28","A interface to the TimeTree of Life Webpage (www.timetree.org). TimeTree is a public database for information on the evolutionary timescale of life. This package includes functions for searching divergence time for  taxa or all nodes of a phylogeny."
"SeqFeatR",1,1,10,"2018-11-30 14:30","Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>."
"rwty",1,1,3,"2016-06-22 05:42","Implements various tests, visualizations, and metrics    for diagnosing convergence of MCMC chains in phylogenetics.  It implements    and automates many of the functions of the AWTY package in the R    environment."
"phytools",0,40,40,"2020-06-01 23:00","A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research."
"pcmabc",0,1,6,"2018-12-18 11:30","Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. Both distory and geiger are orphaned at the moment."
"paleotree",0,2,25,"2019-06-05 00:20","Provides tools for transforming, a posteriori time-scaling, and    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of    fossil taxa using stratigraphic data. This package also contains a large number    of likelihood functions for estimating sampling and diversification rates from    different types of data available from the fossil record (e.g. range data,    occurrence data, etc). paleotree users can also simulate diversification and    sampling in the fossil record using the function simFossilRecord, which is a    detailed simulator for branching birth-death-sampling processes composed of    discrete taxonomic units arranged in ancestor-descendant relationships. Users    can use simFossilRecord to simulate diversification in incompletely sampled    fossil records, under various models of morphological differentiation (i.e.    the various patterns by which morphotaxa originate from one another), and    with time-dependent, longevity-dependent and/or diversity-dependent rates of    diversification, extinction and sampling. Additional functions allow users to    translate simulated ancestor-descendant data from simFossilRecord into standard    time-scaled phylogenies or unscaled cladograms that reflect the relationships    among taxon units."
"markophylo",1,1,7,"2019-03-22 16:00","Allows for fitting of maximum likelihood models using Markov chains    on phylogenetic trees for analysis of discrete character data. Examples of such    discrete character data include restriction sites, gene family presence/absence,    intron presence/absence, and gene family size data. Hypothesis-driven user-    specified substitution rate matrices can be estimated. Allows for biologically    realistic models combining constrained substitution rate matrices, site rate    variation, site partitioning, branch-specific rates, allowing for non-stationary    prior root probabilities, correcting for sampling bias, etc. See Dang and Golding     (2016) <doi:10.1093/bioinformatics/btv541> for more details."
"ips",0,1,2,"2014-11-10 00:38","Functions that wrap popular phylogenetic software for sequence    alignment, masking of sequence alignments, and estimation of phylogenies and    ancestral character states."
"indelmiss",0,1,3,"2018-01-30 19:41","Genome-wide gene insertion and deletion rates can be modelled in a maximum     likelihood framework with the additional flexibility of modelling potential missing     data using the models included within. These models simultaneously estimate insertion     and deletion (indel) rates of gene families and proportions of ""missing"" data for     (multiple) taxa of interest. The likelihood framework is utilized for parameter     estimation. A phylogenetic tree of the taxa and gene presence/absence patterns     (with data ordered by the tips of the tree) are required. See Dang et al.    (2016) <doi:10.1534/genetics.116.191973> for more details."
"haplotypes",0,1,4,"2019-03-16 23:50","Provides S4 classes and methods for reading and manipulating aligned DNA sequences, supporting an indel coding methods (only simple indel coding method is available in the current version), showing base substitutions and indels, calculating absolute pairwise distances between DNA sequences, and collapses identical DNA sequences into haplotypes or inferring haplotypes using user provided absolute pairwise character difference matrix.  This package also includes S4 classes and methods for estimating genealogical relationships among haplotypes using statistical parsimony and plotting parsimony networks.  "
"CommT",0,1,1,"2015-06-16","Provides functions to measure the difference between constrained and unconstrained gene tree distributions using various tree distance metrics. Constraints are enforced prior to this analysis via the estimation of a tree under the community tree model."
"coalescentMCMC",2,1,5,"2013-12-04 09:41","Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters."
"BSS",0,1,1,"2020-06-24","Efficient simulation of Brownian semistationary (BSS) processes using the hybrid simulation scheme, as described in     Bennedsen, Lunde, Pakkannen (2017) <arXiv:1507.03004v4>, as well as functions to fit BSS processes    to data, and functions to estimate the stochastic volatility process of a BSS process."
"BoSSA",3,1,14,"2018-09-25 21:10","Reads and plots phylogenetic placements."
"beastier",1,2,4,"2019-12-02 13:50","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is a command-line tool.    This package provides a way to call 'BEAST2'     from an 'R' function call."
"babette",5,1,3,"2020-02-01 21:00","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is commonly accompanied by 'BEAUti 2', 'Tracer' and 'DensiTree'.    'babette' provides for an alternative workflow of using     all these tools separately. This allows doing complex Bayesian    phylogenetics easily and reproducibly from 'R'. "
"MSCquartets",0,1,1,"2019-12-16","Methods for analyzing and using quartets displayed on a collection of gene trees,       primarily to make inferences about the species tree or network under the multi-species       coalescent model. These include quartet hypothesis tests for the model, as developed by       Mitchell et al. (2019) <doi:10.1214/19-EJS1576>, the species tree inference routines       based on quartet distances of Rhodes (2019) <doi:10.1109/TCBB.2019.2917204> and       Yourdkhani and Rhodes (2019), and the NANUQ algorithm for inference of level-1       species networks of Allman et al. (2019) <arXiv:1905.07050>."
"tracerer",1,2,4,"2019-12-02 13:10","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'Tracer' (<http://tree.bio.ed.ac.uk/software/tracer/>) is a GUI tool   to parse and analyze the files generated by 'BEAST2'.  This package provides a way to parse and analyze 'BEAST2' input  files without active user input, but using  R function calls instead."
"Quartet",2,2,5,"2019-12-30 18:30","Calculates the number of four-taxon subtrees consistent with a pair  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),  Reconstructing the shape of a tree from observed dissimilarity data,  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>,   and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for  computing the quartet and triplet distances between binary or general trees,  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>  for pairs of binary trees."
"CongreveLamsdell2016",3,1,3,"2019-02-07 15:13","Includes the 100 datasets simulated by Congreve and Lamsdell (2016)  <doi:10.1111/pala.12236>, and analyses of the partition and quartet distance of  reconstructed trees from the generative tree, as analysed by Smith (2019)  <doi:10.1098/rsbl.2018.0632>."
"TreeTools",3,4,8,"2020-09-22 18:00","Efficient implementations of functions for the creation,   modification and analysis of phylogenetic trees.  Applications include:  generation of trees with specified shapes;  analysis of tree shape;  rooting of trees and extraction of subtrees;  calculation and depiction of node support;  calculation of ancestor-descendant relationships;  import and export of trees from Newick, Nexus (Maddison et al. 1997)  <doi:10.1093/sysbio/46.4.590>,  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;  and analysis of splits and cladistic information."
"TreeSearch",4,2,13,"2020-07-07 13:00","Searches for phylogenetic trees that are optimal using a   user-defined criterion.   Handles inapplicable data using the algorithm of   Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.  Implements Profile Parsimony (Faith and Trueman, 2001)   <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969)   <doi:10.2307/2412182>."
"TreeDist",5,1,4,"2020-08-28 13:30","Implements measures of tree similarity, including   information-based generalized Robinson-Foulds distances  (Phylogenetic Information Distance, Clustering Information Distance,  Matching Split Information Distance; Smith, 2020)  <doi:10.1093/bioinformatics/btaa614>;   Jaccard-Robinson-Foulds distances (Bocker et al. 2013)  <doi:10.1007/978-3-642-40453-5_13>,   including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;  the Matching Split Distance (Bogdanowicz & Giaro 2012)  <doi:10.1109/TCBB.2011.48>;  Maximum Agreement Subtree distances;  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al.   (1996) <doi:10.1007/3-540-61332-3_168>.  Calculates the median of a set of trees under any distance metric."
"timetree",0,1,1,"2015-04-28","A interface to the TimeTree of Life Webpage (www.timetree.org). TimeTree is a public database for information on the evolutionary timescale of life. This package includes functions for searching divergence time for  taxa or all nodes of a phylogeny."
"SeqFeatR",1,1,10,"2018-11-30 14:30","Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>."
"rwty",1,1,3,"2016-06-22 05:42","Implements various tests, visualizations, and metrics    for diagnosing convergence of MCMC chains in phylogenetics.  It implements    and automates many of the functions of the AWTY package in the R    environment."
"phytools",0,40,40,"2020-06-01 23:00","A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research."
"pcmabc",0,1,6,"2018-12-18 11:30","Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. Both distory and geiger are orphaned at the moment."
"paleotree",0,2,25,"2019-06-05 00:20","Provides tools for transforming, a posteriori time-scaling, and    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of    fossil taxa using stratigraphic data. This package also contains a large number    of likelihood functions for estimating sampling and diversification rates from    different types of data available from the fossil record (e.g. range data,    occurrence data, etc). paleotree users can also simulate diversification and    sampling in the fossil record using the function simFossilRecord, which is a    detailed simulator for branching birth-death-sampling processes composed of    discrete taxonomic units arranged in ancestor-descendant relationships. Users    can use simFossilRecord to simulate diversification in incompletely sampled    fossil records, under various models of morphological differentiation (i.e.    the various patterns by which morphotaxa originate from one another), and    with time-dependent, longevity-dependent and/or diversity-dependent rates of    diversification, extinction and sampling. Additional functions allow users to    translate simulated ancestor-descendant data from simFossilRecord into standard    time-scaled phylogenies or unscaled cladograms that reflect the relationships    among taxon units."
"markophylo",1,1,7,"2019-03-22 16:00","Allows for fitting of maximum likelihood models using Markov chains    on phylogenetic trees for analysis of discrete character data. Examples of such    discrete character data include restriction sites, gene family presence/absence,    intron presence/absence, and gene family size data. Hypothesis-driven user-    specified substitution rate matrices can be estimated. Allows for biologically    realistic models combining constrained substitution rate matrices, site rate    variation, site partitioning, branch-specific rates, allowing for non-stationary    prior root probabilities, correcting for sampling bias, etc. See Dang and Golding     (2016) <doi:10.1093/bioinformatics/btv541> for more details."
"ips",0,1,2,"2014-11-10 00:38","Functions that wrap popular phylogenetic software for sequence    alignment, masking of sequence alignments, and estimation of phylogenies and    ancestral character states."
"indelmiss",0,1,3,"2018-01-30 19:41","Genome-wide gene insertion and deletion rates can be modelled in a maximum     likelihood framework with the additional flexibility of modelling potential missing     data using the models included within. These models simultaneously estimate insertion     and deletion (indel) rates of gene families and proportions of ""missing"" data for     (multiple) taxa of interest. The likelihood framework is utilized for parameter     estimation. A phylogenetic tree of the taxa and gene presence/absence patterns     (with data ordered by the tips of the tree) are required. See Dang et al.    (2016) <doi:10.1534/genetics.116.191973> for more details."
"haplotypes",0,1,4,"2019-03-16 23:50","Provides S4 classes and methods for reading and manipulating aligned DNA sequences, supporting an indel coding methods (only simple indel coding method is available in the current version), showing base substitutions and indels, calculating absolute pairwise distances between DNA sequences, and collapses identical DNA sequences into haplotypes or inferring haplotypes using user provided absolute pairwise character difference matrix.  This package also includes S4 classes and methods for estimating genealogical relationships among haplotypes using statistical parsimony and plotting parsimony networks.  "
"CommT",0,1,1,"2015-06-16","Provides functions to measure the difference between constrained and unconstrained gene tree distributions using various tree distance metrics. Constraints are enforced prior to this analysis via the estimation of a tree under the community tree model."
"coalescentMCMC",2,1,5,"2013-12-04 09:41","Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters."
"BSS",0,1,1,"2020-06-24","Efficient simulation of Brownian semistationary (BSS) processes using the hybrid simulation scheme, as described in     Bennedsen, Lunde, Pakkannen (2017) <arXiv:1507.03004v4>, as well as functions to fit BSS processes    to data, and functions to estimate the stochastic volatility process of a BSS process."
"BoSSA",3,1,14,"2018-09-25 21:10","Reads and plots phylogenetic placements."
"beastier",1,2,4,"2019-12-02 13:50","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is a command-line tool.    This package provides a way to call 'BEAST2'     from an 'R' function call."
"babette",5,1,3,"2020-02-01 21:00","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is commonly accompanied by 'BEAUti 2', 'Tracer' and 'DensiTree'.    'babette' provides for an alternative workflow of using     all these tools separately. This allows doing complex Bayesian    phylogenetics easily and reproducibly from 'R'. "
"MSCquartets",0,1,1,"2019-12-16","Methods for analyzing and using quartets displayed on a collection of gene trees,       primarily to make inferences about the species tree or network under the multi-species       coalescent model. These include quartet hypothesis tests for the model, as developed by       Mitchell et al. (2019) <doi:10.1214/19-EJS1576>, the species tree inference routines       based on quartet distances of Rhodes (2019) <doi:10.1109/TCBB.2019.2917204> and       Yourdkhani and Rhodes (2019), and the NANUQ algorithm for inference of level-1       species networks of Allman et al. (2019) <arXiv:1905.07050>."
"tracerer",1,2,4,"2019-12-02 13:10","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'Tracer' (<http://tree.bio.ed.ac.uk/software/tracer/>) is a GUI tool   to parse and analyze the files generated by 'BEAST2'.  This package provides a way to parse and analyze 'BEAST2' input  files without active user input, but using  R function calls instead."
"Quartet",2,2,5,"2019-12-30 18:30","Calculates the number of four-taxon subtrees consistent with a pair  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),  Reconstructing the shape of a tree from observed dissimilarity data,  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>,   and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for  computing the quartet and triplet distances between binary or general trees,  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>  for pairs of binary trees."
"CongreveLamsdell2016",3,1,3,"2019-02-07 15:13","Includes the 100 datasets simulated by Congreve and Lamsdell (2016)  <doi:10.1111/pala.12236>, and analyses of the partition and quartet distance of  reconstructed trees from the generative tree, as analysed by Smith (2019)  <doi:10.1098/rsbl.2018.0632>."
"TreeTools",3,4,8,"2020-09-22 18:00","Efficient implementations of functions for the creation,   modification and analysis of phylogenetic trees.  Applications include:  generation of trees with specified shapes;  analysis of tree shape;  rooting of trees and extraction of subtrees;  calculation and depiction of node support;  calculation of ancestor-descendant relationships;  import and export of trees from Newick, Nexus (Maddison et al. 1997)  <doi:10.1093/sysbio/46.4.590>,  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;  and analysis of splits and cladistic information."
"TreeSearch",4,2,13,"2020-07-07 13:00","Searches for phylogenetic trees that are optimal using a   user-defined criterion.   Handles inapplicable data using the algorithm of   Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.  Implements Profile Parsimony (Faith and Trueman, 2001)   <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969)   <doi:10.2307/2412182>."
"TreeDist",5,1,4,"2020-08-28 13:30","Implements measures of tree similarity, including   information-based generalized Robinson-Foulds distances  (Phylogenetic Information Distance, Clustering Information Distance,  Matching Split Information Distance; Smith, 2020)  <doi:10.1093/bioinformatics/btaa614>;   Jaccard-Robinson-Foulds distances (Bocker et al. 2013)  <doi:10.1007/978-3-642-40453-5_13>,   including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;  the Matching Split Distance (Bogdanowicz & Giaro 2012)  <doi:10.1109/TCBB.2011.48>;  Maximum Agreement Subtree distances;  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al.   (1996) <doi:10.1007/3-540-61332-3_168>.  Calculates the median of a set of trees under any distance metric."
"timetree",0,1,1,"2015-04-28","A interface to the TimeTree of Life Webpage (www.timetree.org). TimeTree is a public database for information on the evolutionary timescale of life. This package includes functions for searching divergence time for  taxa or all nodes of a phylogeny."
"SeqFeatR",1,1,10,"2018-11-30 14:30","Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>."
"rwty",1,1,3,"2016-06-22 05:42","Implements various tests, visualizations, and metrics    for diagnosing convergence of MCMC chains in phylogenetics.  It implements    and automates many of the functions of the AWTY package in the R    environment."
"phytools",0,40,40,"2020-06-01 23:00","A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research."
"pcmabc",0,1,6,"2018-12-18 11:30","Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. Both distory and geiger are orphaned at the moment."
"paleotree",0,2,25,"2019-06-05 00:20","Provides tools for transforming, a posteriori time-scaling, and    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of    fossil taxa using stratigraphic data. This package also contains a large number    of likelihood functions for estimating sampling and diversification rates from    different types of data available from the fossil record (e.g. range data,    occurrence data, etc). paleotree users can also simulate diversification and    sampling in the fossil record using the function simFossilRecord, which is a    detailed simulator for branching birth-death-sampling processes composed of    discrete taxonomic units arranged in ancestor-descendant relationships. Users    can use simFossilRecord to simulate diversification in incompletely sampled    fossil records, under various models of morphological differentiation (i.e.    the various patterns by which morphotaxa originate from one another), and    with time-dependent, longevity-dependent and/or diversity-dependent rates of    diversification, extinction and sampling. Additional functions allow users to    translate simulated ancestor-descendant data from simFossilRecord into standard    time-scaled phylogenies or unscaled cladograms that reflect the relationships    among taxon units."
"markophylo",1,1,7,"2019-03-22 16:00","Allows for fitting of maximum likelihood models using Markov chains    on phylogenetic trees for analysis of discrete character data. Examples of such    discrete character data include restriction sites, gene family presence/absence,    intron presence/absence, and gene family size data. Hypothesis-driven user-    specified substitution rate matrices can be estimated. Allows for biologically    realistic models combining constrained substitution rate matrices, site rate    variation, site partitioning, branch-specific rates, allowing for non-stationary    prior root probabilities, correcting for sampling bias, etc. See Dang and Golding     (2016) <doi:10.1093/bioinformatics/btv541> for more details."
"ips",0,1,2,"2014-11-10 00:38","Functions that wrap popular phylogenetic software for sequence    alignment, masking of sequence alignments, and estimation of phylogenies and    ancestral character states."
"indelmiss",0,1,3,"2018-01-30 19:41","Genome-wide gene insertion and deletion rates can be modelled in a maximum     likelihood framework with the additional flexibility of modelling potential missing     data using the models included within. These models simultaneously estimate insertion     and deletion (indel) rates of gene families and proportions of ""missing"" data for     (multiple) taxa of interest. The likelihood framework is utilized for parameter     estimation. A phylogenetic tree of the taxa and gene presence/absence patterns     (with data ordered by the tips of the tree) are required. See Dang et al.    (2016) <doi:10.1534/genetics.116.191973> for more details."
"haplotypes",0,1,4,"2019-03-16 23:50","Provides S4 classes and methods for reading and manipulating aligned DNA sequences, supporting an indel coding methods (only simple indel coding method is available in the current version), showing base substitutions and indels, calculating absolute pairwise distances between DNA sequences, and collapses identical DNA sequences into haplotypes or inferring haplotypes using user provided absolute pairwise character difference matrix.  This package also includes S4 classes and methods for estimating genealogical relationships among haplotypes using statistical parsimony and plotting parsimony networks.  "
"CommT",0,1,1,"2015-06-16","Provides functions to measure the difference between constrained and unconstrained gene tree distributions using various tree distance metrics. Constraints are enforced prior to this analysis via the estimation of a tree under the community tree model."
"coalescentMCMC",2,1,5,"2013-12-04 09:41","Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters."
"BSS",0,1,1,"2020-06-24","Efficient simulation of Brownian semistationary (BSS) processes using the hybrid simulation scheme, as described in     Bennedsen, Lunde, Pakkannen (2017) <arXiv:1507.03004v4>, as well as functions to fit BSS processes    to data, and functions to estimate the stochastic volatility process of a BSS process."
"BoSSA",3,1,14,"2018-09-25 21:10","Reads and plots phylogenetic placements."
"beastier",1,2,4,"2019-12-02 13:50","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is a command-line tool.    This package provides a way to call 'BEAST2'     from an 'R' function call."
"babette",5,1,3,"2020-02-01 21:00","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is commonly accompanied by 'BEAUti 2', 'Tracer' and 'DensiTree'.    'babette' provides for an alternative workflow of using     all these tools separately. This allows doing complex Bayesian    phylogenetics easily and reproducibly from 'R'. "
"MSCquartets",0,1,1,"2019-12-16","Methods for analyzing and using quartets displayed on a collection of gene trees,       primarily to make inferences about the species tree or network under the multi-species       coalescent model. These include quartet hypothesis tests for the model, as developed by       Mitchell et al. (2019) <doi:10.1214/19-EJS1576>, the species tree inference routines       based on quartet distances of Rhodes (2019) <doi:10.1109/TCBB.2019.2917204> and       Yourdkhani and Rhodes (2019), and the NANUQ algorithm for inference of level-1       species networks of Allman et al. (2019) <arXiv:1905.07050>."
"tracerer",1,2,4,"2019-12-02 13:10","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'Tracer' (<http://tree.bio.ed.ac.uk/software/tracer/>) is a GUI tool   to parse and analyze the files generated by 'BEAST2'.  This package provides a way to parse and analyze 'BEAST2' input  files without active user input, but using  R function calls instead."
"Quartet",2,2,5,"2019-12-30 18:30","Calculates the number of four-taxon subtrees consistent with a pair  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),  Reconstructing the shape of a tree from observed dissimilarity data,  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>,   and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for  computing the quartet and triplet distances between binary or general trees,  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>  for pairs of binary trees."
"CongreveLamsdell2016",3,1,3,"2019-02-07 15:13","Includes the 100 datasets simulated by Congreve and Lamsdell (2016)  <doi:10.1111/pala.12236>, and analyses of the partition and quartet distance of  reconstructed trees from the generative tree, as analysed by Smith (2019)  <doi:10.1098/rsbl.2018.0632>."
"TreeTools",3,4,8,"2020-09-22 18:00","Efficient implementations of functions for the creation,   modification and analysis of phylogenetic trees.  Applications include:  generation of trees with specified shapes;  analysis of tree shape;  rooting of trees and extraction of subtrees;  calculation and depiction of node support;  calculation of ancestor-descendant relationships;  import and export of trees from Newick, Nexus (Maddison et al. 1997)  <doi:10.1093/sysbio/46.4.590>,  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;  and analysis of splits and cladistic information."
"TreeSearch",4,2,13,"2020-07-07 13:00","Searches for phylogenetic trees that are optimal using a   user-defined criterion.   Handles inapplicable data using the algorithm of   Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.  Implements Profile Parsimony (Faith and Trueman, 2001)   <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969)   <doi:10.2307/2412182>."
"TreeDist",5,1,4,"2020-08-28 13:30","Implements measures of tree similarity, including   information-based generalized Robinson-Foulds distances  (Phylogenetic Information Distance, Clustering Information Distance,  Matching Split Information Distance; Smith, 2020)  <doi:10.1093/bioinformatics/btaa614>;   Jaccard-Robinson-Foulds distances (Bocker et al. 2013)  <doi:10.1007/978-3-642-40453-5_13>,   including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;  the Matching Split Distance (Bogdanowicz & Giaro 2012)  <doi:10.1109/TCBB.2011.48>;  Maximum Agreement Subtree distances;  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al.   (1996) <doi:10.1007/3-540-61332-3_168>.  Calculates the median of a set of trees under any distance metric."
"timetree",0,1,1,"2015-04-28","A interface to the TimeTree of Life Webpage (www.timetree.org). TimeTree is a public database for information on the evolutionary timescale of life. This package includes functions for searching divergence time for  taxa or all nodes of a phylogeny."
"SeqFeatR",1,1,10,"2018-11-30 14:30","Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>."
"rwty",1,1,3,"2016-06-22 05:42","Implements various tests, visualizations, and metrics    for diagnosing convergence of MCMC chains in phylogenetics.  It implements    and automates many of the functions of the AWTY package in the R    environment."
"phytools",0,40,40,"2020-06-01 23:00","A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research."
"pcmabc",0,1,6,"2018-12-18 11:30","Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. Both distory and geiger are orphaned at the moment."
"paleotree",0,2,25,"2019-06-05 00:20","Provides tools for transforming, a posteriori time-scaling, and    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of    fossil taxa using stratigraphic data. This package also contains a large number    of likelihood functions for estimating sampling and diversification rates from    different types of data available from the fossil record (e.g. range data,    occurrence data, etc). paleotree users can also simulate diversification and    sampling in the fossil record using the function simFossilRecord, which is a    detailed simulator for branching birth-death-sampling processes composed of    discrete taxonomic units arranged in ancestor-descendant relationships. Users    can use simFossilRecord to simulate diversification in incompletely sampled    fossil records, under various models of morphological differentiation (i.e.    the various patterns by which morphotaxa originate from one another), and    with time-dependent, longevity-dependent and/or diversity-dependent rates of    diversification, extinction and sampling. Additional functions allow users to    translate simulated ancestor-descendant data from simFossilRecord into standard    time-scaled phylogenies or unscaled cladograms that reflect the relationships    among taxon units."
"markophylo",1,1,7,"2019-03-22 16:00","Allows for fitting of maximum likelihood models using Markov chains    on phylogenetic trees for analysis of discrete character data. Examples of such    discrete character data include restriction sites, gene family presence/absence,    intron presence/absence, and gene family size data. Hypothesis-driven user-    specified substitution rate matrices can be estimated. Allows for biologically    realistic models combining constrained substitution rate matrices, site rate    variation, site partitioning, branch-specific rates, allowing for non-stationary    prior root probabilities, correcting for sampling bias, etc. See Dang and Golding     (2016) <doi:10.1093/bioinformatics/btv541> for more details."
"ips",0,1,2,"2014-11-10 00:38","Functions that wrap popular phylogenetic software for sequence    alignment, masking of sequence alignments, and estimation of phylogenies and    ancestral character states."
"indelmiss",0,1,3,"2018-01-30 19:41","Genome-wide gene insertion and deletion rates can be modelled in a maximum     likelihood framework with the additional flexibility of modelling potential missing     data using the models included within. These models simultaneously estimate insertion     and deletion (indel) rates of gene families and proportions of ""missing"" data for     (multiple) taxa of interest. The likelihood framework is utilized for parameter     estimation. A phylogenetic tree of the taxa and gene presence/absence patterns     (with data ordered by the tips of the tree) are required. See Dang et al.    (2016) <doi:10.1534/genetics.116.191973> for more details."
"haplotypes",0,1,4,"2019-03-16 23:50","Provides S4 classes and methods for reading and manipulating aligned DNA sequences, supporting an indel coding methods (only simple indel coding method is available in the current version), showing base substitutions and indels, calculating absolute pairwise distances between DNA sequences, and collapses identical DNA sequences into haplotypes or inferring haplotypes using user provided absolute pairwise character difference matrix.  This package also includes S4 classes and methods for estimating genealogical relationships among haplotypes using statistical parsimony and plotting parsimony networks.  "
"CommT",0,1,1,"2015-06-16","Provides functions to measure the difference between constrained and unconstrained gene tree distributions using various tree distance metrics. Constraints are enforced prior to this analysis via the estimation of a tree under the community tree model."
"coalescentMCMC",2,1,5,"2013-12-04 09:41","Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters."
"BSS",0,1,1,"2020-06-24","Efficient simulation of Brownian semistationary (BSS) processes using the hybrid simulation scheme, as described in     Bennedsen, Lunde, Pakkannen (2017) <arXiv:1507.03004v4>, as well as functions to fit BSS processes    to data, and functions to estimate the stochastic volatility process of a BSS process."
"BoSSA",3,1,14,"2018-09-25 21:10","Reads and plots phylogenetic placements."
"beastier",1,2,4,"2019-12-02 13:50","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is a command-line tool.    This package provides a way to call 'BEAST2'     from an 'R' function call."
"babette",5,1,3,"2020-02-01 21:00","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is commonly accompanied by 'BEAUti 2', 'Tracer' and 'DensiTree'.    'babette' provides for an alternative workflow of using     all these tools separately. This allows doing complex Bayesian    phylogenetics easily and reproducibly from 'R'. "
"MSCquartets",0,1,1,"2019-12-16","Methods for analyzing and using quartets displayed on a collection of gene trees,       primarily to make inferences about the species tree or network under the multi-species       coalescent model. These include quartet hypothesis tests for the model, as developed by       Mitchell et al. (2019) <doi:10.1214/19-EJS1576>, the species tree inference routines       based on quartet distances of Rhodes (2019) <doi:10.1109/TCBB.2019.2917204> and       Yourdkhani and Rhodes (2019), and the NANUQ algorithm for inference of level-1       species networks of Allman et al. (2019) <arXiv:1905.07050>."
"tracerer",1,2,4,"2019-12-02 13:10","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'Tracer' (<http://tree.bio.ed.ac.uk/software/tracer/>) is a GUI tool   to parse and analyze the files generated by 'BEAST2'.  This package provides a way to parse and analyze 'BEAST2' input  files without active user input, but using  R function calls instead."
"Quartet",2,2,5,"2019-12-30 18:30","Calculates the number of four-taxon subtrees consistent with a pair  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),  Reconstructing the shape of a tree from observed dissimilarity data,  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>,   and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for  computing the quartet and triplet distances between binary or general trees,  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>  for pairs of binary trees."
"CongreveLamsdell2016",3,1,3,"2019-02-07 15:13","Includes the 100 datasets simulated by Congreve and Lamsdell (2016)  <doi:10.1111/pala.12236>, and analyses of the partition and quartet distance of  reconstructed trees from the generative tree, as analysed by Smith (2019)  <doi:10.1098/rsbl.2018.0632>."
"TreeTools",3,4,8,"2020-09-22 18:00","Efficient implementations of functions for the creation,   modification and analysis of phylogenetic trees.  Applications include:  generation of trees with specified shapes;  analysis of tree shape;  rooting of trees and extraction of subtrees;  calculation and depiction of node support;  calculation of ancestor-descendant relationships;  import and export of trees from Newick, Nexus (Maddison et al. 1997)  <doi:10.1093/sysbio/46.4.590>,  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;  and analysis of splits and cladistic information."
"TreeSearch",4,2,13,"2020-07-07 13:00","Searches for phylogenetic trees that are optimal using a   user-defined criterion.   Handles inapplicable data using the algorithm of   Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.  Implements Profile Parsimony (Faith and Trueman, 2001)   <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969)   <doi:10.2307/2412182>."
"TreeDist",5,1,4,"2020-08-28 13:30","Implements measures of tree similarity, including   information-based generalized Robinson-Foulds distances  (Phylogenetic Information Distance, Clustering Information Distance,  Matching Split Information Distance; Smith, 2020)  <doi:10.1093/bioinformatics/btaa614>;   Jaccard-Robinson-Foulds distances (Bocker et al. 2013)  <doi:10.1007/978-3-642-40453-5_13>,   including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;  the Matching Split Distance (Bogdanowicz & Giaro 2012)  <doi:10.1109/TCBB.2011.48>;  Maximum Agreement Subtree distances;  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al.   (1996) <doi:10.1007/3-540-61332-3_168>.  Calculates the median of a set of trees under any distance metric."
"timetree",0,1,1,"2015-04-28","A interface to the TimeTree of Life Webpage (www.timetree.org). TimeTree is a public database for information on the evolutionary timescale of life. This package includes functions for searching divergence time for  taxa or all nodes of a phylogeny."
"SeqFeatR",1,1,10,"2018-11-30 14:30","Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>."
"rwty",1,1,3,"2016-06-22 05:42","Implements various tests, visualizations, and metrics    for diagnosing convergence of MCMC chains in phylogenetics.  It implements    and automates many of the functions of the AWTY package in the R    environment."
"phytools",0,40,40,"2020-06-01 23:00","A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research."
"pcmabc",0,1,6,"2018-12-18 11:30","Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. Both distory and geiger are orphaned at the moment."
"paleotree",0,2,25,"2019-06-05 00:20","Provides tools for transforming, a posteriori time-scaling, and    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of    fossil taxa using stratigraphic data. This package also contains a large number    of likelihood functions for estimating sampling and diversification rates from    different types of data available from the fossil record (e.g. range data,    occurrence data, etc). paleotree users can also simulate diversification and    sampling in the fossil record using the function simFossilRecord, which is a    detailed simulator for branching birth-death-sampling processes composed of    discrete taxonomic units arranged in ancestor-descendant relationships. Users    can use simFossilRecord to simulate diversification in incompletely sampled    fossil records, under various models of morphological differentiation (i.e.    the various patterns by which morphotaxa originate from one another), and    with time-dependent, longevity-dependent and/or diversity-dependent rates of    diversification, extinction and sampling. Additional functions allow users to    translate simulated ancestor-descendant data from simFossilRecord into standard    time-scaled phylogenies or unscaled cladograms that reflect the relationships    among taxon units."
"markophylo",1,1,7,"2019-03-22 16:00","Allows for fitting of maximum likelihood models using Markov chains    on phylogenetic trees for analysis of discrete character data. Examples of such    discrete character data include restriction sites, gene family presence/absence,    intron presence/absence, and gene family size data. Hypothesis-driven user-    specified substitution rate matrices can be estimated. Allows for biologically    realistic models combining constrained substitution rate matrices, site rate    variation, site partitioning, branch-specific rates, allowing for non-stationary    prior root probabilities, correcting for sampling bias, etc. See Dang and Golding     (2016) <doi:10.1093/bioinformatics/btv541> for more details."
"ips",0,1,2,"2014-11-10 00:38","Functions that wrap popular phylogenetic software for sequence    alignment, masking of sequence alignments, and estimation of phylogenies and    ancestral character states."
"indelmiss",0,1,3,"2018-01-30 19:41","Genome-wide gene insertion and deletion rates can be modelled in a maximum     likelihood framework with the additional flexibility of modelling potential missing     data using the models included within. These models simultaneously estimate insertion     and deletion (indel) rates of gene families and proportions of ""missing"" data for     (multiple) taxa of interest. The likelihood framework is utilized for parameter     estimation. A phylogenetic tree of the taxa and gene presence/absence patterns     (with data ordered by the tips of the tree) are required. See Dang et al.    (2016) <doi:10.1534/genetics.116.191973> for more details."
"haplotypes",0,1,4,"2019-03-16 23:50","Provides S4 classes and methods for reading and manipulating aligned DNA sequences, supporting an indel coding methods (only simple indel coding method is available in the current version), showing base substitutions and indels, calculating absolute pairwise distances between DNA sequences, and collapses identical DNA sequences into haplotypes or inferring haplotypes using user provided absolute pairwise character difference matrix.  This package also includes S4 classes and methods for estimating genealogical relationships among haplotypes using statistical parsimony and plotting parsimony networks.  "
"CommT",0,1,1,"2015-06-16","Provides functions to measure the difference between constrained and unconstrained gene tree distributions using various tree distance metrics. Constraints are enforced prior to this analysis via the estimation of a tree under the community tree model."
"coalescentMCMC",2,1,5,"2013-12-04 09:41","Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters."
"BSS",0,1,1,"2020-06-24","Efficient simulation of Brownian semistationary (BSS) processes using the hybrid simulation scheme, as described in     Bennedsen, Lunde, Pakkannen (2017) <arXiv:1507.03004v4>, as well as functions to fit BSS processes    to data, and functions to estimate the stochastic volatility process of a BSS process."
"BoSSA",3,1,14,"2018-09-25 21:10","Reads and plots phylogenetic placements."
"beastier",1,2,4,"2019-12-02 13:50","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is a command-line tool.    This package provides a way to call 'BEAST2'     from an 'R' function call."
"babette",5,1,3,"2020-02-01 21:00","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is commonly accompanied by 'BEAUti 2', 'Tracer' and 'DensiTree'.    'babette' provides for an alternative workflow of using     all these tools separately. This allows doing complex Bayesian    phylogenetics easily and reproducibly from 'R'. "
"MSCquartets",0,1,1,"2019-12-16","Methods for analyzing and using quartets displayed on a collection of gene trees,       primarily to make inferences about the species tree or network under the multi-species       coalescent model. These include quartet hypothesis tests for the model, as developed by       Mitchell et al. (2019) <doi:10.1214/19-EJS1576>, the species tree inference routines       based on quartet distances of Rhodes (2019) <doi:10.1109/TCBB.2019.2917204> and       Yourdkhani and Rhodes (2019), and the NANUQ algorithm for inference of level-1       species networks of Allman et al. (2019) <arXiv:1905.07050>."
"tracerer",1,2,4,"2019-12-02 13:10","'BEAST2' (<https://www.beast2.org>) is a widely used  Bayesian phylogenetic tool, that uses DNA/RNA/protein data  and many model priors to create a posterior of jointly estimated   phylogenies and parameters.  'Tracer' (<http://tree.bio.ed.ac.uk/software/tracer/>) is a GUI tool   to parse and analyze the files generated by 'BEAST2'.  This package provides a way to parse and analyze 'BEAST2' input  files without active user input, but using  R function calls instead."
"Quartet",2,2,5,"2019-12-30 18:30","Calculates the number of four-taxon subtrees consistent with a pair  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),  Reconstructing the shape of a tree from observed dissimilarity data,  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>,   and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for  computing the quartet and triplet distances between binary or general trees,  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>  for pairs of binary trees."
"CongreveLamsdell2016",3,1,3,"2019-02-07 15:13","Includes the 100 datasets simulated by Congreve and Lamsdell (2016)  <doi:10.1111/pala.12236>, and analyses of the partition and quartet distance of  reconstructed trees from the generative tree, as analysed by Smith (2019)  <doi:10.1098/rsbl.2018.0632>."
"TreeTools",3,4,8,"2020-09-22 18:00","Efficient implementations of functions for the creation,   modification and analysis of phylogenetic trees.  Applications include:  generation of trees with specified shapes;  analysis of tree shape;  rooting of trees and extraction of subtrees;  calculation and depiction of node support;  calculation of ancestor-descendant relationships;  import and export of trees from Newick, Nexus (Maddison et al. 1997)  <doi:10.1093/sysbio/46.4.590>,  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;  and analysis of splits and cladistic information."
"TreeSearch",4,2,13,"2020-07-07 13:00","Searches for phylogenetic trees that are optimal using a   user-defined criterion.   Handles inapplicable data using the algorithm of   Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.  Implements Profile Parsimony (Faith and Trueman, 2001)   <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969)   <doi:10.2307/2412182>."
"TreeDist",5,1,4,"2020-08-28 13:30","Implements measures of tree similarity, including   information-based generalized Robinson-Foulds distances  (Phylogenetic Information Distance, Clustering Information Distance,  Matching Split Information Distance; Smith, 2020)  <doi:10.1093/bioinformatics/btaa614>;   Jaccard-Robinson-Foulds distances (Bocker et al. 2013)  <doi:10.1007/978-3-642-40453-5_13>,   including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;  the Matching Split Distance (Bogdanowicz & Giaro 2012)  <doi:10.1109/TCBB.2011.48>;  Maximum Agreement Subtree distances;  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al.   (1996) <doi:10.1007/3-540-61332-3_168>.  Calculates the median of a set of trees under any distance metric."
"timetree",0,1,1,"2015-04-28","A interface to the TimeTree of Life Webpage (www.timetree.org). TimeTree is a public database for information on the evolutionary timescale of life. This package includes functions for searching divergence time for  taxa or all nodes of a phylogeny."
"SeqFeatR",1,1,10,"2018-11-30 14:30","Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>."
"rwty",1,1,3,"2016-06-22 05:42","Implements various tests, visualizations, and metrics    for diagnosing convergence of MCMC chains in phylogenetics.  It implements    and automates many of the functions of the AWTY package in the R    environment."
"phytools",0,40,40,"2020-06-01 23:00","A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research."
"pcmabc",0,1,6,"2018-12-18 11:30","Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. Both distory and geiger are orphaned at the moment."
"paleotree",0,2,25,"2019-06-05 00:20","Provides tools for transforming, a posteriori time-scaling, and    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of    fossil taxa using stratigraphic data. This package also contains a large number    of likelihood functions for estimating sampling and diversification rates from    different types of data available from the fossil record (e.g. range data,    occurrence data, etc). paleotree users can also simulate diversification and    sampling in the fossil record using the function simFossilRecord, which is a    detailed simulator for branching birth-death-sampling processes composed of    discrete taxonomic units arranged in ancestor-descendant relationships. Users    can use simFossilRecord to simulate diversification in incompletely sampled    fossil records, under various models of morphological differentiation (i.e.    the various patterns by which morphotaxa originate from one another), and    with time-dependent, longevity-dependent and/or diversity-dependent rates of    diversification, extinction and sampling. Additional functions allow users to    translate simulated ancestor-descendant data from simFossilRecord into standard    time-scaled phylogenies or unscaled cladograms that reflect the relationships    among taxon units."
"markophylo",1,1,7,"2019-03-22 16:00","Allows for fitting of maximum likelihood models using Markov chains    on phylogenetic trees for analysis of discrete character data. Examples of such    discrete character data include restriction sites, gene family presence/absence,    intron presence/absence, and gene family size data. Hypothesis-driven user-    specified substitution rate matrices can be estimated. Allows for biologically    realistic models combining constrained substitution rate matrices, site rate    variation, site partitioning, branch-specific rates, allowing for non-stationary    prior root probabilities, correcting for sampling bias, etc. See Dang and Golding     (2016) <doi:10.1093/bioinformatics/btv541> for more details."
"ips",0,1,2,"2014-11-10 00:38","Functions that wrap popular phylogenetic software for sequence    alignment, masking of sequence alignments, and estimation of phylogenies and    ancestral character states."
"indelmiss",0,1,3,"2018-01-30 19:41","Genome-wide gene insertion and deletion rates can be modelled in a maximum     likelihood framework with the additional flexibility of modelling potential missing     data using the models included within. These models simultaneously estimate insertion     and deletion (indel) rates of gene families and proportions of ""missing"" data for     (multiple) taxa of interest. The likelihood framework is utilized for parameter     estimation. A phylogenetic tree of the taxa and gene presence/absence patterns     (with data ordered by the tips of the tree) are required. See Dang et al.    (2016) <doi:10.1534/genetics.116.191973> for more details."
"haplotypes",0,1,4,"2019-03-16 23:50","Provides S4 classes and methods for reading and manipulating aligned DNA sequences, supporting an indel coding methods (only simple indel coding method is available in the current version), showing base substitutions and indels, calculating absolute pairwise distances between DNA sequences, and collapses identical DNA sequences into haplotypes or inferring haplotypes using user provided absolute pairwise character difference matrix.  This package also includes S4 classes and methods for estimating genealogical relationships among haplotypes using statistical parsimony and plotting parsimony networks.  "
"CommT",0,1,1,"2015-06-16","Provides functions to measure the difference between constrained and unconstrained gene tree distributions using various tree distance metrics. Constraints are enforced prior to this analysis via the estimation of a tree under the community tree model."
"coalescentMCMC",2,1,5,"2013-12-04 09:41","Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters."
"BSS",0,1,1,"2020-06-24","Efficient simulation of Brownian semistationary (BSS) processes using the hybrid simulation scheme, as described in     Bennedsen, Lunde, Pakkannen (2017) <arXiv:1507.03004v4>, as well as functions to fit BSS processes    to data, and functions to estimate the stochastic volatility process of a BSS process."
"BoSSA",3,1,14,"2018-09-25 21:10","Reads and plots phylogenetic placements."
"beastier",1,2,4,"2019-12-02 13:50","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is a command-line tool.    This package provides a way to call 'BEAST2'     from an 'R' function call."
"babette",5,1,3,"2020-02-01 21:00","'BEAST2' (<https://www.beast2.org>) is a widely used    Bayesian phylogenetic tool, that uses DNA/RNA/protein data    and many model priors to create a posterior of jointly estimated     phylogenies and parameters.    'BEAST2' is commonly accompanied by 'BEAUti 2', 'Tracer' and 'DensiTree'.    'babette' provides for an alternative workflow of using     all these tools separately. This allows doing complex Bayesian    phylogenetics easily and reproducibly from 'R'. "
"MSCquartets",0,1,1,"2019-12-16","Methods for analyzing and using quartets displayed on a collection of gene trees,       primarily to make inferences about the species tree or network under the multi-species       coalescent model. These include quartet hypothesis tests for the model, as developed by       Mitchell et al. (2019) <doi:10.1214/19-EJS1576>, the species tree inference routines       based on quartet distances of Rhodes (2019) <doi:10.1109/TCBB.2019.2917204> and       Yourdkhani and Rhodes (2019), and the NANUQ algorithm for inference of level-1       species networks of Allman et al. (2019) <arXiv:1905.07050>."
"SCCI",0,1,3,"2019-05-24 11:40","An efficient implementation of SCCI using 'Rcpp'. SCCI is short for the Stochastic Complexity-based Conditional Independence criterium (Marx and Vreeken, 2019). SCCI is an asymptotically unbiased and L2 consistent estimator of (conditional) mutual information for discrete data. "
"ParallelPC",0,1,3,"2015-10-03 01:08","Parallelise constraint based causality discovery and causal inference methods. The parallelised algorithms in the package will generate the same results as that of the 'pcalg' package but will be much more efficient. "
"iTOP",1,1,3,"2018-04-04 15:11","Infers a topology of relationships between different datasets, such as multi-omics and phenotypic data recorded on the same samples. We based this methodology on the RV coefficient (Robert & Escoufier, 1976, <doi:10.2307/2347233>), a measure of matrix correlation, which we have extended for partial matrix correlations and binary data (Aben et al., 2018, <doi:10.1101/293993>)."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"pcgen",0,1,2,"2018-08-02 23:20","Implements the pcgen algorithm, which is a modified version of the standard pc-algorithm,             with specific conditional independence tests and modified orientation rules. pcgen extends 			 the approach of Valente et al. (2010) <doi:10.1534/genetics.109.112979> with reconstruction of 			 direct genetic effects."
"MRPC",0,1,4,"2019-05-09 23:30","A PC Algorithm with the Principle of Mendelian Randomization. This package implements the MRPC             (PC with the principle of Mendelian randomization) algorithm to infer causal graphs. It also             contains functions to simulate data under a certain topology, to visualize a graph in different             ways, and to compare graphs and quantify the differences.             See Badsha and Fu (2019) <doi.org/10.3389/fgene.2019.00460>,Badsha, Martin and Fu (2018) <arXiv:1806.01899>. "
"mDAG",0,1,4,"2019-03-13 19:20","Learning a mixed directed acyclic graph based on both continuous and categorical data."
"BiDAG",0,1,9,"2020-04-12 11:50","Implementation of a collection of MCMC methods for Bayesian structure learning    of directed acyclic graphs (DAGs), both from continuous and discrete data. For efficient    inference on larger DAGs, the space of DAGs is pruned according to the data. To filter    the search space, the algorithm employs a hybrid approach, combining constraint-based     learning with search and score. A reduced search space is initially defined on the basis    of a skeleton obtained by means of the PC-algorithm, and then iteratively improved with    search and score. Search and score is then performed following two approaches:    Order MCMC, or Partition MCMC.    The BGe score is implemented for continuous data and the BDe score is implemented    for binary data or categorical data. The algorithms may provide the maximum a posteriori     (MAP) graph or a sample (a collection of DAGs) from the posterior distribution given the data.    All algorithms are also applicable for structure learning and sampling for dynamic Bayesian networks.    References:    J. Kuipers, P. Suter and G. Moffa (2018) <arXiv:1803.07859v2>,    N. Friedman and D. Koller (2003) <doi:10.1023/A:1020249912095>,     D. Geiger and D. Heckerman (2002) <doi:10.1214/aos/1035844981>,     J. Kuipers and G. Moffa (2017) <doi:10.1080/01621459.2015.1133426>,     M. Kalisch et al.(2012) <doi:10.18637/jss.v047.i11>."
"SCCI",0,1,3,"2019-05-24 11:40","An efficient implementation of SCCI using 'Rcpp'. SCCI is short for the Stochastic Complexity-based Conditional Independence criterium (Marx and Vreeken, 2019). SCCI is an asymptotically unbiased and L2 consistent estimator of (conditional) mutual information for discrete data. "
"ParallelPC",0,1,3,"2015-10-03 01:08","Parallelise constraint based causality discovery and causal inference methods. The parallelised algorithms in the package will generate the same results as that of the 'pcalg' package but will be much more efficient. "
"iTOP",1,1,3,"2018-04-04 15:11","Infers a topology of relationships between different datasets, such as multi-omics and phenotypic data recorded on the same samples. We based this methodology on the RV coefficient (Robert & Escoufier, 1976, <doi:10.2307/2347233>), a measure of matrix correlation, which we have extended for partial matrix correlations and binary data (Aben et al., 2018, <doi:10.1101/293993>)."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"pcgen",0,1,2,"2018-08-02 23:20","Implements the pcgen algorithm, which is a modified version of the standard pc-algorithm,             with specific conditional independence tests and modified orientation rules. pcgen extends 			 the approach of Valente et al. (2010) <doi:10.1534/genetics.109.112979> with reconstruction of 			 direct genetic effects."
"MRPC",0,1,4,"2019-05-09 23:30","A PC Algorithm with the Principle of Mendelian Randomization. This package implements the MRPC             (PC with the principle of Mendelian randomization) algorithm to infer causal graphs. It also             contains functions to simulate data under a certain topology, to visualize a graph in different             ways, and to compare graphs and quantify the differences.             See Badsha and Fu (2019) <doi.org/10.3389/fgene.2019.00460>,Badsha, Martin and Fu (2018) <arXiv:1806.01899>. "
"mDAG",0,1,4,"2019-03-13 19:20","Learning a mixed directed acyclic graph based on both continuous and categorical data."
"BiDAG",0,1,9,"2020-04-12 11:50","Implementation of a collection of MCMC methods for Bayesian structure learning    of directed acyclic graphs (DAGs), both from continuous and discrete data. For efficient    inference on larger DAGs, the space of DAGs is pruned according to the data. To filter    the search space, the algorithm employs a hybrid approach, combining constraint-based     learning with search and score. A reduced search space is initially defined on the basis    of a skeleton obtained by means of the PC-algorithm, and then iteratively improved with    search and score. Search and score is then performed following two approaches:    Order MCMC, or Partition MCMC.    The BGe score is implemented for continuous data and the BDe score is implemented    for binary data or categorical data. The algorithms may provide the maximum a posteriori     (MAP) graph or a sample (a collection of DAGs) from the posterior distribution given the data.    All algorithms are also applicable for structure learning and sampling for dynamic Bayesian networks.    References:    J. Kuipers, P. Suter and G. Moffa (2018) <arXiv:1803.07859v2>,    N. Friedman and D. Koller (2003) <doi:10.1023/A:1020249912095>,     D. Geiger and D. Heckerman (2002) <doi:10.1214/aos/1035844981>,     J. Kuipers and G. Moffa (2017) <doi:10.1080/01621459.2015.1133426>,     M. Kalisch et al.(2012) <doi:10.18637/jss.v047.i11>."
"SCCI",0,1,3,"2019-05-24 11:40","An efficient implementation of SCCI using 'Rcpp'. SCCI is short for the Stochastic Complexity-based Conditional Independence criterium (Marx and Vreeken, 2019). SCCI is an asymptotically unbiased and L2 consistent estimator of (conditional) mutual information for discrete data. "
"ParallelPC",0,1,3,"2015-10-03 01:08","Parallelise constraint based causality discovery and causal inference methods. The parallelised algorithms in the package will generate the same results as that of the 'pcalg' package but will be much more efficient. "
"iTOP",1,1,3,"2018-04-04 15:11","Infers a topology of relationships between different datasets, such as multi-omics and phenotypic data recorded on the same samples. We based this methodology on the RV coefficient (Robert & Escoufier, 1976, <doi:10.2307/2347233>), a measure of matrix correlation, which we have extended for partial matrix correlations and binary data (Aben et al., 2018, <doi:10.1101/293993>)."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"pcgen",0,1,2,"2018-08-02 23:20","Implements the pcgen algorithm, which is a modified version of the standard pc-algorithm,             with specific conditional independence tests and modified orientation rules. pcgen extends 			 the approach of Valente et al. (2010) <doi:10.1534/genetics.109.112979> with reconstruction of 			 direct genetic effects."
"MRPC",0,1,4,"2019-05-09 23:30","A PC Algorithm with the Principle of Mendelian Randomization. This package implements the MRPC             (PC with the principle of Mendelian randomization) algorithm to infer causal graphs. It also             contains functions to simulate data under a certain topology, to visualize a graph in different             ways, and to compare graphs and quantify the differences.             See Badsha and Fu (2019) <doi.org/10.3389/fgene.2019.00460>,Badsha, Martin and Fu (2018) <arXiv:1806.01899>. "
"mDAG",0,1,4,"2019-03-13 19:20","Learning a mixed directed acyclic graph based on both continuous and categorical data."
"BiDAG",0,1,9,"2020-04-12 11:50","Implementation of a collection of MCMC methods for Bayesian structure learning    of directed acyclic graphs (DAGs), both from continuous and discrete data. For efficient    inference on larger DAGs, the space of DAGs is pruned according to the data. To filter    the search space, the algorithm employs a hybrid approach, combining constraint-based     learning with search and score. A reduced search space is initially defined on the basis    of a skeleton obtained by means of the PC-algorithm, and then iteratively improved with    search and score. Search and score is then performed following two approaches:    Order MCMC, or Partition MCMC.    The BGe score is implemented for continuous data and the BDe score is implemented    for binary data or categorical data. The algorithms may provide the maximum a posteriori     (MAP) graph or a sample (a collection of DAGs) from the posterior distribution given the data.    All algorithms are also applicable for structure learning and sampling for dynamic Bayesian networks.    References:    J. Kuipers, P. Suter and G. Moffa (2018) <arXiv:1803.07859v2>,    N. Friedman and D. Koller (2003) <doi:10.1023/A:1020249912095>,     D. Geiger and D. Heckerman (2002) <doi:10.1214/aos/1035844981>,     J. Kuipers and G. Moffa (2017) <doi:10.1080/01621459.2015.1133426>,     M. Kalisch et al.(2012) <doi:10.18637/jss.v047.i11>."
"SCCI",0,1,3,"2019-05-24 11:40","An efficient implementation of SCCI using 'Rcpp'. SCCI is short for the Stochastic Complexity-based Conditional Independence criterium (Marx and Vreeken, 2019). SCCI is an asymptotically unbiased and L2 consistent estimator of (conditional) mutual information for discrete data. "
"ParallelPC",0,1,3,"2015-10-03 01:08","Parallelise constraint based causality discovery and causal inference methods. The parallelised algorithms in the package will generate the same results as that of the 'pcalg' package but will be much more efficient. "
"iTOP",1,1,3,"2018-04-04 15:11","Infers a topology of relationships between different datasets, such as multi-omics and phenotypic data recorded on the same samples. We based this methodology on the RV coefficient (Robert & Escoufier, 1976, <doi:10.2307/2347233>), a measure of matrix correlation, which we have extended for partial matrix correlations and binary data (Aben et al., 2018, <doi:10.1101/293993>)."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"pcgen",0,1,2,"2018-08-02 23:20","Implements the pcgen algorithm, which is a modified version of the standard pc-algorithm,             with specific conditional independence tests and modified orientation rules. pcgen extends 			 the approach of Valente et al. (2010) <doi:10.1534/genetics.109.112979> with reconstruction of 			 direct genetic effects."
"MRPC",0,1,4,"2019-05-09 23:30","A PC Algorithm with the Principle of Mendelian Randomization. This package implements the MRPC             (PC with the principle of Mendelian randomization) algorithm to infer causal graphs. It also             contains functions to simulate data under a certain topology, to visualize a graph in different             ways, and to compare graphs and quantify the differences.             See Badsha and Fu (2019) <doi.org/10.3389/fgene.2019.00460>,Badsha, Martin and Fu (2018) <arXiv:1806.01899>. "
"mDAG",0,1,4,"2019-03-13 19:20","Learning a mixed directed acyclic graph based on both continuous and categorical data."
"BiDAG",0,1,9,"2020-04-12 11:50","Implementation of a collection of MCMC methods for Bayesian structure learning    of directed acyclic graphs (DAGs), both from continuous and discrete data. For efficient    inference on larger DAGs, the space of DAGs is pruned according to the data. To filter    the search space, the algorithm employs a hybrid approach, combining constraint-based     learning with search and score. A reduced search space is initially defined on the basis    of a skeleton obtained by means of the PC-algorithm, and then iteratively improved with    search and score. Search and score is then performed following two approaches:    Order MCMC, or Partition MCMC.    The BGe score is implemented for continuous data and the BDe score is implemented    for binary data or categorical data. The algorithms may provide the maximum a posteriori     (MAP) graph or a sample (a collection of DAGs) from the posterior distribution given the data.    All algorithms are also applicable for structure learning and sampling for dynamic Bayesian networks.    References:    J. Kuipers, P. Suter and G. Moffa (2018) <arXiv:1803.07859v2>,    N. Friedman and D. Koller (2003) <doi:10.1023/A:1020249912095>,     D. Geiger and D. Heckerman (2002) <doi:10.1214/aos/1035844981>,     J. Kuipers and G. Moffa (2017) <doi:10.1080/01621459.2015.1133426>,     M. Kalisch et al.(2012) <doi:10.18637/jss.v047.i11>."
"SCCI",0,1,3,"2019-05-24 11:40","An efficient implementation of SCCI using 'Rcpp'. SCCI is short for the Stochastic Complexity-based Conditional Independence criterium (Marx and Vreeken, 2019). SCCI is an asymptotically unbiased and L2 consistent estimator of (conditional) mutual information for discrete data. "
"ParallelPC",0,1,3,"2015-10-03 01:08","Parallelise constraint based causality discovery and causal inference methods. The parallelised algorithms in the package will generate the same results as that of the 'pcalg' package but will be much more efficient. "
"iTOP",1,1,3,"2018-04-04 15:11","Infers a topology of relationships between different datasets, such as multi-omics and phenotypic data recorded on the same samples. We based this methodology on the RV coefficient (Robert & Escoufier, 1976, <doi:10.2307/2347233>), a measure of matrix correlation, which we have extended for partial matrix correlations and binary data (Aben et al., 2018, <doi:10.1101/293993>)."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"pcgen",0,1,2,"2018-08-02 23:20","Implements the pcgen algorithm, which is a modified version of the standard pc-algorithm,             with specific conditional independence tests and modified orientation rules. pcgen extends 			 the approach of Valente et al. (2010) <doi:10.1534/genetics.109.112979> with reconstruction of 			 direct genetic effects."
"MRPC",0,1,4,"2019-05-09 23:30","A PC Algorithm with the Principle of Mendelian Randomization. This package implements the MRPC             (PC with the principle of Mendelian randomization) algorithm to infer causal graphs. It also             contains functions to simulate data under a certain topology, to visualize a graph in different             ways, and to compare graphs and quantify the differences.             See Badsha and Fu (2019) <doi.org/10.3389/fgene.2019.00460>,Badsha, Martin and Fu (2018) <arXiv:1806.01899>. "
"mDAG",0,1,4,"2019-03-13 19:20","Learning a mixed directed acyclic graph based on both continuous and categorical data."
"BiDAG",0,1,9,"2020-04-12 11:50","Implementation of a collection of MCMC methods for Bayesian structure learning    of directed acyclic graphs (DAGs), both from continuous and discrete data. For efficient    inference on larger DAGs, the space of DAGs is pruned according to the data. To filter    the search space, the algorithm employs a hybrid approach, combining constraint-based     learning with search and score. A reduced search space is initially defined on the basis    of a skeleton obtained by means of the PC-algorithm, and then iteratively improved with    search and score. Search and score is then performed following two approaches:    Order MCMC, or Partition MCMC.    The BGe score is implemented for continuous data and the BDe score is implemented    for binary data or categorical data. The algorithms may provide the maximum a posteriori     (MAP) graph or a sample (a collection of DAGs) from the posterior distribution given the data.    All algorithms are also applicable for structure learning and sampling for dynamic Bayesian networks.    References:    J. Kuipers, P. Suter and G. Moffa (2018) <arXiv:1803.07859v2>,    N. Friedman and D. Koller (2003) <doi:10.1023/A:1020249912095>,     D. Geiger and D. Heckerman (2002) <doi:10.1214/aos/1035844981>,     J. Kuipers and G. Moffa (2017) <doi:10.1080/01621459.2015.1133426>,     M. Kalisch et al.(2012) <doi:10.18637/jss.v047.i11>."
"dalmatian",7,1,2,"2018-01-29 13:44","Automates fitting of double GLM in 'JAGS'. Includes automatic    generation of 'JAGS' scripts, running 'JAGS' or 'nimble' via the 'rjags'    and 'nimble' package, and summarizing the resulting output."
"bridgesampling",7,8,9,"2020-01-16 14:40","Provides functions for estimating marginal likelihoods, Bayes    factors, posterior model probabilities, and normalizing constants in general,    via different versions of bridge sampling (Meng & Wong, 1996,     <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."
"exPrior",1,1,2,"2019-11-13 18:50","The aim of this package is to provide practitioners of statistics in  geology, hydrology, etc. a tool to derive prior distributions for Bayesian   inference. Prior distributions summarize knowledge from studies at similar sites.   The main features of the package are to (i) generate prior distributions based   on external data only; (ii) to account for possible autocorrelation  in the data, and (iii) to account for available soft data, say, in   the form of expert information on bounds and moments."
"BayesNSGP",0,1,2,"2019-08-29 14:50","Enables off-the-shelf functionality for fully Bayesian, nonstationary Gaussian process modeling. The approach to nonstationary modeling involves a closed-form, convolution-based covariance function with spatially-varying parameters; these parameter processes can be specified either deterministically (using covariates or basis functions) or stochastically (using approximate Gaussian processes). Stationary Gaussian processes are a special case of our methodology, and we furthermore implement approximate Gaussian process inference to account for very large spatial data sets (Finley, et al (2017) <arXiv:1702.00434v2>). Bayesian inference is carried out using Markov chain Monte Carlo methods via the 'nimble' package, and posterior prediction for the Gaussian process at unobserved locations is provided as a post-processing step."
"dalmatian",7,1,2,"2018-01-29 13:44","Automates fitting of double GLM in 'JAGS'. Includes automatic    generation of 'JAGS' scripts, running 'JAGS' or 'nimble' via the 'rjags'    and 'nimble' package, and summarizing the resulting output."
"bridgesampling",7,8,9,"2020-01-16 14:40","Provides functions for estimating marginal likelihoods, Bayes    factors, posterior model probabilities, and normalizing constants in general,    via different versions of bridge sampling (Meng & Wong, 1996,     <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."
"exPrior",1,1,2,"2019-11-13 18:50","The aim of this package is to provide practitioners of statistics in  geology, hydrology, etc. a tool to derive prior distributions for Bayesian   inference. Prior distributions summarize knowledge from studies at similar sites.   The main features of the package are to (i) generate prior distributions based   on external data only; (ii) to account for possible autocorrelation  in the data, and (iii) to account for available soft data, say, in   the form of expert information on bounds and moments."
"BayesNSGP",0,1,2,"2019-08-29 14:50","Enables off-the-shelf functionality for fully Bayesian, nonstationary Gaussian process modeling. The approach to nonstationary modeling involves a closed-form, convolution-based covariance function with spatially-varying parameters; these parameter processes can be specified either deterministically (using covariates or basis functions) or stochastically (using approximate Gaussian processes). Stationary Gaussian processes are a special case of our methodology, and we furthermore implement approximate Gaussian process inference to account for very large spatial data sets (Finley, et al (2017) <arXiv:1702.00434v2>). Bayesian inference is carried out using Markov chain Monte Carlo methods via the 'nimble' package, and posterior prediction for the Gaussian process at unobserved locations is provided as a post-processing step."
"dalmatian",7,1,2,"2018-01-29 13:44","Automates fitting of double GLM in 'JAGS'. Includes automatic    generation of 'JAGS' scripts, running 'JAGS' or 'nimble' via the 'rjags'    and 'nimble' package, and summarizing the resulting output."
"bridgesampling",7,8,9,"2020-01-16 14:40","Provides functions for estimating marginal likelihoods, Bayes    factors, posterior model probabilities, and normalizing constants in general,    via different versions of bridge sampling (Meng & Wong, 1996,     <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."
"exPrior",1,1,2,"2019-11-13 18:50","The aim of this package is to provide practitioners of statistics in  geology, hydrology, etc. a tool to derive prior distributions for Bayesian   inference. Prior distributions summarize knowledge from studies at similar sites.   The main features of the package are to (i) generate prior distributions based   on external data only; (ii) to account for possible autocorrelation  in the data, and (iii) to account for available soft data, say, in   the form of expert information on bounds and moments."
"BayesNSGP",0,1,2,"2019-08-29 14:50","Enables off-the-shelf functionality for fully Bayesian, nonstationary Gaussian process modeling. The approach to nonstationary modeling involves a closed-form, convolution-based covariance function with spatially-varying parameters; these parameter processes can be specified either deterministically (using covariates or basis functions) or stochastically (using approximate Gaussian processes). Stationary Gaussian processes are a special case of our methodology, and we furthermore implement approximate Gaussian process inference to account for very large spatial data sets (Finley, et al (2017) <arXiv:1702.00434v2>). Bayesian inference is carried out using Markov chain Monte Carlo methods via the 'nimble' package, and posterior prediction for the Gaussian process at unobserved locations is provided as a post-processing step."
"dalmatian",7,1,2,"2018-01-29 13:44","Automates fitting of double GLM in 'JAGS'. Includes automatic    generation of 'JAGS' scripts, running 'JAGS' or 'nimble' via the 'rjags'    and 'nimble' package, and summarizing the resulting output."
"bridgesampling",7,8,9,"2020-01-16 14:40","Provides functions for estimating marginal likelihoods, Bayes    factors, posterior model probabilities, and normalizing constants in general,    via different versions of bridge sampling (Meng & Wong, 1996,     <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."
"exPrior",1,1,2,"2019-11-13 18:50","The aim of this package is to provide practitioners of statistics in  geology, hydrology, etc. a tool to derive prior distributions for Bayesian   inference. Prior distributions summarize knowledge from studies at similar sites.   The main features of the package are to (i) generate prior distributions based   on external data only; (ii) to account for possible autocorrelation  in the data, and (iii) to account for available soft data, say, in   the form of expert information on bounds and moments."
"BayesNSGP",0,1,2,"2019-08-29 14:50","Enables off-the-shelf functionality for fully Bayesian, nonstationary Gaussian process modeling. The approach to nonstationary modeling involves a closed-form, convolution-based covariance function with spatially-varying parameters; these parameter processes can be specified either deterministically (using covariates or basis functions) or stochastically (using approximate Gaussian processes). Stationary Gaussian processes are a special case of our methodology, and we furthermore implement approximate Gaussian process inference to account for very large spatial data sets (Finley, et al (2017) <arXiv:1702.00434v2>). Bayesian inference is carried out using Markov chain Monte Carlo methods via the 'nimble' package, and posterior prediction for the Gaussian process at unobserved locations is provided as a post-processing step."
"dalmatian",7,1,2,"2018-01-29 13:44","Automates fitting of double GLM in 'JAGS'. Includes automatic    generation of 'JAGS' scripts, running 'JAGS' or 'nimble' via the 'rjags'    and 'nimble' package, and summarizing the resulting output."
"bridgesampling",7,8,9,"2020-01-16 14:40","Provides functions for estimating marginal likelihoods, Bayes    factors, posterior model probabilities, and normalizing constants in general,    via different versions of bridge sampling (Meng & Wong, 1996,     <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."
"exPrior",1,1,2,"2019-11-13 18:50","The aim of this package is to provide practitioners of statistics in  geology, hydrology, etc. a tool to derive prior distributions for Bayesian   inference. Prior distributions summarize knowledge from studies at similar sites.   The main features of the package are to (i) generate prior distributions based   on external data only; (ii) to account for possible autocorrelation  in the data, and (iii) to account for available soft data, say, in   the form of expert information on bounds and moments."
"BayesNSGP",0,1,2,"2019-08-29 14:50","Enables off-the-shelf functionality for fully Bayesian, nonstationary Gaussian process modeling. The approach to nonstationary modeling involves a closed-form, convolution-based covariance function with spatially-varying parameters; these parameter processes can be specified either deterministically (using covariates or basis functions) or stochastically (using approximate Gaussian processes). Stationary Gaussian processes are a special case of our methodology, and we furthermore implement approximate Gaussian process inference to account for very large spatial data sets (Finley, et al (2017) <arXiv:1702.00434v2>). Bayesian inference is carried out using Markov chain Monte Carlo methods via the 'nimble' package, and posterior prediction for the Gaussian process at unobserved locations is provided as a post-processing step."
"dalmatian",7,1,2,"2018-01-29 13:44","Automates fitting of double GLM in 'JAGS'. Includes automatic    generation of 'JAGS' scripts, running 'JAGS' or 'nimble' via the 'rjags'    and 'nimble' package, and summarizing the resulting output."
"bridgesampling",7,8,9,"2020-01-16 14:40","Provides functions for estimating marginal likelihoods, Bayes    factors, posterior model probabilities, and normalizing constants in general,    via different versions of bridge sampling (Meng & Wong, 1996,     <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."
"exPrior",1,1,2,"2019-11-13 18:50","The aim of this package is to provide practitioners of statistics in  geology, hydrology, etc. a tool to derive prior distributions for Bayesian   inference. Prior distributions summarize knowledge from studies at similar sites.   The main features of the package are to (i) generate prior distributions based   on external data only; (ii) to account for possible autocorrelation  in the data, and (iii) to account for available soft data, say, in   the form of expert information on bounds and moments."
"BayesNSGP",0,1,2,"2019-08-29 14:50","Enables off-the-shelf functionality for fully Bayesian, nonstationary Gaussian process modeling. The approach to nonstationary modeling involves a closed-form, convolution-based covariance function with spatially-varying parameters; these parameter processes can be specified either deterministically (using covariates or basis functions) or stochastically (using approximate Gaussian processes). Stationary Gaussian processes are a special case of our methodology, and we furthermore implement approximate Gaussian process inference to account for very large spatial data sets (Finley, et al (2017) <arXiv:1702.00434v2>). Bayesian inference is carried out using Markov chain Monte Carlo methods via the 'nimble' package, and posterior prediction for the Gaussian process at unobserved locations is provided as a post-processing step."
"dalmatian",7,1,2,"2018-01-29 13:44","Automates fitting of double GLM in 'JAGS'. Includes automatic    generation of 'JAGS' scripts, running 'JAGS' or 'nimble' via the 'rjags'    and 'nimble' package, and summarizing the resulting output."
"bridgesampling",7,8,9,"2020-01-16 14:40","Provides functions for estimating marginal likelihoods, Bayes    factors, posterior model probabilities, and normalizing constants in general,    via different versions of bridge sampling (Meng & Wong, 1996,     <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."
"exPrior",1,1,2,"2019-11-13 18:50","The aim of this package is to provide practitioners of statistics in  geology, hydrology, etc. a tool to derive prior distributions for Bayesian   inference. Prior distributions summarize knowledge from studies at similar sites.   The main features of the package are to (i) generate prior distributions based   on external data only; (ii) to account for possible autocorrelation  in the data, and (iii) to account for available soft data, say, in   the form of expert information on bounds and moments."
"BayesNSGP",0,1,2,"2019-08-29 14:50","Enables off-the-shelf functionality for fully Bayesian, nonstationary Gaussian process modeling. The approach to nonstationary modeling involves a closed-form, convolution-based covariance function with spatially-varying parameters; these parameter processes can be specified either deterministically (using covariates or basis functions) or stochastically (using approximate Gaussian processes). Stationary Gaussian processes are a special case of our methodology, and we furthermore implement approximate Gaussian process inference to account for very large spatial data sets (Finley, et al (2017) <arXiv:1702.00434v2>). Bayesian inference is carried out using Markov chain Monte Carlo methods via the 'nimble' package, and posterior prediction for the Gaussian process at unobserved locations is provided as a post-processing step."
"dalmatian",7,1,2,"2018-01-29 13:44","Automates fitting of double GLM in 'JAGS'. Includes automatic    generation of 'JAGS' scripts, running 'JAGS' or 'nimble' via the 'rjags'    and 'nimble' package, and summarizing the resulting output."
"bridgesampling",7,8,9,"2020-01-16 14:40","Provides functions for estimating marginal likelihoods, Bayes    factors, posterior model probabilities, and normalizing constants in general,    via different versions of bridge sampling (Meng & Wong, 1996,     <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."
"exPrior",1,1,2,"2019-11-13 18:50","The aim of this package is to provide practitioners of statistics in  geology, hydrology, etc. a tool to derive prior distributions for Bayesian   inference. Prior distributions summarize knowledge from studies at similar sites.   The main features of the package are to (i) generate prior distributions based   on external data only; (ii) to account for possible autocorrelation  in the data, and (iii) to account for available soft data, say, in   the form of expert information on bounds and moments."
"BayesNSGP",0,1,2,"2019-08-29 14:50","Enables off-the-shelf functionality for fully Bayesian, nonstationary Gaussian process modeling. The approach to nonstationary modeling involves a closed-form, convolution-based covariance function with spatially-varying parameters; these parameter processes can be specified either deterministically (using covariates or basis functions) or stochastically (using approximate Gaussian processes). Stationary Gaussian processes are a special case of our methodology, and we furthermore implement approximate Gaussian process inference to account for very large spatial data sets (Finley, et al (2017) <arXiv:1702.00434v2>). Bayesian inference is carried out using Markov chain Monte Carlo methods via the 'nimble' package, and posterior prediction for the Gaussian process at unobserved locations is provided as a post-processing step."
"dalmatian",7,1,2,"2018-01-29 13:44","Automates fitting of double GLM in 'JAGS'. Includes automatic    generation of 'JAGS' scripts, running 'JAGS' or 'nimble' via the 'rjags'    and 'nimble' package, and summarizing the resulting output."
"bridgesampling",7,8,9,"2020-01-16 14:40","Provides functions for estimating marginal likelihoods, Bayes    factors, posterior model probabilities, and normalizing constants in general,    via different versions of bridge sampling (Meng & Wong, 1996,     <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."
"exPrior",1,1,2,"2019-11-13 18:50","The aim of this package is to provide practitioners of statistics in  geology, hydrology, etc. a tool to derive prior distributions for Bayesian   inference. Prior distributions summarize knowledge from studies at similar sites.   The main features of the package are to (i) generate prior distributions based   on external data only; (ii) to account for possible autocorrelation  in the data, and (iii) to account for available soft data, say, in   the form of expert information on bounds and moments."
"BayesNSGP",0,1,2,"2019-08-29 14:50","Enables off-the-shelf functionality for fully Bayesian, nonstationary Gaussian process modeling. The approach to nonstationary modeling involves a closed-form, convolution-based covariance function with spatially-varying parameters; these parameter processes can be specified either deterministically (using covariates or basis functions) or stochastically (using approximate Gaussian processes). Stationary Gaussian processes are a special case of our methodology, and we furthermore implement approximate Gaussian process inference to account for very large spatial data sets (Finley, et al (2017) <arXiv:1702.00434v2>). Bayesian inference is carried out using Markov chain Monte Carlo methods via the 'nimble' package, and posterior prediction for the Gaussian process at unobserved locations is provided as a post-processing step."
"NetworkComparisonTest",0,1,3,"2016-10-29 00:25","This permutation based hypothesis test, suited for Gaussian and binary data,     assesses the difference between two networks based on several invariance measures     (e.g., network structure invariance, global strength invariance, edge invariance).     Network structures are estimated with l1-regularized partial correlations (Gaussian data)     or with l1-regularized logistic regression (eLasso, binary data). Suited for comparison     of independent and dependent samples. For dependent samples, only supported for data of     one group which is measured twice. See van Borkulo et al. (2017)     <doi:10.13140/RG.2.2.29455.38569>."
"BGGM",11,2,4,"2020-07-23 22:32","Fit Bayesian Gaussian graphical models. The methods are separated into     two Bayesian approaches for inference: hypothesis testing and estimation. There are     extensions for confirmatory hypothesis testing, comparing Gaussian graphical models,     and node wise predictability. These methods were recently introduced in the Gaussian     graphical model literature, including     Williams (2019) <doi:10.31234/osf.io/x8dpr>,     Williams and Mulder (2019) <doi:10.31234/osf.io/ypxd8>,    Williams, Rast, Pericchi, and Mulder (2019) <doi:10.31234/osf.io/yt386>."
"NetworkComparisonTest",0,1,3,"2016-10-29 00:25","This permutation based hypothesis test, suited for Gaussian and binary data,     assesses the difference between two networks based on several invariance measures     (e.g., network structure invariance, global strength invariance, edge invariance).     Network structures are estimated with l1-regularized partial correlations (Gaussian data)     or with l1-regularized logistic regression (eLasso, binary data). Suited for comparison     of independent and dependent samples. For dependent samples, only supported for data of     one group which is measured twice. See van Borkulo et al. (2017)     <doi:10.13140/RG.2.2.29455.38569>."
"BGGM",11,2,4,"2020-07-23 22:32","Fit Bayesian Gaussian graphical models. The methods are separated into     two Bayesian approaches for inference: hypothesis testing and estimation. There are     extensions for confirmatory hypothesis testing, comparing Gaussian graphical models,     and node wise predictability. These methods were recently introduced in the Gaussian     graphical model literature, including     Williams (2019) <doi:10.31234/osf.io/x8dpr>,     Williams and Mulder (2019) <doi:10.31234/osf.io/ypxd8>,    Williams, Rast, Pericchi, and Mulder (2019) <doi:10.31234/osf.io/yt386>."
"NetworkComparisonTest",0,1,3,"2016-10-29 00:25","This permutation based hypothesis test, suited for Gaussian and binary data,     assesses the difference between two networks based on several invariance measures     (e.g., network structure invariance, global strength invariance, edge invariance).     Network structures are estimated with l1-regularized partial correlations (Gaussian data)     or with l1-regularized logistic regression (eLasso, binary data). Suited for comparison     of independent and dependent samples. For dependent samples, only supported for data of     one group which is measured twice. See van Borkulo et al. (2017)     <doi:10.13140/RG.2.2.29455.38569>."
"BGGM",11,2,4,"2020-07-23 22:32","Fit Bayesian Gaussian graphical models. The methods are separated into     two Bayesian approaches for inference: hypothesis testing and estimation. There are     extensions for confirmatory hypothesis testing, comparing Gaussian graphical models,     and node wise predictability. These methods were recently introduced in the Gaussian     graphical model literature, including     Williams (2019) <doi:10.31234/osf.io/x8dpr>,     Williams and Mulder (2019) <doi:10.31234/osf.io/ypxd8>,    Williams, Rast, Pericchi, and Mulder (2019) <doi:10.31234/osf.io/yt386>."
"teachingApps",0,1,4,"2018-06-10 06:22","Contains apps and gadgets for teaching data analysis and     statistics concepts along with how to implement them in R.  Includes     tools to make app development easier and faster by nesting apps together."
"stylo",0,1,26,"2020-04-20 12:50","Supervised and unsupervised multivariate methods, supplemented by GUI and some visualizations, to perform various analyses in the field of computational stylistics, authorship attribution, etc. For further reference, see Eder et al. (2016), <https://journal.r-project.org/archive/2016/RJ-2016-007/index.html>. You are also encouraged to visit the Computational Stylistics Group's website <https://computationalstylistics.github.io/>, where a reasonable amount of information about the package and related projects are provided."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"OpenLand",1,1,2,"2020-03-23 17:20","Tools for the analysis of land use and cover (LUC) time series. It     includes support for loading spatiotemporal raster data and synthesized     spatial plotting. Several LUC change (LUCC) metrics in regular or irregular     time intervals can be extracted and visualized through one- and multistep     sankey and chord diagrams. A complete intensity analysis according to     Aldwaik and Pontius (2012) <doi:10.1016/j.landurbplan.2012.02.010> is     implemented, including tools for the generation of standardized multilevel     output graphics."
"LUCIDus",1,1,4,"2020-05-18 10:20","An implementation for the 'LUCID' model (Peng (2019) <doi:10.1093/bioinformatics/btz667>) to jointly estimate latent unknown clusters/subgroups with integrated data.   An EM algorithm is used to obtain the latent cluster assignment and model parameter estimates.   Feature selection is achieved by applying the L1 regularization method."
"dextergui",1,1,9,"2019-10-15 14:00","Classical Test and Item analysis,   Item Response analysis and data management for educational and psychological tests."
"DataExplorer",1,1,10,"2019-03-17 07:33","Automated data exploration process for analytic tasks and predictive modeling, so    that users could focus on understanding data and extracting insights. The package scans and    analyzes each variable, and visualizes them with typical graphical techniques. Common    data processing methods are also available to treat and format data."
"BayesianNetwork",1,1,4,"2017-07-12 03:51","A 'Shiny' web application for creating interactive Bayesian Network models,    learning the structure and parameters of Bayesian networks, and utilities for classic    network analysis."
"aaSEA",1,1,2,"2019-08-01 11:10","Given a protein multiple sequence alignment, it is daunting task to assess the effects of substitutions along sequence length. 'aaSEA' package is intended to help researchers to rapidly analyse property changes caused by single, multiple and correlated amino acid substitutions in proteins. Methods for identification of co-evolving positions from multiple sequence alignment are as described in :  Pelé et al., (2017) <doi:10.4172/2379-1764.1000250>."
"teachingApps",0,1,4,"2018-06-10 06:22","Contains apps and gadgets for teaching data analysis and     statistics concepts along with how to implement them in R.  Includes     tools to make app development easier and faster by nesting apps together."
"stylo",0,1,26,"2020-04-20 12:50","Supervised and unsupervised multivariate methods, supplemented by GUI and some visualizations, to perform various analyses in the field of computational stylistics, authorship attribution, etc. For further reference, see Eder et al. (2016), <https://journal.r-project.org/archive/2016/RJ-2016-007/index.html>. You are also encouraged to visit the Computational Stylistics Group's website <https://computationalstylistics.github.io/>, where a reasonable amount of information about the package and related projects are provided."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"OpenLand",1,1,2,"2020-03-23 17:20","Tools for the analysis of land use and cover (LUC) time series. It     includes support for loading spatiotemporal raster data and synthesized     spatial plotting. Several LUC change (LUCC) metrics in regular or irregular     time intervals can be extracted and visualized through one- and multistep     sankey and chord diagrams. A complete intensity analysis according to     Aldwaik and Pontius (2012) <doi:10.1016/j.landurbplan.2012.02.010> is     implemented, including tools for the generation of standardized multilevel     output graphics."
"LUCIDus",1,1,4,"2020-05-18 10:20","An implementation for the 'LUCID' model (Peng (2019) <doi:10.1093/bioinformatics/btz667>) to jointly estimate latent unknown clusters/subgroups with integrated data.   An EM algorithm is used to obtain the latent cluster assignment and model parameter estimates.   Feature selection is achieved by applying the L1 regularization method."
"dextergui",1,1,9,"2019-10-15 14:00","Classical Test and Item analysis,   Item Response analysis and data management for educational and psychological tests."
"DataExplorer",1,1,10,"2019-03-17 07:33","Automated data exploration process for analytic tasks and predictive modeling, so    that users could focus on understanding data and extracting insights. The package scans and    analyzes each variable, and visualizes them with typical graphical techniques. Common    data processing methods are also available to treat and format data."
"BayesianNetwork",1,1,4,"2017-07-12 03:51","A 'Shiny' web application for creating interactive Bayesian Network models,    learning the structure and parameters of Bayesian networks, and utilities for classic    network analysis."
"aaSEA",1,1,2,"2019-08-01 11:10","Given a protein multiple sequence alignment, it is daunting task to assess the effects of substitutions along sequence length. 'aaSEA' package is intended to help researchers to rapidly analyse property changes caused by single, multiple and correlated amino acid substitutions in proteins. Methods for identification of co-evolving positions from multiple sequence alignment are as described in :  Pelé et al., (2017) <doi:10.4172/2379-1764.1000250>."
"teachingApps",0,1,4,"2018-06-10 06:22","Contains apps and gadgets for teaching data analysis and     statistics concepts along with how to implement them in R.  Includes     tools to make app development easier and faster by nesting apps together."
"stylo",0,1,26,"2020-04-20 12:50","Supervised and unsupervised multivariate methods, supplemented by GUI and some visualizations, to perform various analyses in the field of computational stylistics, authorship attribution, etc. For further reference, see Eder et al. (2016), <https://journal.r-project.org/archive/2016/RJ-2016-007/index.html>. You are also encouraged to visit the Computational Stylistics Group's website <https://computationalstylistics.github.io/>, where a reasonable amount of information about the package and related projects are provided."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"OpenLand",1,1,2,"2020-03-23 17:20","Tools for the analysis of land use and cover (LUC) time series. It     includes support for loading spatiotemporal raster data and synthesized     spatial plotting. Several LUC change (LUCC) metrics in regular or irregular     time intervals can be extracted and visualized through one- and multistep     sankey and chord diagrams. A complete intensity analysis according to     Aldwaik and Pontius (2012) <doi:10.1016/j.landurbplan.2012.02.010> is     implemented, including tools for the generation of standardized multilevel     output graphics."
"LUCIDus",1,1,4,"2020-05-18 10:20","An implementation for the 'LUCID' model (Peng (2019) <doi:10.1093/bioinformatics/btz667>) to jointly estimate latent unknown clusters/subgroups with integrated data.   An EM algorithm is used to obtain the latent cluster assignment and model parameter estimates.   Feature selection is achieved by applying the L1 regularization method."
"dextergui",1,1,9,"2019-10-15 14:00","Classical Test and Item analysis,   Item Response analysis and data management for educational and psychological tests."
"DataExplorer",1,1,10,"2019-03-17 07:33","Automated data exploration process for analytic tasks and predictive modeling, so    that users could focus on understanding data and extracting insights. The package scans and    analyzes each variable, and visualizes them with typical graphical techniques. Common    data processing methods are also available to treat and format data."
"BayesianNetwork",1,1,4,"2017-07-12 03:51","A 'Shiny' web application for creating interactive Bayesian Network models,    learning the structure and parameters of Bayesian networks, and utilities for classic    network analysis."
"aaSEA",1,1,2,"2019-08-01 11:10","Given a protein multiple sequence alignment, it is daunting task to assess the effects of substitutions along sequence length. 'aaSEA' package is intended to help researchers to rapidly analyse property changes caused by single, multiple and correlated amino acid substitutions in proteins. Methods for identification of co-evolving positions from multiple sequence alignment are as described in :  Pelé et al., (2017) <doi:10.4172/2379-1764.1000250>."
"teachingApps",0,1,4,"2018-06-10 06:22","Contains apps and gadgets for teaching data analysis and     statistics concepts along with how to implement them in R.  Includes     tools to make app development easier and faster by nesting apps together."
"stylo",0,1,26,"2020-04-20 12:50","Supervised and unsupervised multivariate methods, supplemented by GUI and some visualizations, to perform various analyses in the field of computational stylistics, authorship attribution, etc. For further reference, see Eder et al. (2016), <https://journal.r-project.org/archive/2016/RJ-2016-007/index.html>. You are also encouraged to visit the Computational Stylistics Group's website <https://computationalstylistics.github.io/>, where a reasonable amount of information about the package and related projects are provided."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"OpenLand",1,1,2,"2020-03-23 17:20","Tools for the analysis of land use and cover (LUC) time series. It     includes support for loading spatiotemporal raster data and synthesized     spatial plotting. Several LUC change (LUCC) metrics in regular or irregular     time intervals can be extracted and visualized through one- and multistep     sankey and chord diagrams. A complete intensity analysis according to     Aldwaik and Pontius (2012) <doi:10.1016/j.landurbplan.2012.02.010> is     implemented, including tools for the generation of standardized multilevel     output graphics."
"LUCIDus",1,1,4,"2020-05-18 10:20","An implementation for the 'LUCID' model (Peng (2019) <doi:10.1093/bioinformatics/btz667>) to jointly estimate latent unknown clusters/subgroups with integrated data.   An EM algorithm is used to obtain the latent cluster assignment and model parameter estimates.   Feature selection is achieved by applying the L1 regularization method."
"dextergui",1,1,9,"2019-10-15 14:00","Classical Test and Item analysis,   Item Response analysis and data management for educational and psychological tests."
"DataExplorer",1,1,10,"2019-03-17 07:33","Automated data exploration process for analytic tasks and predictive modeling, so    that users could focus on understanding data and extracting insights. The package scans and    analyzes each variable, and visualizes them with typical graphical techniques. Common    data processing methods are also available to treat and format data."
"BayesianNetwork",1,1,4,"2017-07-12 03:51","A 'Shiny' web application for creating interactive Bayesian Network models,    learning the structure and parameters of Bayesian networks, and utilities for classic    network analysis."
"aaSEA",1,1,2,"2019-08-01 11:10","Given a protein multiple sequence alignment, it is daunting task to assess the effects of substitutions along sequence length. 'aaSEA' package is intended to help researchers to rapidly analyse property changes caused by single, multiple and correlated amino acid substitutions in proteins. Methods for identification of co-evolving positions from multiple sequence alignment are as described in :  Pelé et al., (2017) <doi:10.4172/2379-1764.1000250>."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"RIA",1,1,8,"2018-06-24 18:58","Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI and             nrrd file formats. RIA calculates first-order, gray level co-occurrence matrix,             gray level run length matrix and geometry-based statistics. Almost all calculations are done             using vectorized formulas to optimize run speeds. Calculation of several thousands of parameters             only takes minutes on a single core of a conventional PC."
"hmgm",0,1,1,"2019-12-16","Provides weighted group lasso framework for high-dimensional mixed data graph estimation.      In the graph estimation stage, the graph structure is estimated by maximizing the conditional     likelihood of one variable given the rest. We focus on the conditional loglikelihood of each variable     and fit separate regressions to estimate the parameters, much in the spirit of the neighborhood     selection approach proposed by Meinshausen-Buhlmann for the Gaussian Graphical Model and by Ravikumar    for the Ising Model. Currently, the discrete variables can only take two values. In the future, method     for general discrete data and for visualizing the estimated graph will be added.     For more details, see the linked paper."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"MPkn",1,1,1,"2018-05-07","A matrix discrete model having the form    'M[i+1] = (I + Q)*M[i]'.    The calculation of the values of 'M[i]' only for    pre-selected values of 'i'. The method of calculation    is presented in the vignette 'Fundament' ('Base'). Maybe it‘s own    idea of the author of the package. A weakness is that the method    gives information only in selected steps of the process.    It mainly refers to cases with matrices that are not Markov chain.    If ’Q' is Markov transition matrix, then MUPkL() may be    used to calculate the steady-state distribution 'p' for    'p = Q*p'.    Matrix power of non integer (matrix.powerni()) gives    the same results as a mpower() from package 'matlib'.    References:    ""Markov chains"",    (<https://en.wikipedia.org/wiki/Markov_chain#Expected_number_of_visits>).    Donald R. Burleson, Ph.D. (2005),     ""ON NON-INTEGER POWERS OF A SQUARE MATRIX"",     (<http://www.blackmesapress.com/Eigenvalues.htm>)."
"FuzzyStatProb",1,1,6,"2015-03-18 07:03","An implementation of a method for computing fuzzy numbers representing stationary probabilities of an unknown Markov chain,        from which a sequence of observations along time has been obtained. The algorithm is based on the proposal presented by James Buckley         in his book on Fuzzy probabilities (Springer, 2005), chapter 6. Package 'FuzzyNumbers' is used to represent the output probabilities."
"ctmcd",2,1,8,"2018-07-23 16:30","Functions for estimating Markov generator matrices from discrete-time observations. The implemented approaches comprise diagonal adjustment, weighted adjustment and quasi-optimization of matrix logarithm based candidate solutions, an expectation-maximization algorithm as well as a Gibbs sampler."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"modesto",0,1,3,"2018-11-09 15:30","Compute important quantities when we consider stochastic systems that are observed continuously. Such as, Cost model, Limiting distribution, Transition matrix, Transition distribution and Occupancy matrix. The methods are described, for example, Ross S. (2014), Introduction to Probability Models. Eleven Edition. Academic Press. "
"lifecontingencies",6,1,37,"2017-07-30 20:34","Classes and methods that allow the user to manage life table,    actuarial tables (also multiple decrements tables). Moreover, functions to easily    perform demographic, financial and actuarial mathematics on life contingencies    insurances calculations are contained therein."
"utiml",1,1,7,"2019-03-16 06:40","Multi-label learning strategies and others procedures to support multi-  label classification in R. The package provides a set of multi-label procedures such as  sampling methods, transformation strategies, threshold functions, pre-processing   techniques and evaluation metrics. A complete overview of the matter can be seen in  Zhang, M. and Zhou, Z. (2014) <doi:10.1109/TKDE.2013.39> and Gibaja, E. and   Ventura, S. (2015) A Tutorial on Multi-label Learning."
"SSLR",5,1,3,"2020-04-13 17:00","Providing a collection of techniques for semi-supervised     classification and regression. In semi-supervised problem, both labeled and unlabeled    data are used to train a classifier. The package includes a collection of     semi-supervised learning techniques: self-training, co-training, democratic,     decision tree, random forest, 'S3VM' ... etc, with a fairly intuitive interface     that is easy to use."
"parsnip",1,17,10,"2020-07-03 18:50","A common interface is provided to allow users to specify a model without having to remember the different argument names across different functions or computational engines (e.g. 'R', 'Spark', 'Stan', etc). "
"mlrMBO",1,5,6,"2019-12-02 18:20","Flexible and comprehensive R toolbox for model-based optimization    ('MBO'), also known as Bayesian optimization. It implements the Efficient    Global Optimization Algorithm and is designed for both single- and multi-    objective optimization with mixed continuous, categorical and conditional    parameters. The machine learning toolbox 'mlr' provide dozens of regression    learners to model the performance of the target algorithm with respect to    the parameter settings. It provides many different infill criteria to guide    the search process. Additional features include multi-point batch proposal,    parallel execution as well as visualization and sophisticated logging    mechanisms, which is especially useful for teaching and understanding of    algorithm behavior. 'mlrMBO' is implemented in a modular fashion, such that    single components can be easily replaced or adapted by the user for specific    use cases."
"mlr3learners",0,5,9,"2020-08-29 21:10","Recommended Learners for 'mlr3'. Extends 'mlr3' and 'mlr3proba'    with interfaces to essential machine learning packages on CRAN.  This    includes, but is not limited to: (penalized) linear and logistic    regression, linear and quadratic discriminant analysis, k-nearest    neighbors, naive Bayes, support vector machines, and gradient    boosting."
"mlr",1,33,21,"2020-01-10 21:00","Interface to a large number of classification and    regression techniques, including machine-readable parameter    descriptions. There is also an experimental extension for survival    analysis, clustering and general, example-specific cost-sensitive    learning. Generic resampling, including cross-validation,    bootstrapping and subsampling. Hyperparameter tuning with modern    optimization techniques, for single- and multi-objective problems.    Filter and wrapper methods for feature selection. Extension of basic    learners with additional operations common in machine learning, also    allowing for easy nested resampling. Most operations can be    parallelized."
"MachineShop",2,1,17,"2020-06-05 00:40","Meta-package for statistical and machine learning with a unified    interface for model fitting, prediction, performance assessment, and    presentation of results.  Approaches for model fitting and prediction of    numerical, categorical, or censored time-to-event outcomes include    traditional regression models, regularization methods, tree-based methods,    support vector machines, neural networks, ensembles, data preprocessing,    filtering, and model tuning and selection.  Performance metrics are provided    for model assessment and can be estimated with independent test sets, split    sampling, cross-validation, or bootstrap resampling.  Resample estimation    can be executed in parallel for faster processing and nested in cases of    model tuning and selection.  Modeling results can be summarized with    descriptive statistics; calibration curves; variable importance; partial    dependence plots; confusion matrices; and ROC, lift, and other performance    curves."
"fscaret",1,1,17,"2015-01-11 13:15","Automated feature selection using variety of models        provided by 'caret' package.        This work was funded by Poland-Singapore bilateral cooperation        project no 2/3/POL-SIN/2012."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"traineR",1,1,1,"2019-10-07","Methods to unify the different ways of creating predictive models and their different predictive formats. It includes        methods such as K-Nearest Neighbors, Decision Trees, ADA Boosting, Extreme Gradient Boosting, Random Forest, Neural Networks, Deep Learning,       Support Vector Machines and Bayesian Methods."
"rminer",0,1,12,"2014-11-07 14:06","Facilitates the use of data mining algorithms in classification and regression (including time series forecasting) tasks by presenting a short and coherent set of functions. Versions: 1.4.6 / 1.4.5 / 1.4.4 new automated machine learning (AutoML) and ensembles, via improved fit(), mining() and mparheuristic() functions, and new categorical preprocessing, via improved delevels() function; 1.4.3 new metrics (e.g., macro precision, explained variance), new ""lssvm"" model and improved mparheuristic() function; 1.4.2 new ""NMAE"" metric, ""xgboost"" and ""cv.glmnet"" models (16 classification and 18 regression models); 1.4.1 new tutorial and more robust version; 1.4 - new classification and regression models, with a total of 14 classification and 15 regression methods, including: Decision Trees, Neural Networks, Support Vector Machines, Random Forests, Bagging and Boosting; 1.3 and 1.3.1 - new classification and regression metrics; 1.2 - new input importance methods via improved Importance() function; 1.0 - first version."
"NoiseFiltersR",1,1,1,"2016-06-24","An extensive implementation of state-of-the-art and classical    algorithms to preprocess label noise in classification problems."
"MM2S",2,1,7,"2016-02-25 00:39","A single-sample classifier that generates Medulloblastoma (MB) subtype predictions for single-samples of human Medulloblastoma (MB) patients and model systems, including cell lines and mouse-models. The MM2S algorithm uses a systems-based methodology that facilitates application of the algorithm on samples irrespective of their platform or source of origin. MM2S demonstrates > 96% accuracy for patients of well-characterized normal cerebellum, Wingless (WNT), or Sonic hedgehog (SHH) subtypes, and the less-characterized Group4 (86%) and Group3 (78.2%). MM2S also enables classification of MB cell lines and mouse models into their human counterparts.This package contains function for implementing the classifier onto human data and mouse data, as well as graphical rendering of the results as PCA (Principal Component Analysis) plots and heatmaps. Deena Gendoo and Benjamin Haibe-Kains (2016) <doi:10.1186/s13029-016-0053-y>."
"mbclusterwise",0,1,1,"2016-11-22","Perform clusterwise multiblock analyses (clusterwise multiblock Partial Least Squares, clusterwise multiblock Redundancy Analysis or a regularized method between the two latter ones) associated with a F-fold cross-validation procedure to select the optimal number of clusters and dimensions."
"EnsembleBase",0,3,5,"2016-02-11 08:27","Extensible S4 classes and methods for batch training of regression and classification algorithms such as Random Forest, Gradient Boosting Machine, Neural Network, Support Vector Machines, K-Nearest Neighbors, Penalized Regression (L1/L2), and Bayesian Additive Regression Trees. These algorithms constitute a set of 'base learners', which can subsequently be combined together to form ensemble predictions. This package provides cross-validation wrappers to allow for downstream application of ensemble integration techniques, including best-error selection. All base learner estimation objects are retained, allowing for repeated prediction calls without the need for re-training. For large problems, an option is provided to save estimation objects to disk, along with prediction methods that utilize these objects. This allows users to train and predict with large ensembles of base learners without being constrained by system RAM."
"utiml",1,1,7,"2019-03-16 06:40","Multi-label learning strategies and others procedures to support multi-  label classification in R. The package provides a set of multi-label procedures such as  sampling methods, transformation strategies, threshold functions, pre-processing   techniques and evaluation metrics. A complete overview of the matter can be seen in  Zhang, M. and Zhou, Z. (2014) <doi:10.1109/TKDE.2013.39> and Gibaja, E. and   Ventura, S. (2015) A Tutorial on Multi-label Learning."
"SSLR",5,1,3,"2020-04-13 17:00","Providing a collection of techniques for semi-supervised     classification and regression. In semi-supervised problem, both labeled and unlabeled    data are used to train a classifier. The package includes a collection of     semi-supervised learning techniques: self-training, co-training, democratic,     decision tree, random forest, 'S3VM' ... etc, with a fairly intuitive interface     that is easy to use."
"parsnip",1,17,10,"2020-07-03 18:50","A common interface is provided to allow users to specify a model without having to remember the different argument names across different functions or computational engines (e.g. 'R', 'Spark', 'Stan', etc). "
"mlrMBO",1,5,6,"2019-12-02 18:20","Flexible and comprehensive R toolbox for model-based optimization    ('MBO'), also known as Bayesian optimization. It implements the Efficient    Global Optimization Algorithm and is designed for both single- and multi-    objective optimization with mixed continuous, categorical and conditional    parameters. The machine learning toolbox 'mlr' provide dozens of regression    learners to model the performance of the target algorithm with respect to    the parameter settings. It provides many different infill criteria to guide    the search process. Additional features include multi-point batch proposal,    parallel execution as well as visualization and sophisticated logging    mechanisms, which is especially useful for teaching and understanding of    algorithm behavior. 'mlrMBO' is implemented in a modular fashion, such that    single components can be easily replaced or adapted by the user for specific    use cases."
"mlr3learners",0,5,9,"2020-08-29 21:10","Recommended Learners for 'mlr3'. Extends 'mlr3' and 'mlr3proba'    with interfaces to essential machine learning packages on CRAN.  This    includes, but is not limited to: (penalized) linear and logistic    regression, linear and quadratic discriminant analysis, k-nearest    neighbors, naive Bayes, support vector machines, and gradient    boosting."
"mlr",1,33,21,"2020-01-10 21:00","Interface to a large number of classification and    regression techniques, including machine-readable parameter    descriptions. There is also an experimental extension for survival    analysis, clustering and general, example-specific cost-sensitive    learning. Generic resampling, including cross-validation,    bootstrapping and subsampling. Hyperparameter tuning with modern    optimization techniques, for single- and multi-objective problems.    Filter and wrapper methods for feature selection. Extension of basic    learners with additional operations common in machine learning, also    allowing for easy nested resampling. Most operations can be    parallelized."
"MachineShop",2,1,17,"2020-06-05 00:40","Meta-package for statistical and machine learning with a unified    interface for model fitting, prediction, performance assessment, and    presentation of results.  Approaches for model fitting and prediction of    numerical, categorical, or censored time-to-event outcomes include    traditional regression models, regularization methods, tree-based methods,    support vector machines, neural networks, ensembles, data preprocessing,    filtering, and model tuning and selection.  Performance metrics are provided    for model assessment and can be estimated with independent test sets, split    sampling, cross-validation, or bootstrap resampling.  Resample estimation    can be executed in parallel for faster processing and nested in cases of    model tuning and selection.  Modeling results can be summarized with    descriptive statistics; calibration curves; variable importance; partial    dependence plots; confusion matrices; and ROC, lift, and other performance    curves."
"fscaret",1,1,17,"2015-01-11 13:15","Automated feature selection using variety of models        provided by 'caret' package.        This work was funded by Poland-Singapore bilateral cooperation        project no 2/3/POL-SIN/2012."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"traineR",1,1,1,"2019-10-07","Methods to unify the different ways of creating predictive models and their different predictive formats. It includes        methods such as K-Nearest Neighbors, Decision Trees, ADA Boosting, Extreme Gradient Boosting, Random Forest, Neural Networks, Deep Learning,       Support Vector Machines and Bayesian Methods."
"rminer",0,1,12,"2014-11-07 14:06","Facilitates the use of data mining algorithms in classification and regression (including time series forecasting) tasks by presenting a short and coherent set of functions. Versions: 1.4.6 / 1.4.5 / 1.4.4 new automated machine learning (AutoML) and ensembles, via improved fit(), mining() and mparheuristic() functions, and new categorical preprocessing, via improved delevels() function; 1.4.3 new metrics (e.g., macro precision, explained variance), new ""lssvm"" model and improved mparheuristic() function; 1.4.2 new ""NMAE"" metric, ""xgboost"" and ""cv.glmnet"" models (16 classification and 18 regression models); 1.4.1 new tutorial and more robust version; 1.4 - new classification and regression models, with a total of 14 classification and 15 regression methods, including: Decision Trees, Neural Networks, Support Vector Machines, Random Forests, Bagging and Boosting; 1.3 and 1.3.1 - new classification and regression metrics; 1.2 - new input importance methods via improved Importance() function; 1.0 - first version."
"NoiseFiltersR",1,1,1,"2016-06-24","An extensive implementation of state-of-the-art and classical    algorithms to preprocess label noise in classification problems."
"MM2S",2,1,7,"2016-02-25 00:39","A single-sample classifier that generates Medulloblastoma (MB) subtype predictions for single-samples of human Medulloblastoma (MB) patients and model systems, including cell lines and mouse-models. The MM2S algorithm uses a systems-based methodology that facilitates application of the algorithm on samples irrespective of their platform or source of origin. MM2S demonstrates > 96% accuracy for patients of well-characterized normal cerebellum, Wingless (WNT), or Sonic hedgehog (SHH) subtypes, and the less-characterized Group4 (86%) and Group3 (78.2%). MM2S also enables classification of MB cell lines and mouse models into their human counterparts.This package contains function for implementing the classifier onto human data and mouse data, as well as graphical rendering of the results as PCA (Principal Component Analysis) plots and heatmaps. Deena Gendoo and Benjamin Haibe-Kains (2016) <doi:10.1186/s13029-016-0053-y>."
"mbclusterwise",0,1,1,"2016-11-22","Perform clusterwise multiblock analyses (clusterwise multiblock Partial Least Squares, clusterwise multiblock Redundancy Analysis or a regularized method between the two latter ones) associated with a F-fold cross-validation procedure to select the optimal number of clusters and dimensions."
"EnsembleBase",0,3,5,"2016-02-11 08:27","Extensible S4 classes and methods for batch training of regression and classification algorithms such as Random Forest, Gradient Boosting Machine, Neural Network, Support Vector Machines, K-Nearest Neighbors, Penalized Regression (L1/L2), and Bayesian Additive Regression Trees. These algorithms constitute a set of 'base learners', which can subsequently be combined together to form ensemble predictions. This package provides cross-validation wrappers to allow for downstream application of ensemble integration techniques, including best-error selection. All base learner estimation objects are retained, allowing for repeated prediction calls without the need for re-training. For large problems, an option is provided to save estimation objects to disk, along with prediction methods that utilize these objects. This allows users to train and predict with large ensembles of base learners without being constrained by system RAM."
"utiml",1,1,7,"2019-03-16 06:40","Multi-label learning strategies and others procedures to support multi-  label classification in R. The package provides a set of multi-label procedures such as  sampling methods, transformation strategies, threshold functions, pre-processing   techniques and evaluation metrics. A complete overview of the matter can be seen in  Zhang, M. and Zhou, Z. (2014) <doi:10.1109/TKDE.2013.39> and Gibaja, E. and   Ventura, S. (2015) A Tutorial on Multi-label Learning."
"SSLR",5,1,3,"2020-04-13 17:00","Providing a collection of techniques for semi-supervised     classification and regression. In semi-supervised problem, both labeled and unlabeled    data are used to train a classifier. The package includes a collection of     semi-supervised learning techniques: self-training, co-training, democratic,     decision tree, random forest, 'S3VM' ... etc, with a fairly intuitive interface     that is easy to use."
"parsnip",1,17,10,"2020-07-03 18:50","A common interface is provided to allow users to specify a model without having to remember the different argument names across different functions or computational engines (e.g. 'R', 'Spark', 'Stan', etc). "
"mlrMBO",1,5,6,"2019-12-02 18:20","Flexible and comprehensive R toolbox for model-based optimization    ('MBO'), also known as Bayesian optimization. It implements the Efficient    Global Optimization Algorithm and is designed for both single- and multi-    objective optimization with mixed continuous, categorical and conditional    parameters. The machine learning toolbox 'mlr' provide dozens of regression    learners to model the performance of the target algorithm with respect to    the parameter settings. It provides many different infill criteria to guide    the search process. Additional features include multi-point batch proposal,    parallel execution as well as visualization and sophisticated logging    mechanisms, which is especially useful for teaching and understanding of    algorithm behavior. 'mlrMBO' is implemented in a modular fashion, such that    single components can be easily replaced or adapted by the user for specific    use cases."
"mlr3learners",0,5,9,"2020-08-29 21:10","Recommended Learners for 'mlr3'. Extends 'mlr3' and 'mlr3proba'    with interfaces to essential machine learning packages on CRAN.  This    includes, but is not limited to: (penalized) linear and logistic    regression, linear and quadratic discriminant analysis, k-nearest    neighbors, naive Bayes, support vector machines, and gradient    boosting."
"mlr",1,33,21,"2020-01-10 21:00","Interface to a large number of classification and    regression techniques, including machine-readable parameter    descriptions. There is also an experimental extension for survival    analysis, clustering and general, example-specific cost-sensitive    learning. Generic resampling, including cross-validation,    bootstrapping and subsampling. Hyperparameter tuning with modern    optimization techniques, for single- and multi-objective problems.    Filter and wrapper methods for feature selection. Extension of basic    learners with additional operations common in machine learning, also    allowing for easy nested resampling. Most operations can be    parallelized."
"MachineShop",2,1,17,"2020-06-05 00:40","Meta-package for statistical and machine learning with a unified    interface for model fitting, prediction, performance assessment, and    presentation of results.  Approaches for model fitting and prediction of    numerical, categorical, or censored time-to-event outcomes include    traditional regression models, regularization methods, tree-based methods,    support vector machines, neural networks, ensembles, data preprocessing,    filtering, and model tuning and selection.  Performance metrics are provided    for model assessment and can be estimated with independent test sets, split    sampling, cross-validation, or bootstrap resampling.  Resample estimation    can be executed in parallel for faster processing and nested in cases of    model tuning and selection.  Modeling results can be summarized with    descriptive statistics; calibration curves; variable importance; partial    dependence plots; confusion matrices; and ROC, lift, and other performance    curves."
"fscaret",1,1,17,"2015-01-11 13:15","Automated feature selection using variety of models        provided by 'caret' package.        This work was funded by Poland-Singapore bilateral cooperation        project no 2/3/POL-SIN/2012."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"traineR",1,1,1,"2019-10-07","Methods to unify the different ways of creating predictive models and their different predictive formats. It includes        methods such as K-Nearest Neighbors, Decision Trees, ADA Boosting, Extreme Gradient Boosting, Random Forest, Neural Networks, Deep Learning,       Support Vector Machines and Bayesian Methods."
"rminer",0,1,12,"2014-11-07 14:06","Facilitates the use of data mining algorithms in classification and regression (including time series forecasting) tasks by presenting a short and coherent set of functions. Versions: 1.4.6 / 1.4.5 / 1.4.4 new automated machine learning (AutoML) and ensembles, via improved fit(), mining() and mparheuristic() functions, and new categorical preprocessing, via improved delevels() function; 1.4.3 new metrics (e.g., macro precision, explained variance), new ""lssvm"" model and improved mparheuristic() function; 1.4.2 new ""NMAE"" metric, ""xgboost"" and ""cv.glmnet"" models (16 classification and 18 regression models); 1.4.1 new tutorial and more robust version; 1.4 - new classification and regression models, with a total of 14 classification and 15 regression methods, including: Decision Trees, Neural Networks, Support Vector Machines, Random Forests, Bagging and Boosting; 1.3 and 1.3.1 - new classification and regression metrics; 1.2 - new input importance methods via improved Importance() function; 1.0 - first version."
"NoiseFiltersR",1,1,1,"2016-06-24","An extensive implementation of state-of-the-art and classical    algorithms to preprocess label noise in classification problems."
"MM2S",2,1,7,"2016-02-25 00:39","A single-sample classifier that generates Medulloblastoma (MB) subtype predictions for single-samples of human Medulloblastoma (MB) patients and model systems, including cell lines and mouse-models. The MM2S algorithm uses a systems-based methodology that facilitates application of the algorithm on samples irrespective of their platform or source of origin. MM2S demonstrates > 96% accuracy for patients of well-characterized normal cerebellum, Wingless (WNT), or Sonic hedgehog (SHH) subtypes, and the less-characterized Group4 (86%) and Group3 (78.2%). MM2S also enables classification of MB cell lines and mouse models into their human counterparts.This package contains function for implementing the classifier onto human data and mouse data, as well as graphical rendering of the results as PCA (Principal Component Analysis) plots and heatmaps. Deena Gendoo and Benjamin Haibe-Kains (2016) <doi:10.1186/s13029-016-0053-y>."
"mbclusterwise",0,1,1,"2016-11-22","Perform clusterwise multiblock analyses (clusterwise multiblock Partial Least Squares, clusterwise multiblock Redundancy Analysis or a regularized method between the two latter ones) associated with a F-fold cross-validation procedure to select the optimal number of clusters and dimensions."
"EnsembleBase",0,3,5,"2016-02-11 08:27","Extensible S4 classes and methods for batch training of regression and classification algorithms such as Random Forest, Gradient Boosting Machine, Neural Network, Support Vector Machines, K-Nearest Neighbors, Penalized Regression (L1/L2), and Bayesian Additive Regression Trees. These algorithms constitute a set of 'base learners', which can subsequently be combined together to form ensemble predictions. This package provides cross-validation wrappers to allow for downstream application of ensemble integration techniques, including best-error selection. All base learner estimation objects are retained, allowing for repeated prediction calls without the need for re-training. For large problems, an option is provided to save estimation objects to disk, along with prediction methods that utilize these objects. This allows users to train and predict with large ensembles of base learners without being constrained by system RAM."
"mboost",4,27,67,"2020-02-18 09:20","Functional gradient descent algorithm  (boosting) for optimizing general risk functions utilizing  component-wise (penalised) least squares estimates or regression  trees as base-learners for fitting generalized linear, additive  and interaction models to potentially high-dimensional data."
"mboost",4,27,67,"2020-02-18 09:20","Functional gradient descent algorithm  (boosting) for optimizing general risk functions utilizing  component-wise (penalised) least squares estimates or regression  trees as base-learners for fitting generalized linear, additive  and interaction models to potentially high-dimensional data."
"mboost",4,27,67,"2020-02-18 09:20","Functional gradient descent algorithm  (boosting) for optimizing general risk functions utilizing  component-wise (penalised) least squares estimates or regression  trees as base-learners for fitting generalized linear, additive  and interaction models to potentially high-dimensional data."
"inldata",0,1,1,"2020-09-17","Contains data for the U.S. Geological Survey-Idaho National   Laboratory (USGS-INL) aquifer monitoring networks administrated by the   Idaho National Laboratory Project Office in cooperation with the U.S.   Department of Energy."
"RSurvey",0,1,30,"2017-11-23 08:25","A geographic information system (GIS) graphical user interface (GUI) that  provides data viewing, management, and analysis tools."
"geomerge",0,1,3,"2018-07-31 23:20","Geospatial data integration framework that merges raster, spatial polygon, and (dynamic) spatial points data into a spatial (panel) data frame at any geographical resolution."
"inldata",0,1,1,"2020-09-17","Contains data for the U.S. Geological Survey-Idaho National   Laboratory (USGS-INL) aquifer monitoring networks administrated by the   Idaho National Laboratory Project Office in cooperation with the U.S.   Department of Energy."
"RSurvey",0,1,30,"2017-11-23 08:25","A geographic information system (GIS) graphical user interface (GUI) that  provides data viewing, management, and analysis tools."
"geomerge",0,1,3,"2018-07-31 23:20","Geospatial data integration framework that merges raster, spatial polygon, and (dynamic) spatial points data into a spatial (panel) data frame at any geographical resolution."
"rwavelet",1,1,2,"2018-09-12 17:10","Perform wavelet analysis (orthogonal, translation invariant, tensorial, 1-2-3d transforms, thresholding, block thresholding, linear,...) with applications to data compression or denoising/regression. The core of the code is a port of 'MATLAB' Wavelab toolbox written by D. Donoho, A. Maleki and M. Shahram (<https://statweb.stanford.edu/~wavelab/>)."
"ProFound",7,1,4,"2018-07-31 10:20","Core package containing all the tools for simple and advanced source extraction. This is used to create inputs for 'ProFit', or for source detection, extraction and photometry in its own right."
"pavo",4,2,21,"2019-12-11 16:30","A cohesive framework for parsing, analyzing and    organizing colour from spectral data."
"magicaxis",0,7,17,"2019-03-08 13:20","Functions to make useful (and pretty) plots for scientific plotting. Additional plotting features are added for base plotting, with particular emphasis on making attractive log axis plots."
"dynaSpec",0,1,1,"2020-06-22","A set of tools to generate dynamic spectrogram visualizations in video format."
"xRing",0,1,1,"2018-08-24","Contains functions to identify tree-ring borders based on X-ray micro-density profiles and a Graphical User Interface to visualize density profiles and correct tree-ring borders."
"TooManyCellsR",0,1,2,"2019-03-05 12:20","An R wrapper for using 'TooManyCells', a command line program for clustering, visualizing, and quantifying cell clade relationships. See <https://gregoryschwartz.github.io/too-many-cells/> for more details."
"SPUTNIK",0,1,13,"2018-10-23 13:50","A set of tools for the peak filtering of mass spectrometry  imaging data (MSI or IMS) based on spatial distribution of signal. Given a   region-of-interest (ROI), representing the spatial region where the informative  signal is expected to be localized, a series of filters determine which peak  signals are characterized by an implausible spatial distribution. The filters  reduce the dataset dimensionality and increase its information vs noise ratio,  improving the quality of the unsupervised analysis results, reducing data  dimensionality and simplifying the chemical interpretation."
"smpic",1,1,1,"2017-10-04","Creates images that are the proper size for social media. Beautiful    plots, charts and graphs wither and die if they are not shared. Social media     is perfect for this but every platform has its own image dimensions. With     'smpic' you can easily save your plots with the exact dimensions needed for     the different platforms."
"sketcher",0,1,1,"2020-05-25","An implementation of image processing effects that convert a photo into a line drawing image.     For details, please refer to Tsuda, H. (2020). sketcher: An R package for converting a photo into a sketch style image.     <doi:10.31234/osf.io/svmw5>."
"Rbgs",0,1,2,"2017-06-06 12:19","Methods that allow video reading and loading in R. Also provides nine different methods for  background subtraction."
"multifluo",0,1,2,"2018-01-24 10:29","Deals with several images of a same object, constituted of different zones. Each image constitutes a variable for a given pixel. The user can interactively select different zones of an image. Then, multivariate analysis (PCA) can be run in order to characterize the different selected zones, according to the different images. Hotelling (Hotelling, 1931, <doi:10.1214/aoms/1177732979>) and Srivastava (Srivastava, 2009, <doi:10.1016/j.jmva.2006.11.002>) tests can be run to detect multivariate differences between the zones. "
"MtreeRing",0,1,5,"2019-09-23 06:20","Use morphological image processing and edge detection algorithms to automatically measure tree ring widths on digital images. Users can also manually mark tree rings on species with complex anatomical structures. The arcs of inner-rings and angles of successive inclined ring boundaries are used to correct ring-width series. The package provides a Shiny-based application, allowing R beginners to easily analyze tree ring images and export ring-width series in standard file formats."
"mand",0,1,1,"2020-05-06","Several functions can be used to analyze neuroimaging data using multivariate methods based on the 'msma' package. For more details, please see Kawaguchi et al. (2017) <doi:10.1093/biostatistics/kxx011> and Kawaguchi (2019) <doi:10.5772/intechopen.80531>."
"Irescale",1,1,3,"2019-04-15 23:22","Provides a scaling method to obtain a standardized Moran's I measure. Moran's I is a measure for the spatial autocorrelation of a data set, it gives a measure of similarity between data and its surrounding. The range of this value must be [-1,1], but this does not happen in practice. This package scale the Moran's I value and map it into the theoretical range of [-1,1]. Once the Moran's I value is rescaled, it facilitates the comparison between projects, for instance, a researcher can calculate Moran's I in a city in China, with a sample size of n1 and area of interest a1. Another researcher runs a similar experiment in a city in Mexico with different sample size, n2, and an area of interest a2. Due to the differences between the conditions, it is not possible to compare Moran's I in a straightforward way. In this version of the package, the spatial autocorrelation Moran's I is calculated as proposed in Chen(2013) <arXiv:1606.03658>."
"IMak",0,1,9,"2020-05-06 12:20","This is an Automatic Item Generator for Psychological Assessment. Items created with the 'IMak' package should not be used in applied settings as part of the working protocol without ensuring first that the items meet the required psychometric quality standards (see Blum & Holling, 2018) <doi:10.3389/fpsyg.2018.01286>."
"ForestTools",3,1,7,"2018-04-04 22:33","Provides tools for analyzing remotely sensed forest data, including functions for detecting treetops from canopy models (Popescu & Wynne, 2004), outlining tree crowns (Meyer & Beucher, 1990) and generating spatial statistics."
"fmriqa",1,1,3,"2017-12-20 16:21","Methods for performing fMRI quality assurance (QA) measurements of  test objects. Heavily  based on the fBIRN procedures detailed by Friedman and   Glover (2006) <doi:10.1002/jmri.20583>."
"epca",1,1,1,"2020-06-26","    Exploratory principal component analysis for large-scale dataset, including sparse principal component analysis and sparse matrix approximation."
"CropDetectR",1,1,1,"2019-09-20","A helpful tool for the identification of crop rows. Methods of this package include: Excess Green color scale <https://www.researchgate.net/publication/270613992_Color_Indices_for_Weed_Identification_Under_Various_Soil_Residue_and_Lighting_Conditions>, Otsu Thresholding <https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4310076>, and Morphology <https://en.wikipedia.org/wiki/Mathematical_morphology>."
"colocr",1,1,2,"2019-05-31 11:40","Automate the co-localization analysis of fluorescence microscopy   images. Selecting regions of interest, extract pixel intensities from   the image channels and calculate different co-localization statistics. The  methods implemented in this package are based on Dunn et al. (2011)   <doi:10.1152/ajpcell.00462.2010>."
"boundingbox",1,1,1,"2020-06-09","Generate ground truth cases for object localization algorithms.     Cycle through a list of images, select points around which to generate bounding     boxes and assign classifiers. Output the coordinates, and images annotated with     boxes and labels. For an example study that uses bounding boxes for image     localization and classification see Ibrahim, Badr, Abdallah, and Eissa (2012)    ""Bounding Box Object Localization Based on Image Superpixelization""    <doi:10.1016/j.procs.2012.09.119>."
"AcuityView",0,1,1,"2017-05-09","This code provides a simple method for representing a visual scene as it may be seen by an animal with less acute vision. When using (or for more information), please cite the original publication."
"ImaginR",0,1,1,"2017-05-31","The pearl oyster, Pinctada margaritifera (Linnaeus, 1758), represents the second economic resource of French Polynesia. It is one of the only bivalves expressing a large varied range of inner shell color, & by correlation, of pearl color. This phenotypic variability is partly under genetic control, but also under environmental influence. With ImaginR, it's now possible to delimit the color phenotype of the pearl oyster's inner shell and to characterize their color variations (by the HSV color code system) with pictures."
"rwavelet",1,1,2,"2018-09-12 17:10","Perform wavelet analysis (orthogonal, translation invariant, tensorial, 1-2-3d transforms, thresholding, block thresholding, linear,...) with applications to data compression or denoising/regression. The core of the code is a port of 'MATLAB' Wavelab toolbox written by D. Donoho, A. Maleki and M. Shahram (<https://statweb.stanford.edu/~wavelab/>)."
"ProFound",7,1,4,"2018-07-31 10:20","Core package containing all the tools for simple and advanced source extraction. This is used to create inputs for 'ProFit', or for source detection, extraction and photometry in its own right."
"pavo",4,2,21,"2019-12-11 16:30","A cohesive framework for parsing, analyzing and    organizing colour from spectral data."
"magicaxis",0,7,17,"2019-03-08 13:20","Functions to make useful (and pretty) plots for scientific plotting. Additional plotting features are added for base plotting, with particular emphasis on making attractive log axis plots."
"dynaSpec",0,1,1,"2020-06-22","A set of tools to generate dynamic spectrogram visualizations in video format."
"xRing",0,1,1,"2018-08-24","Contains functions to identify tree-ring borders based on X-ray micro-density profiles and a Graphical User Interface to visualize density profiles and correct tree-ring borders."
"TooManyCellsR",0,1,2,"2019-03-05 12:20","An R wrapper for using 'TooManyCells', a command line program for clustering, visualizing, and quantifying cell clade relationships. See <https://gregoryschwartz.github.io/too-many-cells/> for more details."
"SPUTNIK",0,1,13,"2018-10-23 13:50","A set of tools for the peak filtering of mass spectrometry  imaging data (MSI or IMS) based on spatial distribution of signal. Given a   region-of-interest (ROI), representing the spatial region where the informative  signal is expected to be localized, a series of filters determine which peak  signals are characterized by an implausible spatial distribution. The filters  reduce the dataset dimensionality and increase its information vs noise ratio,  improving the quality of the unsupervised analysis results, reducing data  dimensionality and simplifying the chemical interpretation."
"smpic",1,1,1,"2017-10-04","Creates images that are the proper size for social media. Beautiful    plots, charts and graphs wither and die if they are not shared. Social media     is perfect for this but every platform has its own image dimensions. With     'smpic' you can easily save your plots with the exact dimensions needed for     the different platforms."
"sketcher",0,1,1,"2020-05-25","An implementation of image processing effects that convert a photo into a line drawing image.     For details, please refer to Tsuda, H. (2020). sketcher: An R package for converting a photo into a sketch style image.     <doi:10.31234/osf.io/svmw5>."
"Rbgs",0,1,2,"2017-06-06 12:19","Methods that allow video reading and loading in R. Also provides nine different methods for  background subtraction."
"multifluo",0,1,2,"2018-01-24 10:29","Deals with several images of a same object, constituted of different zones. Each image constitutes a variable for a given pixel. The user can interactively select different zones of an image. Then, multivariate analysis (PCA) can be run in order to characterize the different selected zones, according to the different images. Hotelling (Hotelling, 1931, <doi:10.1214/aoms/1177732979>) and Srivastava (Srivastava, 2009, <doi:10.1016/j.jmva.2006.11.002>) tests can be run to detect multivariate differences between the zones. "
"MtreeRing",0,1,5,"2019-09-23 06:20","Use morphological image processing and edge detection algorithms to automatically measure tree ring widths on digital images. Users can also manually mark tree rings on species with complex anatomical structures. The arcs of inner-rings and angles of successive inclined ring boundaries are used to correct ring-width series. The package provides a Shiny-based application, allowing R beginners to easily analyze tree ring images and export ring-width series in standard file formats."
"mand",0,1,1,"2020-05-06","Several functions can be used to analyze neuroimaging data using multivariate methods based on the 'msma' package. For more details, please see Kawaguchi et al. (2017) <doi:10.1093/biostatistics/kxx011> and Kawaguchi (2019) <doi:10.5772/intechopen.80531>."
"Irescale",1,1,3,"2019-04-15 23:22","Provides a scaling method to obtain a standardized Moran's I measure. Moran's I is a measure for the spatial autocorrelation of a data set, it gives a measure of similarity between data and its surrounding. The range of this value must be [-1,1], but this does not happen in practice. This package scale the Moran's I value and map it into the theoretical range of [-1,1]. Once the Moran's I value is rescaled, it facilitates the comparison between projects, for instance, a researcher can calculate Moran's I in a city in China, with a sample size of n1 and area of interest a1. Another researcher runs a similar experiment in a city in Mexico with different sample size, n2, and an area of interest a2. Due to the differences between the conditions, it is not possible to compare Moran's I in a straightforward way. In this version of the package, the spatial autocorrelation Moran's I is calculated as proposed in Chen(2013) <arXiv:1606.03658>."
"IMak",0,1,9,"2020-05-06 12:20","This is an Automatic Item Generator for Psychological Assessment. Items created with the 'IMak' package should not be used in applied settings as part of the working protocol without ensuring first that the items meet the required psychometric quality standards (see Blum & Holling, 2018) <doi:10.3389/fpsyg.2018.01286>."
"ForestTools",3,1,7,"2018-04-04 22:33","Provides tools for analyzing remotely sensed forest data, including functions for detecting treetops from canopy models (Popescu & Wynne, 2004), outlining tree crowns (Meyer & Beucher, 1990) and generating spatial statistics."
"fmriqa",1,1,3,"2017-12-20 16:21","Methods for performing fMRI quality assurance (QA) measurements of  test objects. Heavily  based on the fBIRN procedures detailed by Friedman and   Glover (2006) <doi:10.1002/jmri.20583>."
"epca",1,1,1,"2020-06-26","    Exploratory principal component analysis for large-scale dataset, including sparse principal component analysis and sparse matrix approximation."
"CropDetectR",1,1,1,"2019-09-20","A helpful tool for the identification of crop rows. Methods of this package include: Excess Green color scale <https://www.researchgate.net/publication/270613992_Color_Indices_for_Weed_Identification_Under_Various_Soil_Residue_and_Lighting_Conditions>, Otsu Thresholding <https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4310076>, and Morphology <https://en.wikipedia.org/wiki/Mathematical_morphology>."
"colocr",1,1,2,"2019-05-31 11:40","Automate the co-localization analysis of fluorescence microscopy   images. Selecting regions of interest, extract pixel intensities from   the image channels and calculate different co-localization statistics. The  methods implemented in this package are based on Dunn et al. (2011)   <doi:10.1152/ajpcell.00462.2010>."
"boundingbox",1,1,1,"2020-06-09","Generate ground truth cases for object localization algorithms.     Cycle through a list of images, select points around which to generate bounding     boxes and assign classifiers. Output the coordinates, and images annotated with     boxes and labels. For an example study that uses bounding boxes for image     localization and classification see Ibrahim, Badr, Abdallah, and Eissa (2012)    ""Bounding Box Object Localization Based on Image Superpixelization""    <doi:10.1016/j.procs.2012.09.119>."
"AcuityView",0,1,1,"2017-05-09","This code provides a simple method for representing a visual scene as it may be seen by an animal with less acute vision. When using (or for more information), please cite the original publication."
"ImaginR",0,1,1,"2017-05-31","The pearl oyster, Pinctada margaritifera (Linnaeus, 1758), represents the second economic resource of French Polynesia. It is one of the only bivalves expressing a large varied range of inner shell color, & by correlation, of pearl color. This phenotypic variability is partly under genetic control, but also under environmental influence. With ImaginR, it's now possible to delimit the color phenotype of the pearl oyster's inner shell and to characterize their color variations (by the HSV color code system) with pictures."
"rwavelet",1,1,2,"2018-09-12 17:10","Perform wavelet analysis (orthogonal, translation invariant, tensorial, 1-2-3d transforms, thresholding, block thresholding, linear,...) with applications to data compression or denoising/regression. The core of the code is a port of 'MATLAB' Wavelab toolbox written by D. Donoho, A. Maleki and M. Shahram (<https://statweb.stanford.edu/~wavelab/>)."
"ProFound",7,1,4,"2018-07-31 10:20","Core package containing all the tools for simple and advanced source extraction. This is used to create inputs for 'ProFit', or for source detection, extraction and photometry in its own right."
"pavo",4,2,21,"2019-12-11 16:30","A cohesive framework for parsing, analyzing and    organizing colour from spectral data."
"magicaxis",0,7,17,"2019-03-08 13:20","Functions to make useful (and pretty) plots for scientific plotting. Additional plotting features are added for base plotting, with particular emphasis on making attractive log axis plots."
"dynaSpec",0,1,1,"2020-06-22","A set of tools to generate dynamic spectrogram visualizations in video format."
"xRing",0,1,1,"2018-08-24","Contains functions to identify tree-ring borders based on X-ray micro-density profiles and a Graphical User Interface to visualize density profiles and correct tree-ring borders."
"TooManyCellsR",0,1,2,"2019-03-05 12:20","An R wrapper for using 'TooManyCells', a command line program for clustering, visualizing, and quantifying cell clade relationships. See <https://gregoryschwartz.github.io/too-many-cells/> for more details."
"SPUTNIK",0,1,13,"2018-10-23 13:50","A set of tools for the peak filtering of mass spectrometry  imaging data (MSI or IMS) based on spatial distribution of signal. Given a   region-of-interest (ROI), representing the spatial region where the informative  signal is expected to be localized, a series of filters determine which peak  signals are characterized by an implausible spatial distribution. The filters  reduce the dataset dimensionality and increase its information vs noise ratio,  improving the quality of the unsupervised analysis results, reducing data  dimensionality and simplifying the chemical interpretation."
"smpic",1,1,1,"2017-10-04","Creates images that are the proper size for social media. Beautiful    plots, charts and graphs wither and die if they are not shared. Social media     is perfect for this but every platform has its own image dimensions. With     'smpic' you can easily save your plots with the exact dimensions needed for     the different platforms."
"sketcher",0,1,1,"2020-05-25","An implementation of image processing effects that convert a photo into a line drawing image.     For details, please refer to Tsuda, H. (2020). sketcher: An R package for converting a photo into a sketch style image.     <doi:10.31234/osf.io/svmw5>."
"Rbgs",0,1,2,"2017-06-06 12:19","Methods that allow video reading and loading in R. Also provides nine different methods for  background subtraction."
"multifluo",0,1,2,"2018-01-24 10:29","Deals with several images of a same object, constituted of different zones. Each image constitutes a variable for a given pixel. The user can interactively select different zones of an image. Then, multivariate analysis (PCA) can be run in order to characterize the different selected zones, according to the different images. Hotelling (Hotelling, 1931, <doi:10.1214/aoms/1177732979>) and Srivastava (Srivastava, 2009, <doi:10.1016/j.jmva.2006.11.002>) tests can be run to detect multivariate differences between the zones. "
"MtreeRing",0,1,5,"2019-09-23 06:20","Use morphological image processing and edge detection algorithms to automatically measure tree ring widths on digital images. Users can also manually mark tree rings on species with complex anatomical structures. The arcs of inner-rings and angles of successive inclined ring boundaries are used to correct ring-width series. The package provides a Shiny-based application, allowing R beginners to easily analyze tree ring images and export ring-width series in standard file formats."
"mand",0,1,1,"2020-05-06","Several functions can be used to analyze neuroimaging data using multivariate methods based on the 'msma' package. For more details, please see Kawaguchi et al. (2017) <doi:10.1093/biostatistics/kxx011> and Kawaguchi (2019) <doi:10.5772/intechopen.80531>."
"Irescale",1,1,3,"2019-04-15 23:22","Provides a scaling method to obtain a standardized Moran's I measure. Moran's I is a measure for the spatial autocorrelation of a data set, it gives a measure of similarity between data and its surrounding. The range of this value must be [-1,1], but this does not happen in practice. This package scale the Moran's I value and map it into the theoretical range of [-1,1]. Once the Moran's I value is rescaled, it facilitates the comparison between projects, for instance, a researcher can calculate Moran's I in a city in China, with a sample size of n1 and area of interest a1. Another researcher runs a similar experiment in a city in Mexico with different sample size, n2, and an area of interest a2. Due to the differences between the conditions, it is not possible to compare Moran's I in a straightforward way. In this version of the package, the spatial autocorrelation Moran's I is calculated as proposed in Chen(2013) <arXiv:1606.03658>."
"IMak",0,1,9,"2020-05-06 12:20","This is an Automatic Item Generator for Psychological Assessment. Items created with the 'IMak' package should not be used in applied settings as part of the working protocol without ensuring first that the items meet the required psychometric quality standards (see Blum & Holling, 2018) <doi:10.3389/fpsyg.2018.01286>."
"ForestTools",3,1,7,"2018-04-04 22:33","Provides tools for analyzing remotely sensed forest data, including functions for detecting treetops from canopy models (Popescu & Wynne, 2004), outlining tree crowns (Meyer & Beucher, 1990) and generating spatial statistics."
"fmriqa",1,1,3,"2017-12-20 16:21","Methods for performing fMRI quality assurance (QA) measurements of  test objects. Heavily  based on the fBIRN procedures detailed by Friedman and   Glover (2006) <doi:10.1002/jmri.20583>."
"epca",1,1,1,"2020-06-26","    Exploratory principal component analysis for large-scale dataset, including sparse principal component analysis and sparse matrix approximation."
"CropDetectR",1,1,1,"2019-09-20","A helpful tool for the identification of crop rows. Methods of this package include: Excess Green color scale <https://www.researchgate.net/publication/270613992_Color_Indices_for_Weed_Identification_Under_Various_Soil_Residue_and_Lighting_Conditions>, Otsu Thresholding <https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4310076>, and Morphology <https://en.wikipedia.org/wiki/Mathematical_morphology>."
"colocr",1,1,2,"2019-05-31 11:40","Automate the co-localization analysis of fluorescence microscopy   images. Selecting regions of interest, extract pixel intensities from   the image channels and calculate different co-localization statistics. The  methods implemented in this package are based on Dunn et al. (2011)   <doi:10.1152/ajpcell.00462.2010>."
"boundingbox",1,1,1,"2020-06-09","Generate ground truth cases for object localization algorithms.     Cycle through a list of images, select points around which to generate bounding     boxes and assign classifiers. Output the coordinates, and images annotated with     boxes and labels. For an example study that uses bounding boxes for image     localization and classification see Ibrahim, Badr, Abdallah, and Eissa (2012)    ""Bounding Box Object Localization Based on Image Superpixelization""    <doi:10.1016/j.procs.2012.09.119>."
"AcuityView",0,1,1,"2017-05-09","This code provides a simple method for representing a visual scene as it may be seen by an animal with less acute vision. When using (or for more information), please cite the original publication."
"ImaginR",0,1,1,"2017-05-31","The pearl oyster, Pinctada margaritifera (Linnaeus, 1758), represents the second economic resource of French Polynesia. It is one of the only bivalves expressing a large varied range of inner shell color, & by correlation, of pearl color. This phenotypic variability is partly under genetic control, but also under environmental influence. With ImaginR, it's now possible to delimit the color phenotype of the pearl oyster's inner shell and to characterize their color variations (by the HSV color code system) with pictures."
"rwavelet",1,1,2,"2018-09-12 17:10","Perform wavelet analysis (orthogonal, translation invariant, tensorial, 1-2-3d transforms, thresholding, block thresholding, linear,...) with applications to data compression or denoising/regression. The core of the code is a port of 'MATLAB' Wavelab toolbox written by D. Donoho, A. Maleki and M. Shahram (<https://statweb.stanford.edu/~wavelab/>)."
"ProFound",7,1,4,"2018-07-31 10:20","Core package containing all the tools for simple and advanced source extraction. This is used to create inputs for 'ProFit', or for source detection, extraction and photometry in its own right."
"pavo",4,2,21,"2019-12-11 16:30","A cohesive framework for parsing, analyzing and    organizing colour from spectral data."
"magicaxis",0,7,17,"2019-03-08 13:20","Functions to make useful (and pretty) plots for scientific plotting. Additional plotting features are added for base plotting, with particular emphasis on making attractive log axis plots."
"dynaSpec",0,1,1,"2020-06-22","A set of tools to generate dynamic spectrogram visualizations in video format."
"xRing",0,1,1,"2018-08-24","Contains functions to identify tree-ring borders based on X-ray micro-density profiles and a Graphical User Interface to visualize density profiles and correct tree-ring borders."
"TooManyCellsR",0,1,2,"2019-03-05 12:20","An R wrapper for using 'TooManyCells', a command line program for clustering, visualizing, and quantifying cell clade relationships. See <https://gregoryschwartz.github.io/too-many-cells/> for more details."
"SPUTNIK",0,1,13,"2018-10-23 13:50","A set of tools for the peak filtering of mass spectrometry  imaging data (MSI or IMS) based on spatial distribution of signal. Given a   region-of-interest (ROI), representing the spatial region where the informative  signal is expected to be localized, a series of filters determine which peak  signals are characterized by an implausible spatial distribution. The filters  reduce the dataset dimensionality and increase its information vs noise ratio,  improving the quality of the unsupervised analysis results, reducing data  dimensionality and simplifying the chemical interpretation."
"smpic",1,1,1,"2017-10-04","Creates images that are the proper size for social media. Beautiful    plots, charts and graphs wither and die if they are not shared. Social media     is perfect for this but every platform has its own image dimensions. With     'smpic' you can easily save your plots with the exact dimensions needed for     the different platforms."
"sketcher",0,1,1,"2020-05-25","An implementation of image processing effects that convert a photo into a line drawing image.     For details, please refer to Tsuda, H. (2020). sketcher: An R package for converting a photo into a sketch style image.     <doi:10.31234/osf.io/svmw5>."
"Rbgs",0,1,2,"2017-06-06 12:19","Methods that allow video reading and loading in R. Also provides nine different methods for  background subtraction."
"multifluo",0,1,2,"2018-01-24 10:29","Deals with several images of a same object, constituted of different zones. Each image constitutes a variable for a given pixel. The user can interactively select different zones of an image. Then, multivariate analysis (PCA) can be run in order to characterize the different selected zones, according to the different images. Hotelling (Hotelling, 1931, <doi:10.1214/aoms/1177732979>) and Srivastava (Srivastava, 2009, <doi:10.1016/j.jmva.2006.11.002>) tests can be run to detect multivariate differences between the zones. "
"MtreeRing",0,1,5,"2019-09-23 06:20","Use morphological image processing and edge detection algorithms to automatically measure tree ring widths on digital images. Users can also manually mark tree rings on species with complex anatomical structures. The arcs of inner-rings and angles of successive inclined ring boundaries are used to correct ring-width series. The package provides a Shiny-based application, allowing R beginners to easily analyze tree ring images and export ring-width series in standard file formats."
"mand",0,1,1,"2020-05-06","Several functions can be used to analyze neuroimaging data using multivariate methods based on the 'msma' package. For more details, please see Kawaguchi et al. (2017) <doi:10.1093/biostatistics/kxx011> and Kawaguchi (2019) <doi:10.5772/intechopen.80531>."
"Irescale",1,1,3,"2019-04-15 23:22","Provides a scaling method to obtain a standardized Moran's I measure. Moran's I is a measure for the spatial autocorrelation of a data set, it gives a measure of similarity between data and its surrounding. The range of this value must be [-1,1], but this does not happen in practice. This package scale the Moran's I value and map it into the theoretical range of [-1,1]. Once the Moran's I value is rescaled, it facilitates the comparison between projects, for instance, a researcher can calculate Moran's I in a city in China, with a sample size of n1 and area of interest a1. Another researcher runs a similar experiment in a city in Mexico with different sample size, n2, and an area of interest a2. Due to the differences between the conditions, it is not possible to compare Moran's I in a straightforward way. In this version of the package, the spatial autocorrelation Moran's I is calculated as proposed in Chen(2013) <arXiv:1606.03658>."
"IMak",0,1,9,"2020-05-06 12:20","This is an Automatic Item Generator for Psychological Assessment. Items created with the 'IMak' package should not be used in applied settings as part of the working protocol without ensuring first that the items meet the required psychometric quality standards (see Blum & Holling, 2018) <doi:10.3389/fpsyg.2018.01286>."
"ForestTools",3,1,7,"2018-04-04 22:33","Provides tools for analyzing remotely sensed forest data, including functions for detecting treetops from canopy models (Popescu & Wynne, 2004), outlining tree crowns (Meyer & Beucher, 1990) and generating spatial statistics."
"fmriqa",1,1,3,"2017-12-20 16:21","Methods for performing fMRI quality assurance (QA) measurements of  test objects. Heavily  based on the fBIRN procedures detailed by Friedman and   Glover (2006) <doi:10.1002/jmri.20583>."
"epca",1,1,1,"2020-06-26","    Exploratory principal component analysis for large-scale dataset, including sparse principal component analysis and sparse matrix approximation."
"CropDetectR",1,1,1,"2019-09-20","A helpful tool for the identification of crop rows. Methods of this package include: Excess Green color scale <https://www.researchgate.net/publication/270613992_Color_Indices_for_Weed_Identification_Under_Various_Soil_Residue_and_Lighting_Conditions>, Otsu Thresholding <https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4310076>, and Morphology <https://en.wikipedia.org/wiki/Mathematical_morphology>."
"colocr",1,1,2,"2019-05-31 11:40","Automate the co-localization analysis of fluorescence microscopy   images. Selecting regions of interest, extract pixel intensities from   the image channels and calculate different co-localization statistics. The  methods implemented in this package are based on Dunn et al. (2011)   <doi:10.1152/ajpcell.00462.2010>."
"boundingbox",1,1,1,"2020-06-09","Generate ground truth cases for object localization algorithms.     Cycle through a list of images, select points around which to generate bounding     boxes and assign classifiers. Output the coordinates, and images annotated with     boxes and labels. For an example study that uses bounding boxes for image     localization and classification see Ibrahim, Badr, Abdallah, and Eissa (2012)    ""Bounding Box Object Localization Based on Image Superpixelization""    <doi:10.1016/j.procs.2012.09.119>."
"AcuityView",0,1,1,"2017-05-09","This code provides a simple method for representing a visual scene as it may be seen by an animal with less acute vision. When using (or for more information), please cite the original publication."
"ImaginR",0,1,1,"2017-05-31","The pearl oyster, Pinctada margaritifera (Linnaeus, 1758), represents the second economic resource of French Polynesia. It is one of the only bivalves expressing a large varied range of inner shell color, & by correlation, of pearl color. This phenotypic variability is partly under genetic control, but also under environmental influence. With ImaginR, it's now possible to delimit the color phenotype of the pearl oyster's inner shell and to characterize their color variations (by the HSV color code system) with pictures."
"rwavelet",1,1,2,"2018-09-12 17:10","Perform wavelet analysis (orthogonal, translation invariant, tensorial, 1-2-3d transforms, thresholding, block thresholding, linear,...) with applications to data compression or denoising/regression. The core of the code is a port of 'MATLAB' Wavelab toolbox written by D. Donoho, A. Maleki and M. Shahram (<https://statweb.stanford.edu/~wavelab/>)."
"ProFound",7,1,4,"2018-07-31 10:20","Core package containing all the tools for simple and advanced source extraction. This is used to create inputs for 'ProFit', or for source detection, extraction and photometry in its own right."
"pavo",4,2,21,"2019-12-11 16:30","A cohesive framework for parsing, analyzing and    organizing colour from spectral data."
"magicaxis",0,7,17,"2019-03-08 13:20","Functions to make useful (and pretty) plots for scientific plotting. Additional plotting features are added for base plotting, with particular emphasis on making attractive log axis plots."
"dynaSpec",0,1,1,"2020-06-22","A set of tools to generate dynamic spectrogram visualizations in video format."
"xRing",0,1,1,"2018-08-24","Contains functions to identify tree-ring borders based on X-ray micro-density profiles and a Graphical User Interface to visualize density profiles and correct tree-ring borders."
"TooManyCellsR",0,1,2,"2019-03-05 12:20","An R wrapper for using 'TooManyCells', a command line program for clustering, visualizing, and quantifying cell clade relationships. See <https://gregoryschwartz.github.io/too-many-cells/> for more details."
"SPUTNIK",0,1,13,"2018-10-23 13:50","A set of tools for the peak filtering of mass spectrometry  imaging data (MSI or IMS) based on spatial distribution of signal. Given a   region-of-interest (ROI), representing the spatial region where the informative  signal is expected to be localized, a series of filters determine which peak  signals are characterized by an implausible spatial distribution. The filters  reduce the dataset dimensionality and increase its information vs noise ratio,  improving the quality of the unsupervised analysis results, reducing data  dimensionality and simplifying the chemical interpretation."
"smpic",1,1,1,"2017-10-04","Creates images that are the proper size for social media. Beautiful    plots, charts and graphs wither and die if they are not shared. Social media     is perfect for this but every platform has its own image dimensions. With     'smpic' you can easily save your plots with the exact dimensions needed for     the different platforms."
"sketcher",0,1,1,"2020-05-25","An implementation of image processing effects that convert a photo into a line drawing image.     For details, please refer to Tsuda, H. (2020). sketcher: An R package for converting a photo into a sketch style image.     <doi:10.31234/osf.io/svmw5>."
"Rbgs",0,1,2,"2017-06-06 12:19","Methods that allow video reading and loading in R. Also provides nine different methods for  background subtraction."
"multifluo",0,1,2,"2018-01-24 10:29","Deals with several images of a same object, constituted of different zones. Each image constitutes a variable for a given pixel. The user can interactively select different zones of an image. Then, multivariate analysis (PCA) can be run in order to characterize the different selected zones, according to the different images. Hotelling (Hotelling, 1931, <doi:10.1214/aoms/1177732979>) and Srivastava (Srivastava, 2009, <doi:10.1016/j.jmva.2006.11.002>) tests can be run to detect multivariate differences between the zones. "
"MtreeRing",0,1,5,"2019-09-23 06:20","Use morphological image processing and edge detection algorithms to automatically measure tree ring widths on digital images. Users can also manually mark tree rings on species with complex anatomical structures. The arcs of inner-rings and angles of successive inclined ring boundaries are used to correct ring-width series. The package provides a Shiny-based application, allowing R beginners to easily analyze tree ring images and export ring-width series in standard file formats."
"mand",0,1,1,"2020-05-06","Several functions can be used to analyze neuroimaging data using multivariate methods based on the 'msma' package. For more details, please see Kawaguchi et al. (2017) <doi:10.1093/biostatistics/kxx011> and Kawaguchi (2019) <doi:10.5772/intechopen.80531>."
"Irescale",1,1,3,"2019-04-15 23:22","Provides a scaling method to obtain a standardized Moran's I measure. Moran's I is a measure for the spatial autocorrelation of a data set, it gives a measure of similarity between data and its surrounding. The range of this value must be [-1,1], but this does not happen in practice. This package scale the Moran's I value and map it into the theoretical range of [-1,1]. Once the Moran's I value is rescaled, it facilitates the comparison between projects, for instance, a researcher can calculate Moran's I in a city in China, with a sample size of n1 and area of interest a1. Another researcher runs a similar experiment in a city in Mexico with different sample size, n2, and an area of interest a2. Due to the differences between the conditions, it is not possible to compare Moran's I in a straightforward way. In this version of the package, the spatial autocorrelation Moran's I is calculated as proposed in Chen(2013) <arXiv:1606.03658>."
"IMak",0,1,9,"2020-05-06 12:20","This is an Automatic Item Generator for Psychological Assessment. Items created with the 'IMak' package should not be used in applied settings as part of the working protocol without ensuring first that the items meet the required psychometric quality standards (see Blum & Holling, 2018) <doi:10.3389/fpsyg.2018.01286>."
"ForestTools",3,1,7,"2018-04-04 22:33","Provides tools for analyzing remotely sensed forest data, including functions for detecting treetops from canopy models (Popescu & Wynne, 2004), outlining tree crowns (Meyer & Beucher, 1990) and generating spatial statistics."
"fmriqa",1,1,3,"2017-12-20 16:21","Methods for performing fMRI quality assurance (QA) measurements of  test objects. Heavily  based on the fBIRN procedures detailed by Friedman and   Glover (2006) <doi:10.1002/jmri.20583>."
"epca",1,1,1,"2020-06-26","    Exploratory principal component analysis for large-scale dataset, including sparse principal component analysis and sparse matrix approximation."
"CropDetectR",1,1,1,"2019-09-20","A helpful tool for the identification of crop rows. Methods of this package include: Excess Green color scale <https://www.researchgate.net/publication/270613992_Color_Indices_for_Weed_Identification_Under_Various_Soil_Residue_and_Lighting_Conditions>, Otsu Thresholding <https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4310076>, and Morphology <https://en.wikipedia.org/wiki/Mathematical_morphology>."
"colocr",1,1,2,"2019-05-31 11:40","Automate the co-localization analysis of fluorescence microscopy   images. Selecting regions of interest, extract pixel intensities from   the image channels and calculate different co-localization statistics. The  methods implemented in this package are based on Dunn et al. (2011)   <doi:10.1152/ajpcell.00462.2010>."
"boundingbox",1,1,1,"2020-06-09","Generate ground truth cases for object localization algorithms.     Cycle through a list of images, select points around which to generate bounding     boxes and assign classifiers. Output the coordinates, and images annotated with     boxes and labels. For an example study that uses bounding boxes for image     localization and classification see Ibrahim, Badr, Abdallah, and Eissa (2012)    ""Bounding Box Object Localization Based on Image Superpixelization""    <doi:10.1016/j.procs.2012.09.119>."
"AcuityView",0,1,1,"2017-05-09","This code provides a simple method for representing a visual scene as it may be seen by an animal with less acute vision. When using (or for more information), please cite the original publication."
"ImaginR",0,1,1,"2017-05-31","The pearl oyster, Pinctada margaritifera (Linnaeus, 1758), represents the second economic resource of French Polynesia. It is one of the only bivalves expressing a large varied range of inner shell color, & by correlation, of pearl color. This phenotypic variability is partly under genetic control, but also under environmental influence. With ImaginR, it's now possible to delimit the color phenotype of the pearl oyster's inner shell and to characterize their color variations (by the HSV color code system) with pictures."
"pulsar",2,1,8,"2019-08-23 00:20","Model selection for penalized graphical models using the Stability Approach to Regularization Selection ('StARS'), with options for speed-ups including Bounded StARS (B-StARS), batch computing, and other stability metrics (e.g., graphlet stability G-StARS). Christian L. Müller, Richard Bonneau, Zachary Kurtz (2016) <arXiv:1605.07072>."
"LICORS",0,3,4,"2012-11-08 07:05","Estimates predictive states from spatio-temporal data and    consequently can provide provably optimal forecasts.    Currently this implementation    supports an N-dimensional spatial grid observed over equally spaced time    intervals. E.g. a video is a 2D spatial systems observed over time. This    package implements mixed LICORS, has plotting tools (for (1+1)D and (2+1)D    systems), and methods for optimal forecasting.  Due to memory limitations    it is recommend to only analyze (1+1)D systems."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"UNPaC",0,1,2,"2019-07-02 18:30","Assess the significance of identified clusters and estimates the true number of clusters by comparing the explained variation due to the clustering from the original data to that produced by clustering a unimodal reference distribution which preserves the covariance structure in the data. The reference distribution is generated using kernel density estimation and a Gaussian copula framework. A dimension reduction strategy and sparse covariance estimation optimize this method for the high-dimensional, low-sample size setting. This method is similar to them method described in Helgeson and Bair (2016) <arXiv:1610.01424> except a Gaussian copula approach is used to account for feature correlation."
"sdafilter",0,1,1,"2020-03-19","We develop a new class of distribution free multiple testing rules for false discovery rate (FDR) control under general dependence. A key element in our proposal is a symmetrized data aggregation (SDA) approach to incorporating the dependence structure via sample splitting, data screening and information pooling. The proposed SDA filter first constructs a sequence of ranking statistics that fulfill global symmetry properties, and then chooses a data driven threshold along the ranking to control the FDR. For more information, see the website below and the accompanying paper: Du et al. (2020), ""False Discovery Rate Control Under General Dependence By Symmetrized Data Aggregation"", <arXiv:2002.11992>."
"IROmiss",0,1,3,"2018-02-19 19:15","Missing data are frequently encountered in high-dimensional data analysis, but they are usually difficult to deal with using standard algorithms, such as the EM algorithm and its variants. This package provides a general algorithm, the so-called Imputation Regularized Optimization (IRO) algorithm, for high-dimensional missing data problems. You can refer to Liang, F., Jia, B., Xue, J., Li, Q. and Luo, Y. (2018) at <arXiv:1802.02251> for detail."
"glmaag",1,1,3,"2019-04-14 01:32","Efficient procedures for adaptive LASSO and network regularized for Gaussian, logistic, and Cox model. Provides network estimation procedure (combination of methods proposed by Ucar, et. al (2007) <doi:10.1093/bioinformatics/btm423> and Meinshausen and Buhlmann (2006) <doi:10.1214/009053606000000281>), cross validation and stability selection proposed by Meinshausen and Buhlmann (2010) <doi:10.1111/j.1467-9868.2010.00740.x> and Liu, Roeder and Wasserman (2010) <arXiv:1006.3316> methods. Interactive R app is available."
"GGMM",0,1,2,"2018-05-09 10:36","The Gaussian graphical model is a widely used tool for learning gene regulatory networks with high-dimensional gene expression data. For many real problems, the data are heterogeneous, which may contain some subgroups or come from different resources. This package provide a Gaussian Graphical Mixture Model (GGMM) for the heterogeneous data. You can refer to Jia, B. and Liang, F. (2018) at <arXiv:1805.02547> for detail."
"pulsar",2,1,8,"2019-08-23 00:20","Model selection for penalized graphical models using the Stability Approach to Regularization Selection ('StARS'), with options for speed-ups including Bounded StARS (B-StARS), batch computing, and other stability metrics (e.g., graphlet stability G-StARS). Christian L. Müller, Richard Bonneau, Zachary Kurtz (2016) <arXiv:1605.07072>."
"LICORS",0,3,4,"2012-11-08 07:05","Estimates predictive states from spatio-temporal data and    consequently can provide provably optimal forecasts.    Currently this implementation    supports an N-dimensional spatial grid observed over equally spaced time    intervals. E.g. a video is a 2D spatial systems observed over time. This    package implements mixed LICORS, has plotting tools (for (1+1)D and (2+1)D    systems), and methods for optimal forecasting.  Due to memory limitations    it is recommend to only analyze (1+1)D systems."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"UNPaC",0,1,2,"2019-07-02 18:30","Assess the significance of identified clusters and estimates the true number of clusters by comparing the explained variation due to the clustering from the original data to that produced by clustering a unimodal reference distribution which preserves the covariance structure in the data. The reference distribution is generated using kernel density estimation and a Gaussian copula framework. A dimension reduction strategy and sparse covariance estimation optimize this method for the high-dimensional, low-sample size setting. This method is similar to them method described in Helgeson and Bair (2016) <arXiv:1610.01424> except a Gaussian copula approach is used to account for feature correlation."
"sdafilter",0,1,1,"2020-03-19","We develop a new class of distribution free multiple testing rules for false discovery rate (FDR) control under general dependence. A key element in our proposal is a symmetrized data aggregation (SDA) approach to incorporating the dependence structure via sample splitting, data screening and information pooling. The proposed SDA filter first constructs a sequence of ranking statistics that fulfill global symmetry properties, and then chooses a data driven threshold along the ranking to control the FDR. For more information, see the website below and the accompanying paper: Du et al. (2020), ""False Discovery Rate Control Under General Dependence By Symmetrized Data Aggregation"", <arXiv:2002.11992>."
"IROmiss",0,1,3,"2018-02-19 19:15","Missing data are frequently encountered in high-dimensional data analysis, but they are usually difficult to deal with using standard algorithms, such as the EM algorithm and its variants. This package provides a general algorithm, the so-called Imputation Regularized Optimization (IRO) algorithm, for high-dimensional missing data problems. You can refer to Liang, F., Jia, B., Xue, J., Li, Q. and Luo, Y. (2018) at <arXiv:1802.02251> for detail."
"glmaag",1,1,3,"2019-04-14 01:32","Efficient procedures for adaptive LASSO and network regularized for Gaussian, logistic, and Cox model. Provides network estimation procedure (combination of methods proposed by Ucar, et. al (2007) <doi:10.1093/bioinformatics/btm423> and Meinshausen and Buhlmann (2006) <doi:10.1214/009053606000000281>), cross validation and stability selection proposed by Meinshausen and Buhlmann (2010) <doi:10.1111/j.1467-9868.2010.00740.x> and Liu, Roeder and Wasserman (2010) <arXiv:1006.3316> methods. Interactive R app is available."
"GGMM",0,1,2,"2018-05-09 10:36","The Gaussian graphical model is a widely used tool for learning gene regulatory networks with high-dimensional gene expression data. For many real problems, the data are heterogeneous, which may contain some subgroups or come from different resources. This package provide a Gaussian Graphical Mixture Model (GGMM) for the heterogeneous data. You can refer to Jia, B. and Liang, F. (2018) at <arXiv:1805.02547> for detail."
"pulsar",2,1,8,"2019-08-23 00:20","Model selection for penalized graphical models using the Stability Approach to Regularization Selection ('StARS'), with options for speed-ups including Bounded StARS (B-StARS), batch computing, and other stability metrics (e.g., graphlet stability G-StARS). Christian L. Müller, Richard Bonneau, Zachary Kurtz (2016) <arXiv:1605.07072>."
"LICORS",0,3,4,"2012-11-08 07:05","Estimates predictive states from spatio-temporal data and    consequently can provide provably optimal forecasts.    Currently this implementation    supports an N-dimensional spatial grid observed over equally spaced time    intervals. E.g. a video is a 2D spatial systems observed over time. This    package implements mixed LICORS, has plotting tools (for (1+1)D and (2+1)D    systems), and methods for optimal forecasting.  Due to memory limitations    it is recommend to only analyze (1+1)D systems."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"UNPaC",0,1,2,"2019-07-02 18:30","Assess the significance of identified clusters and estimates the true number of clusters by comparing the explained variation due to the clustering from the original data to that produced by clustering a unimodal reference distribution which preserves the covariance structure in the data. The reference distribution is generated using kernel density estimation and a Gaussian copula framework. A dimension reduction strategy and sparse covariance estimation optimize this method for the high-dimensional, low-sample size setting. This method is similar to them method described in Helgeson and Bair (2016) <arXiv:1610.01424> except a Gaussian copula approach is used to account for feature correlation."
"sdafilter",0,1,1,"2020-03-19","We develop a new class of distribution free multiple testing rules for false discovery rate (FDR) control under general dependence. A key element in our proposal is a symmetrized data aggregation (SDA) approach to incorporating the dependence structure via sample splitting, data screening and information pooling. The proposed SDA filter first constructs a sequence of ranking statistics that fulfill global symmetry properties, and then chooses a data driven threshold along the ranking to control the FDR. For more information, see the website below and the accompanying paper: Du et al. (2020), ""False Discovery Rate Control Under General Dependence By Symmetrized Data Aggregation"", <arXiv:2002.11992>."
"IROmiss",0,1,3,"2018-02-19 19:15","Missing data are frequently encountered in high-dimensional data analysis, but they are usually difficult to deal with using standard algorithms, such as the EM algorithm and its variants. This package provides a general algorithm, the so-called Imputation Regularized Optimization (IRO) algorithm, for high-dimensional missing data problems. You can refer to Liang, F., Jia, B., Xue, J., Li, Q. and Luo, Y. (2018) at <arXiv:1802.02251> for detail."
"glmaag",1,1,3,"2019-04-14 01:32","Efficient procedures for adaptive LASSO and network regularized for Gaussian, logistic, and Cox model. Provides network estimation procedure (combination of methods proposed by Ucar, et. al (2007) <doi:10.1093/bioinformatics/btm423> and Meinshausen and Buhlmann (2006) <doi:10.1214/009053606000000281>), cross validation and stability selection proposed by Meinshausen and Buhlmann (2010) <doi:10.1111/j.1467-9868.2010.00740.x> and Liu, Roeder and Wasserman (2010) <arXiv:1006.3316> methods. Interactive R app is available."
"GGMM",0,1,2,"2018-05-09 10:36","The Gaussian graphical model is a widely used tool for learning gene regulatory networks with high-dimensional gene expression data. For many real problems, the data are heterogeneous, which may contain some subgroups or come from different resources. This package provide a Gaussian Graphical Mixture Model (GGMM) for the heterogeneous data. You can refer to Jia, B. and Liang, F. (2018) at <arXiv:1805.02547> for detail."
"pulsar",2,1,8,"2019-08-23 00:20","Model selection for penalized graphical models using the Stability Approach to Regularization Selection ('StARS'), with options for speed-ups including Bounded StARS (B-StARS), batch computing, and other stability metrics (e.g., graphlet stability G-StARS). Christian L. Müller, Richard Bonneau, Zachary Kurtz (2016) <arXiv:1605.07072>."
"LICORS",0,3,4,"2012-11-08 07:05","Estimates predictive states from spatio-temporal data and    consequently can provide provably optimal forecasts.    Currently this implementation    supports an N-dimensional spatial grid observed over equally spaced time    intervals. E.g. a video is a 2D spatial systems observed over time. This    package implements mixed LICORS, has plotting tools (for (1+1)D and (2+1)D    systems), and methods for optimal forecasting.  Due to memory limitations    it is recommend to only analyze (1+1)D systems."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"UNPaC",0,1,2,"2019-07-02 18:30","Assess the significance of identified clusters and estimates the true number of clusters by comparing the explained variation due to the clustering from the original data to that produced by clustering a unimodal reference distribution which preserves the covariance structure in the data. The reference distribution is generated using kernel density estimation and a Gaussian copula framework. A dimension reduction strategy and sparse covariance estimation optimize this method for the high-dimensional, low-sample size setting. This method is similar to them method described in Helgeson and Bair (2016) <arXiv:1610.01424> except a Gaussian copula approach is used to account for feature correlation."
"sdafilter",0,1,1,"2020-03-19","We develop a new class of distribution free multiple testing rules for false discovery rate (FDR) control under general dependence. A key element in our proposal is a symmetrized data aggregation (SDA) approach to incorporating the dependence structure via sample splitting, data screening and information pooling. The proposed SDA filter first constructs a sequence of ranking statistics that fulfill global symmetry properties, and then chooses a data driven threshold along the ranking to control the FDR. For more information, see the website below and the accompanying paper: Du et al. (2020), ""False Discovery Rate Control Under General Dependence By Symmetrized Data Aggregation"", <arXiv:2002.11992>."
"IROmiss",0,1,3,"2018-02-19 19:15","Missing data are frequently encountered in high-dimensional data analysis, but they are usually difficult to deal with using standard algorithms, such as the EM algorithm and its variants. This package provides a general algorithm, the so-called Imputation Regularized Optimization (IRO) algorithm, for high-dimensional missing data problems. You can refer to Liang, F., Jia, B., Xue, J., Li, Q. and Luo, Y. (2018) at <arXiv:1802.02251> for detail."
"glmaag",1,1,3,"2019-04-14 01:32","Efficient procedures for adaptive LASSO and network regularized for Gaussian, logistic, and Cox model. Provides network estimation procedure (combination of methods proposed by Ucar, et. al (2007) <doi:10.1093/bioinformatics/btm423> and Meinshausen and Buhlmann (2006) <doi:10.1214/009053606000000281>), cross validation and stability selection proposed by Meinshausen and Buhlmann (2010) <doi:10.1111/j.1467-9868.2010.00740.x> and Liu, Roeder and Wasserman (2010) <arXiv:1006.3316> methods. Interactive R app is available."
"GGMM",0,1,2,"2018-05-09 10:36","The Gaussian graphical model is a widely used tool for learning gene regulatory networks with high-dimensional gene expression data. For many real problems, the data are heterogeneous, which may contain some subgroups or come from different resources. This package provide a Gaussian Graphical Mixture Model (GGMM) for the heterogeneous data. You can refer to Jia, B. and Liang, F. (2018) at <arXiv:1805.02547> for detail."
"xplorerr",0,4,2,"2019-01-09 18:10","Tools for interactive data exploration built using 'shiny'. Includes apps for descriptive     statistics, visualizing probability distributions, inferential statistics, linear regression,     logistic regression and RFM analysis."
"shinyjqui",3,11,6,"2018-07-25 15:10","An extension to shiny that brings interactions and animation effects from    'jQuery UI' library."
"vistime",1,1,12,"2020-04-17 22:20","A library for creating time based charts, like Gantt or timelines. Possible outputs   include 'ggplot' diagrams, 'Plotly' graphs, 'Highchart' widgets and 'data.frames'. Results can be  used in the 'RStudio' viewer pane, in 'RMarkdown' documents or in 'Shiny' apps. In the   interactive outputs created by 'Plotly.js' and 'Highcharts.js', you can interact with the   plot using mouse hover or zoom."
"RKorAPClient",0,1,3,"2020-03-17 13:50","A client package that makes the 'KorAP' web service API accessible from R.  The corpus analysis platform 'KorAP' has been developed as a scientific tool to make  potentially large, stratified and multiply annotated corpora, such as the 'German Reference Corpus DeReKo'  or the 'Corpus of the Contemporary Romanian Language CoRoLa', accessible for linguists to let them verify  hypotheses and to find interesting patterns in real language use.  The 'RKorAPClient' package provides access to 'KorAP' and the corpora behind it for user-created R code,  as a programmatic alternative to the 'KorAP' web user-interface.  You can learn more about 'KorAP' and use it directly on 'DeReKo' at <https://korap.ids-mannheim.de/>."
"xplorerr",0,4,2,"2019-01-09 18:10","Tools for interactive data exploration built using 'shiny'. Includes apps for descriptive     statistics, visualizing probability distributions, inferential statistics, linear regression,     logistic regression and RFM analysis."
"shinyjqui",3,11,6,"2018-07-25 15:10","An extension to shiny that brings interactions and animation effects from    'jQuery UI' library."
"vistime",1,1,12,"2020-04-17 22:20","A library for creating time based charts, like Gantt or timelines. Possible outputs   include 'ggplot' diagrams, 'Plotly' graphs, 'Highchart' widgets and 'data.frames'. Results can be  used in the 'RStudio' viewer pane, in 'RMarkdown' documents or in 'Shiny' apps. In the   interactive outputs created by 'Plotly.js' and 'Highcharts.js', you can interact with the   plot using mouse hover or zoom."
"RKorAPClient",0,1,3,"2020-03-17 13:50","A client package that makes the 'KorAP' web service API accessible from R.  The corpus analysis platform 'KorAP' has been developed as a scientific tool to make  potentially large, stratified and multiply annotated corpora, such as the 'German Reference Corpus DeReKo'  or the 'Corpus of the Contemporary Romanian Language CoRoLa', accessible for linguists to let them verify  hypotheses and to find interesting patterns in real language use.  The 'RKorAPClient' package provides access to 'KorAP' and the corpora behind it for user-created R code,  as a programmatic alternative to the 'KorAP' web user-interface.  You can learn more about 'KorAP' and use it directly on 'DeReKo' at <https://korap.ids-mannheim.de/>."
"xplorerr",0,4,2,"2019-01-09 18:10","Tools for interactive data exploration built using 'shiny'. Includes apps for descriptive     statistics, visualizing probability distributions, inferential statistics, linear regression,     logistic regression and RFM analysis."
"shinyjqui",3,11,6,"2018-07-25 15:10","An extension to shiny that brings interactions and animation effects from    'jQuery UI' library."
"vistime",1,1,12,"2020-04-17 22:20","A library for creating time based charts, like Gantt or timelines. Possible outputs   include 'ggplot' diagrams, 'Plotly' graphs, 'Highchart' widgets and 'data.frames'. Results can be  used in the 'RStudio' viewer pane, in 'RMarkdown' documents or in 'Shiny' apps. In the   interactive outputs created by 'Plotly.js' and 'Highcharts.js', you can interact with the   plot using mouse hover or zoom."
"RKorAPClient",0,1,3,"2020-03-17 13:50","A client package that makes the 'KorAP' web service API accessible from R.  The corpus analysis platform 'KorAP' has been developed as a scientific tool to make  potentially large, stratified and multiply annotated corpora, such as the 'German Reference Corpus DeReKo'  or the 'Corpus of the Contemporary Romanian Language CoRoLa', accessible for linguists to let them verify  hypotheses and to find interesting patterns in real language use.  The 'RKorAPClient' package provides access to 'KorAP' and the corpora behind it for user-created R code,  as a programmatic alternative to the 'KorAP' web user-interface.  You can learn more about 'KorAP' and use it directly on 'DeReKo' at <https://korap.ids-mannheim.de/>."
"bnclassify",3,1,11,"2020-01-20 20:20","State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection."
"topologyGSA",0,2,13,"2015-07-09 18:13","Using Gaussian graphical models we propose a novel approach to             perform pathway analysis using gene expression. Given the             structure of a graph (a pathway) we introduce two statistical             tests to compare the mean and the concentration matrices between             two groups. Specifically, these tests can be performed on the             graph and on its connected components (cliques)."
"gRc",0,1,12,"2016-12-01 11:36","Estimation, model selection and other aspects of    statistical inference in Graphical Gaussian models with edge and    vertex symmetries (Graphical Gaussian models with colours).    Documentation about 'gRc' is provided in the paper by Hojsgaard and    Lauritzen (2007, <doi:10.18637/jss.v023.i06>) and the paper by    Hojsgaard and Lauritzen (2008, <doi:10.1111/j.1467-9868.2008.00666.x>)."
"bnclassify",3,1,11,"2020-01-20 20:20","State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection."
"topologyGSA",0,2,13,"2015-07-09 18:13","Using Gaussian graphical models we propose a novel approach to             perform pathway analysis using gene expression. Given the             structure of a graph (a pathway) we introduce two statistical             tests to compare the mean and the concentration matrices between             two groups. Specifically, these tests can be performed on the             graph and on its connected components (cliques)."
"gRc",0,1,12,"2016-12-01 11:36","Estimation, model selection and other aspects of    statistical inference in Graphical Gaussian models with edge and    vertex symmetries (Graphical Gaussian models with colours).    Documentation about 'gRc' is provided in the paper by Hojsgaard and    Lauritzen (2007, <doi:10.18637/jss.v023.i06>) and the paper by    Hojsgaard and Lauritzen (2008, <doi:10.1111/j.1467-9868.2008.00666.x>)."
"bnclassify",3,1,11,"2020-01-20 20:20","State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection."
"topologyGSA",0,2,13,"2015-07-09 18:13","Using Gaussian graphical models we propose a novel approach to             perform pathway analysis using gene expression. Given the             structure of a graph (a pathway) we introduce two statistical             tests to compare the mean and the concentration matrices between             two groups. Specifically, these tests can be performed on the             graph and on its connected components (cliques)."
"gRc",0,1,12,"2016-12-01 11:36","Estimation, model selection and other aspects of    statistical inference in Graphical Gaussian models with edge and    vertex symmetries (Graphical Gaussian models with colours).    Documentation about 'gRc' is provided in the paper by Hojsgaard and    Lauritzen (2007, <doi:10.18637/jss.v023.i06>) and the paper by    Hojsgaard and Lauritzen (2008, <doi:10.1111/j.1467-9868.2008.00666.x>)."
"bnclassify",3,1,11,"2020-01-20 20:20","State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection."
"topologyGSA",0,2,13,"2015-07-09 18:13","Using Gaussian graphical models we propose a novel approach to             perform pathway analysis using gene expression. Given the             structure of a graph (a pathway) we introduce two statistical             tests to compare the mean and the concentration matrices between             two groups. Specifically, these tests can be performed on the             graph and on its connected components (cliques)."
"gRc",0,1,12,"2016-12-01 11:36","Estimation, model selection and other aspects of    statistical inference in Graphical Gaussian models with edge and    vertex symmetries (Graphical Gaussian models with colours).    Documentation about 'gRc' is provided in the paper by Hojsgaard and    Lauritzen (2007, <doi:10.18637/jss.v023.i06>) and the paper by    Hojsgaard and Lauritzen (2008, <doi:10.1111/j.1467-9868.2008.00666.x>)."
"psychonetrics",0,1,10,"2020-04-01 23:00","Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search."
"mlVAR",0,1,10,"2017-09-02 18:22","Estimates the multi-level vector autoregression model on time-series data.             Three network structures are obtained: temporal networks, contemporaneous             networks and between-subjects networks."
"bnclassify",3,1,11,"2020-01-20 20:20","State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection."
"bnclassify",3,1,11,"2020-01-20 20:20","State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection."
"bnclassify",3,1,11,"2020-01-20 20:20","State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection."
"bnclassify",3,1,11,"2020-01-20 20:20","State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection."
"BasketballAnalyzeR",0,1,1,"2020-06-26","Contains data and code to accompany  the book              P. Zuccolotto and M. Manisera (2020) Basketball Data Science. Applications with R. CRC Press. ISBN 9781138600799."
"TGS",1,1,2,"2018-11-16 17:10","Rapid advancements in high-throughput gene sequencing    technologies have resulted in genome-scale time-series datasets.     Uncovering the underlying temporal sequence of gene regulatory events     in the form of time-varying gene regulatory networks demands     accurate and computationally efficient algorithms. Such an    algorithm is 'TGS'. It is proposed in Saptarshi Pyne, Alok Ranjan     Kumar, and Ashish Anand. Rapid reconstruction of time-varying     gene regulatory networks. IEEE/ACM Transactions on Computational     Biology and Bioinformatics, 17(1):278{291, Jan-Feb 2020. The TGS     algorithm is shown to consume only 29 minutes for a microarray     dataset with 4028 genes. This package provides an implementation     of the TGS algorithm and its variants."
"pcgen",0,1,2,"2018-08-02 23:20","Implements the pcgen algorithm, which is a modified version of the standard pc-algorithm,             with specific conditional independence tests and modified orientation rules. pcgen extends 			 the approach of Valente et al. (2010) <doi:10.1534/genetics.109.112979> with reconstruction of 			 direct genetic effects."
"osmplotr",3,1,9,"2017-06-30 15:08","Bespoke images of 'OpenStreetMap' ('OSM') data and data    visualisation using 'OSM' objects."
"MoTBFs",0,1,5,"2020-04-06 12:12","Learning, manipulation and  evaluation of mixtures of  truncated basis  functions   (MoTBFs),  which include mixtures of  polynomials (MOPs) and  mixtures of truncated   exponentials (MTEs). MoTBFs are a flexible framework for modelling hybrid Bayesian  networks (I. Pérez-Bernabé, A. Salmerón, H. Langseth (2015) <doi:10.1007/978-3-319-20807-7_36>; H. Langseth, T.D. Nielsen, I. Pérez-Bernabé, A. Salmerón (2014) <doi:10.1016/j.ijar.2013.09.012>; I. Pérez-Bernabé, A. Fernández, R. Rumí, A. Salmerón (2016) <doi:10.1007/s10618-015-0429-7>). The  package provides  functionality for learning  univariate, multivariate and  conditional  densities, with the  possibility of incorporating prior  knowledge. Structural  learning of hybrid Bayesian  networks is also provided. A set of useful tools is provided,  including  plotting, printing  and likelihood  evaluation. This  package  makes use of  S3   objects, with two new classes called 'motbf' and 'jointmotbf'."
"dynamicGraph",0,1,14,"2007-10-03 12:52","Interactive graphical tool for manipulating graphs"
"mvGPS",1,1,1,"2020-09-17","Methods for estimating weights and generalized propensity score for multiple continuous exposures via the generalized propensity score described in Williams, J.R, and Cresi, C.M (2020) <arxiv:2008.13767>. Weights are constructed assuming an underlying multivariate normal density for the marginal and conditional distribution of exposures given a set of confounders. These weights can then be used to estimate dose-response curves or surfaces. This method achieves balance across all exposure dimension rather than along a single dimension."
"mvGPS",1,1,1,"2020-09-17","Methods for estimating weights and generalized propensity score for multiple continuous exposures via the generalized propensity score described in Williams, J.R, and Cresi, C.M (2020) <arxiv:2008.13767>. Weights are constructed assuming an underlying multivariate normal density for the marginal and conditional distribution of exposures given a set of confounders. These weights can then be used to estimate dose-response curves or surfaces. This method achieves balance across all exposure dimension rather than along a single dimension."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"rsm",3,11,27,"2018-09-02 22:50","Provides functions to generate response-surface designs,     fit first- and second-order response-surface models,     make surface plots, obtain the path of steepest ascent,     and do canonical analysis. A good reference on these methods     is Chapter 10 of Wu, C-F J and Hamada, M (2009)     ""Experiments: Planning, Analysis, and Parameter Design Optimization""    ISBN 978-0-471-69946-0."
"mistat",0,1,6,"2013-09-26 11:57","Provide all the data sets and statistical analysis applications used in ""Modern Industrial Statistics: with applications in R, MINITAB and JMP"" by R.S. Kenett and S. Zacks with contributions by D. Amberti, John Wiley and Sons, 2013, which is a second revised and expanded revision of ""Modern Industrial Statistics: Design and Control of Quality and Reliability"", R. Kenett and S. Zacks, Duxbury/Wadsworth Publishing, 1998."
"labsimplex",1,1,1,"2020-06-03","Simplex optimization algorithms as firstly proposed by Spendley    et al. (1962) <doi:10.1080/00401706.1962.10490033> and later modified by     Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308> for laboratory and     manufacturing processes. The package also provides tools for graphical     representation of the simplexes and some example response surfaces that     are useful in illustrating the optimization process."
"BHH2",1,1,9,"2015-06-26 10:49","Functions and data sets reproducing some examples in             Box, Hunter and Hunter II.  Useful for statistical design             of experiments, especially factorial experiments.  "
"agridat",3,2,18,"2018-07-06 18:39","Datasets from books, papers, and websites related to agriculture.    Example graphics and analyses are included. Data come from small-plot trials,    multi-environment trials, uniformity trials, yield monitors, and more."
"simrel",1,1,3,"2019-04-01 20:00","Simulate multivariate linear model data is useful in research and education weather     for comparison or create data with specific properties. This package lets user to simulate    linear model data of wide range of properties with few tuning parameters.    The package also consist of function to create plots for the simulation    objects and A shiny app as RStudio gadget. It can be a handy tool for model comparison,     testing and many other purposes."
"pid",0,1,5,"2015-08-07 09:01","A collection of scripts and data files for the statistics text:  ""Process Improvement using Data"" <https://learnche.org/pid> and the online  course ""Experimentation for Improvement"" found on Coursera. The package  contains code for designed experiments, data sets and other convenience  functions used in the book."
"minimaxdesign",0,1,6,"2017-12-11 09:12","Provides two main functions, minimax() and miniMaxPro(), for computing minimax     and minimax projection designs using the minimax clustering algorithm in Mak and     Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options     include the unit hypercube (""hypercube""), the unit simplex (""simplex""), the unit ball    (""ball""), as well as user-defined constraints on the unit hypercube (""custom""). Minimax    designs can also be computed on user-provided images using the function minimax.map().     Design quality can be assessed using the function mMdist(), which computes the minimax    (fill) distance of a design."
"daewr",1,1,11,"2020-05-08 12:50","Contains Data frames and functions used in the book ""Design and Analysis of Experiments with R""."
"RcmdrPlugin.DoE",0,1,36,"2013-02-11 20:54","The package provides a platform-independent GUI for design of experiments. It is implemented as a plugin to the R-Commander, which is a more general  graphical user interface for statistics in R based on tcl/tk.  DoE functionality can be accessed through the menu Design that is added to the  R-Commander menus."
"DoE.wrapper",0,2,21,"2019-05-11 07:40","Various kinds of designs for (industrial) experiments can be created. The package uses, and sometimes enhances,        design generation routines from other packages.         So far, response surface designs from package rsm, latin hypercube        samples from packages lhs and DiceDesign, and         D-optimal designs from package AlgDesign have been implemented."
"apsimx",2,1,2,"2020-07-01 15:10","The functions in this package inspect, read, edit and run files for 'APSIM' ""Next Generation"" ('JSON')             and 'APSIM' ""Classic"" ('XML'). The files with an 'apsim' extension correspond to	     'APSIM' Classic (7.x) - Windows only - and the ones with an 'apsimx' extension correspond to 'APSIM' ""Next Generation"".	     For more information about 'APSIM' see (<https://www.apsim.info/>) and for 'APSIM'	     next generation (<https://apsimnextgeneration.netlify.com/>). "
"apsimx",2,1,2,"2020-07-01 15:10","The functions in this package inspect, read, edit and run files for 'APSIM' ""Next Generation"" ('JSON')             and 'APSIM' ""Classic"" ('XML'). The files with an 'apsim' extension correspond to	     'APSIM' Classic (7.x) - Windows only - and the ones with an 'apsimx' extension correspond to 'APSIM' ""Next Generation"".	     For more information about 'APSIM' see (<https://www.apsim.info/>) and for 'APSIM'	     next generation (<https://apsimnextgeneration.netlify.com/>). "
"apsimx",2,1,2,"2020-07-01 15:10","The functions in this package inspect, read, edit and run files for 'APSIM' ""Next Generation"" ('JSON')             and 'APSIM' ""Classic"" ('XML'). The files with an 'apsim' extension correspond to	     'APSIM' Classic (7.x) - Windows only - and the ones with an 'apsimx' extension correspond to 'APSIM' ""Next Generation"".	     For more information about 'APSIM' see (<https://www.apsim.info/>) and for 'APSIM'	     next generation (<https://apsimnextgeneration.netlify.com/>). "
"apsimx",2,1,2,"2020-07-01 15:10","The functions in this package inspect, read, edit and run files for 'APSIM' ""Next Generation"" ('JSON')             and 'APSIM' ""Classic"" ('XML'). The files with an 'apsim' extension correspond to	     'APSIM' Classic (7.x) - Windows only - and the ones with an 'apsimx' extension correspond to 'APSIM' ""Next Generation"".	     For more information about 'APSIM' see (<https://www.apsim.info/>) and for 'APSIM'	     next generation (<https://apsimnextgeneration.netlify.com/>). "
"apsimx",2,1,2,"2020-07-01 15:10","The functions in this package inspect, read, edit and run files for 'APSIM' ""Next Generation"" ('JSON')             and 'APSIM' ""Classic"" ('XML'). The files with an 'apsim' extension correspond to	     'APSIM' Classic (7.x) - Windows only - and the ones with an 'apsimx' extension correspond to 'APSIM' ""Next Generation"".	     For more information about 'APSIM' see (<https://www.apsim.info/>) and for 'APSIM'	     next generation (<https://apsimnextgeneration.netlify.com/>). "
"packager",1,1,7,"2020-08-24 10:30","Helper functions for package creation, building and    maintenance. Designed to work with a build system such as 'GNU make' or    package 'fakemake' to help you to conditionally work through the stages of    package development (such as spell checking, linting, testing, before    building and checking a package)."
"packager",1,1,7,"2020-08-24 10:30","Helper functions for package creation, building and    maintenance. Designed to work with a build system such as 'GNU make' or    package 'fakemake' to help you to conditionally work through the stages of    package development (such as spell checking, linting, testing, before    building and checking a package)."
"packager",1,1,7,"2020-08-24 10:30","Helper functions for package creation, building and    maintenance. Designed to work with a build system such as 'GNU make' or    package 'fakemake' to help you to conditionally work through the stages of    package development (such as spell checking, linting, testing, before    building and checking a package)."
"packager",1,1,7,"2020-08-24 10:30","Helper functions for package creation, building and    maintenance. Designed to work with a build system such as 'GNU make' or    package 'fakemake' to help you to conditionally work through the stages of    package development (such as spell checking, linting, testing, before    building and checking a package)."
"baitmet",1,1,2,"2017-01-14 12:12","Automated quantification of metabolites by targeting mass spectral/retention time libraries into full scan-acquired gas chromatography - mass spectrometry (GC-MS) chromatograms. Baitmet outputs a table with compounds name, spectral matching score, retention index error, and compounds area in each sample. Baitmet can automatically determine the compounds retention indexes with or without co-injection of internal standards with samples."
"IROmiss",0,1,3,"2018-02-19 19:15","Missing data are frequently encountered in high-dimensional data analysis, but they are usually difficult to deal with using standard algorithms, such as the EM algorithm and its variants. This package provides a general algorithm, the so-called Imputation Regularized Optimization (IRO) algorithm, for high-dimensional missing data problems. You can refer to Liang, F., Jia, B., Xue, J., Li, Q. and Luo, Y. (2018) at <arXiv:1802.02251> for detail."
"GGMM",0,1,2,"2018-05-09 10:36","The Gaussian graphical model is a widely used tool for learning gene regulatory networks with high-dimensional gene expression data. For many real problems, the data are heterogeneous, which may contain some subgroups or come from different resources. This package provide a Gaussian Graphical Mixture Model (GGMM) for the heterogeneous data. You can refer to Jia, B. and Liang, F. (2018) at <arXiv:1805.02547> for detail."
"epiflows",2,1,1,"2018-08-14","Provides functions and classes designed to handle and visualise    epidemiological flows between locations. Also contains a statistical method    for predicting disease spread from flow data initially described in    Dorigatti et al. (2017) <doi:10.2807/1560-7917.ES.2017.22.28.30572>.    This package is part of the RECON (<http://www.repidemicsconsortium.org/>)    toolkit for outbreak analysis."
"epiflows",2,1,1,"2018-08-14","Provides functions and classes designed to handle and visualise    epidemiological flows between locations. Also contains a statistical method    for predicting disease spread from flow data initially described in    Dorigatti et al. (2017) <doi:10.2807/1560-7917.ES.2017.22.28.30572>.    This package is part of the RECON (<http://www.repidemicsconsortium.org/>)    toolkit for outbreak analysis."
"parameters",8,15,13,"2020-09-12 07:50","Utilities for processing the parameters of various    statistical models. Beyond computing p values, CIs, and other indices    for a wide variety of models (see support list of insight; Lüdecke,    Waggoner & Makowski (2019) <doi:10.21105/joss.01412>), this package    implements features like bootstrapping or simulating of parameters and    models, feature reduction (feature extraction and variable selection)    as well as functions to describe data and variable characteristics    (e.g. skewness, kurtosis, smoothness or distribution)."
"parameters",8,15,13,"2020-09-12 07:50","Utilities for processing the parameters of various    statistical models. Beyond computing p values, CIs, and other indices    for a wide variety of models (see support list of insight; Lüdecke,    Waggoner & Makowski (2019) <doi:10.21105/joss.01412>), this package    implements features like bootstrapping or simulating of parameters and    models, feature reduction (feature extraction and variable selection)    as well as functions to describe data and variable characteristics    (e.g. skewness, kurtosis, smoothness or distribution)."
"parameters",8,15,13,"2020-09-12 07:50","Utilities for processing the parameters of various    statistical models. Beyond computing p values, CIs, and other indices    for a wide variety of models (see support list of insight; Lüdecke,    Waggoner & Makowski (2019) <doi:10.21105/joss.01412>), this package    implements features like bootstrapping or simulating of parameters and    models, feature reduction (feature extraction and variable selection)    as well as functions to describe data and variable characteristics    (e.g. skewness, kurtosis, smoothness or distribution)."
"parameters",8,15,13,"2020-09-12 07:50","Utilities for processing the parameters of various    statistical models. Beyond computing p values, CIs, and other indices    for a wide variety of models (see support list of insight; Lüdecke,    Waggoner & Makowski (2019) <doi:10.21105/joss.01412>), this package    implements features like bootstrapping or simulating of parameters and    models, feature reduction (feature extraction and variable selection)    as well as functions to describe data and variable characteristics    (e.g. skewness, kurtosis, smoothness or distribution)."
"parameters",8,15,13,"2020-09-12 07:50","Utilities for processing the parameters of various    statistical models. Beyond computing p values, CIs, and other indices    for a wide variety of models (see support list of insight; Lüdecke,    Waggoner & Makowski (2019) <doi:10.21105/joss.01412>), this package    implements features like bootstrapping or simulating of parameters and    models, feature reduction (feature extraction and variable selection)    as well as functions to describe data and variable characteristics    (e.g. skewness, kurtosis, smoothness or distribution)."
"mfe",1,1,6,"2019-12-16 21:10","Extracts meta-features from datasets to support the design of   recommendation systems based on Meta-Learning. The meta-features, also called   characterization measures, are able to characterize the complexity of datasets  and to provide estimates of algorithm performance. The package contains not   only the standard characterization measures, but also more recent   characterization measures. By making available a large set of meta-feature   extraction functions, tasks like comprehensive data characterization, deep   data exploration and large number of Meta-Learning based data analysis can be  performed. These concepts are described in the paper: Rivolli A., Garcia L.,   Soares c., Vanschoren J. and Carvalho A. (2018) <arXiv:1808.10406>."
"mfe",1,1,6,"2019-12-16 21:10","Extracts meta-features from datasets to support the design of   recommendation systems based on Meta-Learning. The meta-features, also called   characterization measures, are able to characterize the complexity of datasets  and to provide estimates of algorithm performance. The package contains not   only the standard characterization measures, but also more recent   characterization measures. By making available a large set of meta-feature   extraction functions, tasks like comprehensive data characterization, deep   data exploration and large number of Meta-Learning based data analysis can be  performed. These concepts are described in the paper: Rivolli A., Garcia L.,   Soares c., Vanschoren J. and Carvalho A. (2018) <arXiv:1808.10406>."
"mfe",1,1,6,"2019-12-16 21:10","Extracts meta-features from datasets to support the design of   recommendation systems based on Meta-Learning. The meta-features, also called   characterization measures, are able to characterize the complexity of datasets  and to provide estimates of algorithm performance. The package contains not   only the standard characterization measures, but also more recent   characterization measures. By making available a large set of meta-feature   extraction functions, tasks like comprehensive data characterization, deep   data exploration and large number of Meta-Learning based data analysis can be  performed. These concepts are described in the paper: Rivolli A., Garcia L.,   Soares c., Vanschoren J. and Carvalho A. (2018) <arXiv:1808.10406>."
"mfe",1,1,6,"2019-12-16 21:10","Extracts meta-features from datasets to support the design of   recommendation systems based on Meta-Learning. The meta-features, also called   characterization measures, are able to characterize the complexity of datasets  and to provide estimates of algorithm performance. The package contains not   only the standard characterization measures, but also more recent   characterization measures. By making available a large set of meta-feature   extraction functions, tasks like comprehensive data characterization, deep   data exploration and large number of Meta-Learning based data analysis can be  performed. These concepts are described in the paper: Rivolli A., Garcia L.,   Soares c., Vanschoren J. and Carvalho A. (2018) <arXiv:1808.10406>."
"mfe",1,1,6,"2019-12-16 21:10","Extracts meta-features from datasets to support the design of   recommendation systems based on Meta-Learning. The meta-features, also called   characterization measures, are able to characterize the complexity of datasets  and to provide estimates of algorithm performance. The package contains not   only the standard characterization measures, but also more recent   characterization measures. By making available a large set of meta-feature   extraction functions, tasks like comprehensive data characterization, deep   data exploration and large number of Meta-Learning based data analysis can be  performed. These concepts are described in the paper: Rivolli A., Garcia L.,   Soares c., Vanschoren J. and Carvalho A. (2018) <arXiv:1808.10406>."
"SCORPIUS",3,1,8,"2017-09-15 17:46","An accurate and easy tool for performing linear trajectory inference on  single cells using single-cell RNA sequencing data. In addition, SCORPIUS  provides functions for discovering the most important genes with respect to  the reconstructed trajectory, as well as nice visualisation tools.  Cannoodt et al. (2016) <doi:10.1101/079509>."
"SCORPIUS",3,1,8,"2017-09-15 17:46","An accurate and easy tool for performing linear trajectory inference on  single cells using single-cell RNA sequencing data. In addition, SCORPIUS  provides functions for discovering the most important genes with respect to  the reconstructed trajectory, as well as nice visualisation tools.  Cannoodt et al. (2016) <doi:10.1101/079509>."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"trialr",8,1,12,"2020-01-08 23:30","A collection of clinical trial designs and methods, implemented in     'rstan' and R, including: the Continual Reassessment Method by O'Quigley et     al. (1990) <doi:10.2307/2531628>; EffTox by Thall & Cook (2004)     <doi:10.1111/j.0006-341X.2004.00218.x>; and the Augmented Binary method by     Wason & Seaman (2013) <doi:10.1002/sim.5867>; and more. We provide functions     to aid model-fitting and analysis. The 'rstan' implementations may also     serve as a cookbook to anyone looking to extend or embellish these models.     We hope that this package encourages the use of Bayesian methods in clinical     trials. There is a preponderance of early phase trial designs because this     is where Bayesian methods are used most. If there is a method you would like     implemented, please get in touch."
"teachingApps",0,1,4,"2018-06-10 06:22","Contains apps and gadgets for teaching data analysis and     statistics concepts along with how to implement them in R.  Includes     tools to make app development easier and faster by nesting apps together."
"shinyEventLogger",1,1,1,"2019-02-22","Logging framework dedicated for complex shiny apps.     Different types of events can be logged     (value of a variable, multi-line output of a function,    result of a unit test, custom error, warning, or diagnostic message).    Each event can be logged with a list of parameters that are event-specific,     common for events within the same scope, session-specific, or app-wide.    Logging can be done simultaneously to R console,     browser JavaScript console, a file log, and a database (MongoDB).    Log data can be further analyzed with the help of process-mining techniques     from 'bupaR' package."
"sem",0,13,46,"2017-04-24 16:56","Functions for fitting general linear structural    equation models (with observed and latent variables) using the RAM approach,     and for fitting structural equations in observed-variable models by two-stage least squares."
"radix",0,1,2,"2018-09-17 17:40","Scientific and technical article format for the web. 'Radix' articles     feature attractive, reader-friendly typography, flexible layout options    for visualizations, and full support for footnotes and citations."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"policytree",0,1,7,"2020-06-22 06:50","Learn optimal policies via doubly robust empirical welfare maximization over trees. This package implements the multi-action doubly robust approach of Zhou, Athey and Wager (2018) <arXiv:1810.04778> in the case where we want to learn policies that belong to the class of depth k decision trees."
"oxcovid19",3,1,2,"2020-07-16 12:10","The OxCOVID19 Project <https://covid19.eng.ox.ac.uk> aims to increase    our understanding of the COVID-19 pandemic and elaborate possible strategies    to reduce the impact on the society through the combined power of statistical,    mathematical modelling, and machine learning techniques. The OxCOVID19    Database is a large, single-centre, multimodal relational database consisting    of information (using acknowledged sources) related to COVID-19 pandemic.    This package provides an R-specific interface to the OxCOVID19 Database based    on widely-used data handling and manipulation approaches in R."
"m2b",1,1,1,"2017-05-03","Prediction of behaviour from movement 	characteristics using observation and random forest for the analyses of movement	data in ecology.	From movement information (speed, bearing...) the model predicts the	observed behaviour (movement, foraging...) using random forest. The	model can then extrapolate behavioural information to movement data	without direct observation of behaviours.	The specificity of this method relies on the derivation of multiple predictor variables from the	movement data over a range of temporal windows. This procedure allows to capture	as much information as possible on the changes and variations of movement and	ensures the use of the random forest algorithm to its best capacity. The method	is very generic, applicable to any set of data providing movement data together with	observation of behaviour."
"irg",1,1,2,"2019-01-04 11:50","Fits a double logistic function to NDVI time series and calculates              instantaneous rate of green (IRG) according to methods described             in Bischoff et al. (2012) <doi:10.1086/667590>. "
"hablar",4,1,3,"2019-06-09 19:20","Simple tools for converting columns to new data types. Intuitive functions for columns with missing values. "
"grf",0,3,14,"2020-03-12 07:30","A pluggable package for forest-based statistical estimation and inference.    GRF currently provides methods for non-parametric least-squares regression,    quantile regression, survival regression and treatment effect estimation (optionally using instrumental    variables), with support for missing values."
"greta",2,1,4,"2018-10-30 11:00","Write statistical models in R and fit them by MCMC and optimisation on CPUs and GPUs, using Google 'TensorFlow'.  greta lets you write your own model like in BUGS, JAGS and Stan, except that you write models right in R, it scales well to massive datasets, and it’s easy to extend and build on.  See the website for more information, including tutorials, examples, package documentation, and the greta forum."
"drf",0,1,1,"2020-06-22","An implementation of distributional random forests as introduced in Cevid & Michel & Meinshausen & Buhlmann (2020) <arXiv:2005.14458>."
"distill",0,1,2,"2019-05-03 15:10","Scientific and technical article format for the web. 'Distill' articles     feature attractive, reader-friendly typography, flexible layout options    for visualizations, and full support for footnotes and citations."
"deisotoper",1,1,4,"2017-12-19 12:40","Provides a low-level interface for a deisotoper container   implemented in the 'Java' programming language and means of S3 helper   functions for plotting and debugging isotopes of mass spectrometric data.   The deisotoper algorithm detects and aggregates peaks which belong to the   same isotopic cluster of a given mass spectrum. "
"vtree",1,1,6,"2020-01-08 23:30","A tool for calculating and drawing ""variable trees"". Variable trees display information about nested subsets of a data frame."
"umx",0,1,30,"2020-02-06 10:40","Quickly create, run, and report structural equation and twin models.    See '?umx' for help, and umx_open_CRAN_page(""umx"") for NEWS."
"simmer.plot",2,1,15,"2019-03-10 21:30","A set of plotting methods for 'simmer' trajectories and simulations."
"PRISMAstatement",2,1,5,"2018-08-09 00:00","Plot a PRISMA <http://prisma-statement.org/> flow    chart describing the identification, screening, eligibility and    inclusion or studies in systematic reviews. The PRISMA statement    defines an evidence-based, minimal set of items for reporting in    systematic reviews and meta-analyses. PRISMA should be used for the    reporting of studies evaluating randomized clinical trials (RCT), and    is also for reporting on systematic reviews of other types of    research. There is also a function to generate flow charts describing    exclusions and inclusions for any kind of study."
"prismadiagramR",1,1,1,"2020-05-04","Creates 'PRISMA' <http://prisma-statement.org/> diagram from a minimal dataset of included and excluded studies and allows for more custom diagrams. 'PRISMA' diagrams are used to track the identification, screening, eligibility, and inclusion of studies in a systematic review. "
"petrinetR",0,2,3,"2018-07-03 17:50","Functions for the construction of Petri Nets. Petri Nets can be replayed by firing enabled transitions.     Silent transitions will be hidden by the execution handler. Also includes functionalities for the visualization of Petri Nets and     export of Petri Nets to PNML (Petri Net Markup Language) files."
"netSEM",6,1,2,"2018-06-12 12:33","The network structural equation modeling conducts a network statistical analysis    on a data frame of coincident observations of multiple continuous variables [1].     It builds a pathway model by exploring a pool of domain knowledge guided candidate statistical    relationships between each of the variable pairs, selecting the 'best fit' on    the basis of a specific criteria such as adjusted r-squared value.    This work was funded under U. S. Dept. of Energy, Prime Award No. DE-E-0004946,     Award Agreement No. 60220829-51077-T.     [1] Bruckman, Laura S., Nicholas R. Wheeler, Junheng Ma, Ethan Wang, Carl K. Wang, Ivan Chou,     Jiayang Sun, and Roger H. French. (2013) <doi:10.1109/ACCESS.2013.2267611>."
"muir",0,1,1,"2015-05-02","A simple tool allowing users to easily and dynamically explore or document a data set using a tree structure."
"HydeNet",4,1,11,"2020-05-15 15:40","Facilities for easy implementation of hybrid Bayesian networks    using R. Bayesian networks are directed acyclic graphs representing joint    probability distributions, where each node represents a random variable and    each edge represents conditionality. The full joint distribution is therefore    factorized as a product of conditional densities, where each node is assumed    to be independent of its non-descendents given information on its parent nodes.    Since exact, closed-form algorithms are computationally burdensome for inference    within hybrid networks that contain a combination of continuous and discrete    nodes, particle-based approximation techniques like Markov Chain Monte Carlo    are popular. We provide a user-friendly interface to constructing these networks    and running inference using the 'rjags' package. Econometric analyses (maximum    expected utility under competing policies, value of information) involving    decision and utility nodes are also supported."
"heuristicsmineR",0,1,5,"2020-03-19 18:10","Provides the heuristics miner algorithm for process discovery    as proposed by Weijters et al. (2011) <doi:10.1109/CIDM.2011.5949453>. The   algorithm builds a causal net from an event log created with the 'bupaR'    package. Event logs are a set of ordered sequences of events for which    'bupaR' provides the S3 class eventlog(). The discovered causal nets    can be visualised as 'htmlwidgets' and it is possible to annotate them with   the occurrence frequency or processing and waiting time of process    activities.  "
"gSEM",0,1,2,"2015-12-23 16:13","Conducts a semi-gSEM statistical analysis (semi-supervised generalized structural equation modeling) on a data frame of coincident observations of multiple predictive or intermediate variables and a final continuous, outcome variable, via two functions sgSEMp1() and sgSEMp2(), representing fittings based on two statistical principles. Principle 1 determines all sensible univariate relationships in the spirit of the Markovian process. The relationship between each pair of variables, including predictors and the final outcome variable, is determined with the Markovian property that the value of the current predictor is sufficient in relating to the next level variable, i.e., the relationship is independent of the specific value of the preceding-level variables to the current predictor, given the current value. Principle 2 resembles the multiple regression principle in the way multiple predictors are considered simultaneously. Specifically, the relationship of the first-level predictors (such as Time and irradiance etc) to the outcome variable (such as, module degradation or yellowing)  is fit by a supervised additive model. Then each significant intermediate variable is taken as the new outcome variable and the other variables (except the final outcome variable) as the predictors in investigating the next-level multivariate relationship by a supervised additive model. This fitting process is continued until all sensible models are investigated."
"escalation",3,1,3,"2020-04-14 17:10","Methods for working with dose-finding clinical trials. We start by     providing a common interface to various dose-finding methodologies like the    continual reassessment method (CRM) by O'Quigley et al. (1990)     <doi:10.2307/2531628>, the Bayesian optimal interval design (BOIN) by Liu &     Yuan (2015) <doi:10.1111/rssc.12089>, and the 3+3 described by Korn et al.     (1994) <doi:10.1002/sim.4780131802>. We then add optional embellishments to     provide extra desirable behaviour, like avoiding skipping doses, stopping     after n patients have been treated at the recommended dose, or demanding     that n patients are treated before stopping is allowed. By daisy-chaining     together these embellishments using the pipe operator from 'magrittr', it is    simple to tailor the behaviour of dose-finding designs so that they do what     you want. Furthermore, using this flexible interface for creating     dose-finding designs, it is simple to run simulations or calculate     dose-pathways for future cohorts of patients."
"cohorttools",0,1,3,"2020-03-02 12:30","Functions to make lifetables and to calculate hazard function estimate using Poisson regression model with splines. Includes function to draw simple flowchart of cohort study. Function boxesLx() makes boxes of transition rates between states. It utilizes 'Epi' package 'Lexis' data."
"bsem",2,1,1,"2020-08-14","Flexible routines to allow structural equation modeling particular cases using 'rstan' integration. 'bsem' includes Bayesian semi Confirmatory Factor Analysis, Confirmatory Factor Analysis, and Structural Equation Model.  VD Mayrink (2013) <doi:10.1214/12-AOAS607>."
"BayesianFROC",2,1,11,"2020-07-02 12:00","Execute BayesianFROC::fit_GUI_Shiny() (or fit_GUI_Shiny_MRMC()) for a graphical user interface via Shiny.  Provides new methods for the so-called Free-response Receiver Operating Characteristic (FROC) analysis. The ultimate aim of FROC analysis is to compare observer performances, which means comparing characteristics, such as area under the curve (AUC) or figure of merit (FOM). In this package, we only use the notion of AUC for modality comparison, where by ""modality"",  we mean imaging methods such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET), ..., etc. So there is a problem that which imaging method is better to detect lesions from shadows in radiographs. To solve modality comparison issues, this package provides new methods using hierarchical Bayesian models proposed by the author of this package. Using this package, one can obtain at least one conclusion that which imaging methods are better for finding lesions in radiographs with the case of your data. Fitting FROC statistical models is sometimes not so good, it can easily confirm by drawing FROC curves and comparing these curves and the points constructed by False Positive fractions (FPFs) and True Positive Fractions (TPFs), we can validate the goodness of fit intuitively. Such validation is also implemented by the Chi square goodness of fit statistics in the Bayesian context which means that the parameter is not deterministic, thus by integrating it with the posterior predictive measure, we get a desired value. To compare modalities (imaging methods: MRI, CT, PET, ... , etc),  we evaluate AUCs for each modality. FROC is developed by Dev Chakraborty, his FROC model in his 1989 paper relies on the maximal likelihood methodology. The author modified and provided the alternative Bayesian FROC model. Strictly speaking, his model does not coincide with models in this package. In FROC context, we means by multiple reader and multiple case (MRMC) the case of the number of reader or modality is two or more. The MRMC data is available for functions of this package. I hope that medical researchers use not only the frequentist method but also alternative Bayesian methods. In medical research, many problems are considered under only frequentist methods, such as the notion of p-values. But p-value is sometimes misunderstood. Bayesian methods provide very simple, direct, intuitive answer for research questions. Combining frequentist methods with Bayesian methods, we can obtain more reliable answer for research questions. Please execute the following R scripts from the R (R studio) console, demo(demo_MRMC, package = ""BayesianFROC""); demo(demo_srsc, package = ""BayesianFROC""); demo(demo_stan, package = ""BayesianFROC""); demo(demo_drawcurves_srsc, package = ""BayesianFROC""); demo_Bayesian_FROC(); demo_Bayesian_FROC_without_pause(). References: Dev Chakraborty (1989) <doi:10.1118/1.596358> Maximum likelihood analysis of free - response receiver operating characteristic (FROC) data. Pre-print: Issei Tsunoda; Bayesian Models for free-response receiver operating characteristic analysis."
"actel",17,1,3,"2020-08-01 13:10","Designed for studies where animals tagged with acoustic tags are expected    to move through receiver arrays. This package combines the advantages of automatic sorting and checking     of animal movements with the possibility for user intervention on tags that deviate from expected     behaviour. The three analysis functions (explore(), migration() and residency())     allow the users to analyse their data in a systematic way, making it easy to compare results from     different studies.    CJS calculations are based on Perry et al. (2012) <https://www.researchgate.net/publication/256443823_Using_mark-recapture_models_to_estimate_survival_from_telemetry_data>."
"sbfc",0,1,3,"2016-07-16 00:01","An MCMC algorithm for simultaneous feature selection and classification,     and visualization of the selected features and feature interactions.     An implementation of SBFC by Krakovna, Du and Liu (2015), <arXiv:1506.02371>."
"integr",1,1,1,"2019-05-24","Generates a 'Graphviz' graph of the most significant 3-way     interaction gains (i.e. conditional information gains) based on a provided     discrete data frame. Various output formats are supported ('Graphviz', SVG,     PNG, PDF, PS). For references, see the webpage of Aleks Jakulin     <http://stat.columbia.edu/~jakulin/Int/>."
"trialr",8,1,12,"2020-01-08 23:30","A collection of clinical trial designs and methods, implemented in     'rstan' and R, including: the Continual Reassessment Method by O'Quigley et     al. (1990) <doi:10.2307/2531628>; EffTox by Thall & Cook (2004)     <doi:10.1111/j.0006-341X.2004.00218.x>; and the Augmented Binary method by     Wason & Seaman (2013) <doi:10.1002/sim.5867>; and more. We provide functions     to aid model-fitting and analysis. The 'rstan' implementations may also     serve as a cookbook to anyone looking to extend or embellish these models.     We hope that this package encourages the use of Bayesian methods in clinical     trials. There is a preponderance of early phase trial designs because this     is where Bayesian methods are used most. If there is a method you would like     implemented, please get in touch."
"teachingApps",0,1,4,"2018-06-10 06:22","Contains apps and gadgets for teaching data analysis and     statistics concepts along with how to implement them in R.  Includes     tools to make app development easier and faster by nesting apps together."
"shinyEventLogger",1,1,1,"2019-02-22","Logging framework dedicated for complex shiny apps.     Different types of events can be logged     (value of a variable, multi-line output of a function,    result of a unit test, custom error, warning, or diagnostic message).    Each event can be logged with a list of parameters that are event-specific,     common for events within the same scope, session-specific, or app-wide.    Logging can be done simultaneously to R console,     browser JavaScript console, a file log, and a database (MongoDB).    Log data can be further analyzed with the help of process-mining techniques     from 'bupaR' package."
"sem",0,13,46,"2017-04-24 16:56","Functions for fitting general linear structural    equation models (with observed and latent variables) using the RAM approach,     and for fitting structural equations in observed-variable models by two-stage least squares."
"radix",0,1,2,"2018-09-17 17:40","Scientific and technical article format for the web. 'Radix' articles     feature attractive, reader-friendly typography, flexible layout options    for visualizations, and full support for footnotes and citations."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"policytree",0,1,7,"2020-06-22 06:50","Learn optimal policies via doubly robust empirical welfare maximization over trees. This package implements the multi-action doubly robust approach of Zhou, Athey and Wager (2018) <arXiv:1810.04778> in the case where we want to learn policies that belong to the class of depth k decision trees."
"oxcovid19",3,1,2,"2020-07-16 12:10","The OxCOVID19 Project <https://covid19.eng.ox.ac.uk> aims to increase    our understanding of the COVID-19 pandemic and elaborate possible strategies    to reduce the impact on the society through the combined power of statistical,    mathematical modelling, and machine learning techniques. The OxCOVID19    Database is a large, single-centre, multimodal relational database consisting    of information (using acknowledged sources) related to COVID-19 pandemic.    This package provides an R-specific interface to the OxCOVID19 Database based    on widely-used data handling and manipulation approaches in R."
"m2b",1,1,1,"2017-05-03","Prediction of behaviour from movement 	characteristics using observation and random forest for the analyses of movement	data in ecology.	From movement information (speed, bearing...) the model predicts the	observed behaviour (movement, foraging...) using random forest. The	model can then extrapolate behavioural information to movement data	without direct observation of behaviours.	The specificity of this method relies on the derivation of multiple predictor variables from the	movement data over a range of temporal windows. This procedure allows to capture	as much information as possible on the changes and variations of movement and	ensures the use of the random forest algorithm to its best capacity. The method	is very generic, applicable to any set of data providing movement data together with	observation of behaviour."
"irg",1,1,2,"2019-01-04 11:50","Fits a double logistic function to NDVI time series and calculates              instantaneous rate of green (IRG) according to methods described             in Bischoff et al. (2012) <doi:10.1086/667590>. "
"hablar",4,1,3,"2019-06-09 19:20","Simple tools for converting columns to new data types. Intuitive functions for columns with missing values. "
"grf",0,3,14,"2020-03-12 07:30","A pluggable package for forest-based statistical estimation and inference.    GRF currently provides methods for non-parametric least-squares regression,    quantile regression, survival regression and treatment effect estimation (optionally using instrumental    variables), with support for missing values."
"greta",2,1,4,"2018-10-30 11:00","Write statistical models in R and fit them by MCMC and optimisation on CPUs and GPUs, using Google 'TensorFlow'.  greta lets you write your own model like in BUGS, JAGS and Stan, except that you write models right in R, it scales well to massive datasets, and it’s easy to extend and build on.  See the website for more information, including tutorials, examples, package documentation, and the greta forum."
"drf",0,1,1,"2020-06-22","An implementation of distributional random forests as introduced in Cevid & Michel & Meinshausen & Buhlmann (2020) <arXiv:2005.14458>."
"distill",0,1,2,"2019-05-03 15:10","Scientific and technical article format for the web. 'Distill' articles     feature attractive, reader-friendly typography, flexible layout options    for visualizations, and full support for footnotes and citations."
"deisotoper",1,1,4,"2017-12-19 12:40","Provides a low-level interface for a deisotoper container   implemented in the 'Java' programming language and means of S3 helper   functions for plotting and debugging isotopes of mass spectrometric data.   The deisotoper algorithm detects and aggregates peaks which belong to the   same isotopic cluster of a given mass spectrum. "
"vtree",1,1,6,"2020-01-08 23:30","A tool for calculating and drawing ""variable trees"". Variable trees display information about nested subsets of a data frame."
"umx",0,1,30,"2020-02-06 10:40","Quickly create, run, and report structural equation and twin models.    See '?umx' for help, and umx_open_CRAN_page(""umx"") for NEWS."
"simmer.plot",2,1,15,"2019-03-10 21:30","A set of plotting methods for 'simmer' trajectories and simulations."
"PRISMAstatement",2,1,5,"2018-08-09 00:00","Plot a PRISMA <http://prisma-statement.org/> flow    chart describing the identification, screening, eligibility and    inclusion or studies in systematic reviews. The PRISMA statement    defines an evidence-based, minimal set of items for reporting in    systematic reviews and meta-analyses. PRISMA should be used for the    reporting of studies evaluating randomized clinical trials (RCT), and    is also for reporting on systematic reviews of other types of    research. There is also a function to generate flow charts describing    exclusions and inclusions for any kind of study."
"prismadiagramR",1,1,1,"2020-05-04","Creates 'PRISMA' <http://prisma-statement.org/> diagram from a minimal dataset of included and excluded studies and allows for more custom diagrams. 'PRISMA' diagrams are used to track the identification, screening, eligibility, and inclusion of studies in a systematic review. "
"petrinetR",0,2,3,"2018-07-03 17:50","Functions for the construction of Petri Nets. Petri Nets can be replayed by firing enabled transitions.     Silent transitions will be hidden by the execution handler. Also includes functionalities for the visualization of Petri Nets and     export of Petri Nets to PNML (Petri Net Markup Language) files."
"netSEM",6,1,2,"2018-06-12 12:33","The network structural equation modeling conducts a network statistical analysis    on a data frame of coincident observations of multiple continuous variables [1].     It builds a pathway model by exploring a pool of domain knowledge guided candidate statistical    relationships between each of the variable pairs, selecting the 'best fit' on    the basis of a specific criteria such as adjusted r-squared value.    This work was funded under U. S. Dept. of Energy, Prime Award No. DE-E-0004946,     Award Agreement No. 60220829-51077-T.     [1] Bruckman, Laura S., Nicholas R. Wheeler, Junheng Ma, Ethan Wang, Carl K. Wang, Ivan Chou,     Jiayang Sun, and Roger H. French. (2013) <doi:10.1109/ACCESS.2013.2267611>."
"muir",0,1,1,"2015-05-02","A simple tool allowing users to easily and dynamically explore or document a data set using a tree structure."
"HydeNet",4,1,11,"2020-05-15 15:40","Facilities for easy implementation of hybrid Bayesian networks    using R. Bayesian networks are directed acyclic graphs representing joint    probability distributions, where each node represents a random variable and    each edge represents conditionality. The full joint distribution is therefore    factorized as a product of conditional densities, where each node is assumed    to be independent of its non-descendents given information on its parent nodes.    Since exact, closed-form algorithms are computationally burdensome for inference    within hybrid networks that contain a combination of continuous and discrete    nodes, particle-based approximation techniques like Markov Chain Monte Carlo    are popular. We provide a user-friendly interface to constructing these networks    and running inference using the 'rjags' package. Econometric analyses (maximum    expected utility under competing policies, value of information) involving    decision and utility nodes are also supported."
"heuristicsmineR",0,1,5,"2020-03-19 18:10","Provides the heuristics miner algorithm for process discovery    as proposed by Weijters et al. (2011) <doi:10.1109/CIDM.2011.5949453>. The   algorithm builds a causal net from an event log created with the 'bupaR'    package. Event logs are a set of ordered sequences of events for which    'bupaR' provides the S3 class eventlog(). The discovered causal nets    can be visualised as 'htmlwidgets' and it is possible to annotate them with   the occurrence frequency or processing and waiting time of process    activities.  "
"gSEM",0,1,2,"2015-12-23 16:13","Conducts a semi-gSEM statistical analysis (semi-supervised generalized structural equation modeling) on a data frame of coincident observations of multiple predictive or intermediate variables and a final continuous, outcome variable, via two functions sgSEMp1() and sgSEMp2(), representing fittings based on two statistical principles. Principle 1 determines all sensible univariate relationships in the spirit of the Markovian process. The relationship between each pair of variables, including predictors and the final outcome variable, is determined with the Markovian property that the value of the current predictor is sufficient in relating to the next level variable, i.e., the relationship is independent of the specific value of the preceding-level variables to the current predictor, given the current value. Principle 2 resembles the multiple regression principle in the way multiple predictors are considered simultaneously. Specifically, the relationship of the first-level predictors (such as Time and irradiance etc) to the outcome variable (such as, module degradation or yellowing)  is fit by a supervised additive model. Then each significant intermediate variable is taken as the new outcome variable and the other variables (except the final outcome variable) as the predictors in investigating the next-level multivariate relationship by a supervised additive model. This fitting process is continued until all sensible models are investigated."
"escalation",3,1,3,"2020-04-14 17:10","Methods for working with dose-finding clinical trials. We start by     providing a common interface to various dose-finding methodologies like the    continual reassessment method (CRM) by O'Quigley et al. (1990)     <doi:10.2307/2531628>, the Bayesian optimal interval design (BOIN) by Liu &     Yuan (2015) <doi:10.1111/rssc.12089>, and the 3+3 described by Korn et al.     (1994) <doi:10.1002/sim.4780131802>. We then add optional embellishments to     provide extra desirable behaviour, like avoiding skipping doses, stopping     after n patients have been treated at the recommended dose, or demanding     that n patients are treated before stopping is allowed. By daisy-chaining     together these embellishments using the pipe operator from 'magrittr', it is    simple to tailor the behaviour of dose-finding designs so that they do what     you want. Furthermore, using this flexible interface for creating     dose-finding designs, it is simple to run simulations or calculate     dose-pathways for future cohorts of patients."
"cohorttools",0,1,3,"2020-03-02 12:30","Functions to make lifetables and to calculate hazard function estimate using Poisson regression model with splines. Includes function to draw simple flowchart of cohort study. Function boxesLx() makes boxes of transition rates between states. It utilizes 'Epi' package 'Lexis' data."
"bsem",2,1,1,"2020-08-14","Flexible routines to allow structural equation modeling particular cases using 'rstan' integration. 'bsem' includes Bayesian semi Confirmatory Factor Analysis, Confirmatory Factor Analysis, and Structural Equation Model.  VD Mayrink (2013) <doi:10.1214/12-AOAS607>."
"BayesianFROC",2,1,11,"2020-07-02 12:00","Execute BayesianFROC::fit_GUI_Shiny() (or fit_GUI_Shiny_MRMC()) for a graphical user interface via Shiny.  Provides new methods for the so-called Free-response Receiver Operating Characteristic (FROC) analysis. The ultimate aim of FROC analysis is to compare observer performances, which means comparing characteristics, such as area under the curve (AUC) or figure of merit (FOM). In this package, we only use the notion of AUC for modality comparison, where by ""modality"",  we mean imaging methods such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET), ..., etc. So there is a problem that which imaging method is better to detect lesions from shadows in radiographs. To solve modality comparison issues, this package provides new methods using hierarchical Bayesian models proposed by the author of this package. Using this package, one can obtain at least one conclusion that which imaging methods are better for finding lesions in radiographs with the case of your data. Fitting FROC statistical models is sometimes not so good, it can easily confirm by drawing FROC curves and comparing these curves and the points constructed by False Positive fractions (FPFs) and True Positive Fractions (TPFs), we can validate the goodness of fit intuitively. Such validation is also implemented by the Chi square goodness of fit statistics in the Bayesian context which means that the parameter is not deterministic, thus by integrating it with the posterior predictive measure, we get a desired value. To compare modalities (imaging methods: MRI, CT, PET, ... , etc),  we evaluate AUCs for each modality. FROC is developed by Dev Chakraborty, his FROC model in his 1989 paper relies on the maximal likelihood methodology. The author modified and provided the alternative Bayesian FROC model. Strictly speaking, his model does not coincide with models in this package. In FROC context, we means by multiple reader and multiple case (MRMC) the case of the number of reader or modality is two or more. The MRMC data is available for functions of this package. I hope that medical researchers use not only the frequentist method but also alternative Bayesian methods. In medical research, many problems are considered under only frequentist methods, such as the notion of p-values. But p-value is sometimes misunderstood. Bayesian methods provide very simple, direct, intuitive answer for research questions. Combining frequentist methods with Bayesian methods, we can obtain more reliable answer for research questions. Please execute the following R scripts from the R (R studio) console, demo(demo_MRMC, package = ""BayesianFROC""); demo(demo_srsc, package = ""BayesianFROC""); demo(demo_stan, package = ""BayesianFROC""); demo(demo_drawcurves_srsc, package = ""BayesianFROC""); demo_Bayesian_FROC(); demo_Bayesian_FROC_without_pause(). References: Dev Chakraborty (1989) <doi:10.1118/1.596358> Maximum likelihood analysis of free - response receiver operating characteristic (FROC) data. Pre-print: Issei Tsunoda; Bayesian Models for free-response receiver operating characteristic analysis."
"actel",17,1,3,"2020-08-01 13:10","Designed for studies where animals tagged with acoustic tags are expected    to move through receiver arrays. This package combines the advantages of automatic sorting and checking     of animal movements with the possibility for user intervention on tags that deviate from expected     behaviour. The three analysis functions (explore(), migration() and residency())     allow the users to analyse their data in a systematic way, making it easy to compare results from     different studies.    CJS calculations are based on Perry et al. (2012) <https://www.researchgate.net/publication/256443823_Using_mark-recapture_models_to_estimate_survival_from_telemetry_data>."
"sbfc",0,1,3,"2016-07-16 00:01","An MCMC algorithm for simultaneous feature selection and classification,     and visualization of the selected features and feature interactions.     An implementation of SBFC by Krakovna, Du and Liu (2015), <arXiv:1506.02371>."
"integr",1,1,1,"2019-05-24","Generates a 'Graphviz' graph of the most significant 3-way     interaction gains (i.e. conditional information gains) based on a provided     discrete data frame. Various output formats are supported ('Graphviz', SVG,     PNG, PDF, PS). For references, see the webpage of Aleks Jakulin     <http://stat.columbia.edu/~jakulin/Int/>."
"trialr",8,1,12,"2020-01-08 23:30","A collection of clinical trial designs and methods, implemented in     'rstan' and R, including: the Continual Reassessment Method by O'Quigley et     al. (1990) <doi:10.2307/2531628>; EffTox by Thall & Cook (2004)     <doi:10.1111/j.0006-341X.2004.00218.x>; and the Augmented Binary method by     Wason & Seaman (2013) <doi:10.1002/sim.5867>; and more. We provide functions     to aid model-fitting and analysis. The 'rstan' implementations may also     serve as a cookbook to anyone looking to extend or embellish these models.     We hope that this package encourages the use of Bayesian methods in clinical     trials. There is a preponderance of early phase trial designs because this     is where Bayesian methods are used most. If there is a method you would like     implemented, please get in touch."
"teachingApps",0,1,4,"2018-06-10 06:22","Contains apps and gadgets for teaching data analysis and     statistics concepts along with how to implement them in R.  Includes     tools to make app development easier and faster by nesting apps together."
"shinyEventLogger",1,1,1,"2019-02-22","Logging framework dedicated for complex shiny apps.     Different types of events can be logged     (value of a variable, multi-line output of a function,    result of a unit test, custom error, warning, or diagnostic message).    Each event can be logged with a list of parameters that are event-specific,     common for events within the same scope, session-specific, or app-wide.    Logging can be done simultaneously to R console,     browser JavaScript console, a file log, and a database (MongoDB).    Log data can be further analyzed with the help of process-mining techniques     from 'bupaR' package."
"sem",0,13,46,"2017-04-24 16:56","Functions for fitting general linear structural    equation models (with observed and latent variables) using the RAM approach,     and for fitting structural equations in observed-variable models by two-stage least squares."
"radix",0,1,2,"2018-09-17 17:40","Scientific and technical article format for the web. 'Radix' articles     feature attractive, reader-friendly typography, flexible layout options    for visualizations, and full support for footnotes and citations."
"profile",0,1,3,"2018-01-05 11:19","Defines a data structure for profiler data, and methods to read and    write from the 'Rprof' and 'pprof' file formats."
"policytree",0,1,7,"2020-06-22 06:50","Learn optimal policies via doubly robust empirical welfare maximization over trees. This package implements the multi-action doubly robust approach of Zhou, Athey and Wager (2018) <arXiv:1810.04778> in the case where we want to learn policies that belong to the class of depth k decision trees."
"oxcovid19",3,1,2,"2020-07-16 12:10","The OxCOVID19 Project <https://covid19.eng.ox.ac.uk> aims to increase    our understanding of the COVID-19 pandemic and elaborate possible strategies    to reduce the impact on the society through the combined power of statistical,    mathematical modelling, and machine learning techniques. The OxCOVID19    Database is a large, single-centre, multimodal relational database consisting    of information (using acknowledged sources) related to COVID-19 pandemic.    This package provides an R-specific interface to the OxCOVID19 Database based    on widely-used data handling and manipulation approaches in R."
"m2b",1,1,1,"2017-05-03","Prediction of behaviour from movement 	characteristics using observation and random forest for the analyses of movement	data in ecology.	From movement information (speed, bearing...) the model predicts the	observed behaviour (movement, foraging...) using random forest. The	model can then extrapolate behavioural information to movement data	without direct observation of behaviours.	The specificity of this method relies on the derivation of multiple predictor variables from the	movement data over a range of temporal windows. This procedure allows to capture	as much information as possible on the changes and variations of movement and	ensures the use of the random forest algorithm to its best capacity. The method	is very generic, applicable to any set of data providing movement data together with	observation of behaviour."
"irg",1,1,2,"2019-01-04 11:50","Fits a double logistic function to NDVI time series and calculates              instantaneous rate of green (IRG) according to methods described             in Bischoff et al. (2012) <doi:10.1086/667590>. "
"hablar",4,1,3,"2019-06-09 19:20","Simple tools for converting columns to new data types. Intuitive functions for columns with missing values. "
"grf",0,3,14,"2020-03-12 07:30","A pluggable package for forest-based statistical estimation and inference.    GRF currently provides methods for non-parametric least-squares regression,    quantile regression, survival regression and treatment effect estimation (optionally using instrumental    variables), with support for missing values."
"greta",2,1,4,"2018-10-30 11:00","Write statistical models in R and fit them by MCMC and optimisation on CPUs and GPUs, using Google 'TensorFlow'.  greta lets you write your own model like in BUGS, JAGS and Stan, except that you write models right in R, it scales well to massive datasets, and it’s easy to extend and build on.  See the website for more information, including tutorials, examples, package documentation, and the greta forum."
"drf",0,1,1,"2020-06-22","An implementation of distributional random forests as introduced in Cevid & Michel & Meinshausen & Buhlmann (2020) <arXiv:2005.14458>."
"distill",0,1,2,"2019-05-03 15:10","Scientific and technical article format for the web. 'Distill' articles     feature attractive, reader-friendly typography, flexible layout options    for visualizations, and full support for footnotes and citations."
"deisotoper",1,1,4,"2017-12-19 12:40","Provides a low-level interface for a deisotoper container   implemented in the 'Java' programming language and means of S3 helper   functions for plotting and debugging isotopes of mass spectrometric data.   The deisotoper algorithm detects and aggregates peaks which belong to the   same isotopic cluster of a given mass spectrum. "
"vtree",1,1,6,"2020-01-08 23:30","A tool for calculating and drawing ""variable trees"". Variable trees display information about nested subsets of a data frame."
"umx",0,1,30,"2020-02-06 10:40","Quickly create, run, and report structural equation and twin models.    See '?umx' for help, and umx_open_CRAN_page(""umx"") for NEWS."
"simmer.plot",2,1,15,"2019-03-10 21:30","A set of plotting methods for 'simmer' trajectories and simulations."
"PRISMAstatement",2,1,5,"2018-08-09 00:00","Plot a PRISMA <http://prisma-statement.org/> flow    chart describing the identification, screening, eligibility and    inclusion or studies in systematic reviews. The PRISMA statement    defines an evidence-based, minimal set of items for reporting in    systematic reviews and meta-analyses. PRISMA should be used for the    reporting of studies evaluating randomized clinical trials (RCT), and    is also for reporting on systematic reviews of other types of    research. There is also a function to generate flow charts describing    exclusions and inclusions for any kind of study."
"prismadiagramR",1,1,1,"2020-05-04","Creates 'PRISMA' <http://prisma-statement.org/> diagram from a minimal dataset of included and excluded studies and allows for more custom diagrams. 'PRISMA' diagrams are used to track the identification, screening, eligibility, and inclusion of studies in a systematic review. "
"petrinetR",0,2,3,"2018-07-03 17:50","Functions for the construction of Petri Nets. Petri Nets can be replayed by firing enabled transitions.     Silent transitions will be hidden by the execution handler. Also includes functionalities for the visualization of Petri Nets and     export of Petri Nets to PNML (Petri Net Markup Language) files."
"netSEM",6,1,2,"2018-06-12 12:33","The network structural equation modeling conducts a network statistical analysis    on a data frame of coincident observations of multiple continuous variables [1].     It builds a pathway model by exploring a pool of domain knowledge guided candidate statistical    relationships between each of the variable pairs, selecting the 'best fit' on    the basis of a specific criteria such as adjusted r-squared value.    This work was funded under U. S. Dept. of Energy, Prime Award No. DE-E-0004946,     Award Agreement No. 60220829-51077-T.     [1] Bruckman, Laura S., Nicholas R. Wheeler, Junheng Ma, Ethan Wang, Carl K. Wang, Ivan Chou,     Jiayang Sun, and Roger H. French. (2013) <doi:10.1109/ACCESS.2013.2267611>."
"muir",0,1,1,"2015-05-02","A simple tool allowing users to easily and dynamically explore or document a data set using a tree structure."
"HydeNet",4,1,11,"2020-05-15 15:40","Facilities for easy implementation of hybrid Bayesian networks    using R. Bayesian networks are directed acyclic graphs representing joint    probability distributions, where each node represents a random variable and    each edge represents conditionality. The full joint distribution is therefore    factorized as a product of conditional densities, where each node is assumed    to be independent of its non-descendents given information on its parent nodes.    Since exact, closed-form algorithms are computationally burdensome for inference    within hybrid networks that contain a combination of continuous and discrete    nodes, particle-based approximation techniques like Markov Chain Monte Carlo    are popular. We provide a user-friendly interface to constructing these networks    and running inference using the 'rjags' package. Econometric analyses (maximum    expected utility under competing policies, value of information) involving    decision and utility nodes are also supported."
"heuristicsmineR",0,1,5,"2020-03-19 18:10","Provides the heuristics miner algorithm for process discovery    as proposed by Weijters et al. (2011) <doi:10.1109/CIDM.2011.5949453>. The   algorithm builds a causal net from an event log created with the 'bupaR'    package. Event logs are a set of ordered sequences of events for which    'bupaR' provides the S3 class eventlog(). The discovered causal nets    can be visualised as 'htmlwidgets' and it is possible to annotate them with   the occurrence frequency or processing and waiting time of process    activities.  "
"gSEM",0,1,2,"2015-12-23 16:13","Conducts a semi-gSEM statistical analysis (semi-supervised generalized structural equation modeling) on a data frame of coincident observations of multiple predictive or intermediate variables and a final continuous, outcome variable, via two functions sgSEMp1() and sgSEMp2(), representing fittings based on two statistical principles. Principle 1 determines all sensible univariate relationships in the spirit of the Markovian process. The relationship between each pair of variables, including predictors and the final outcome variable, is determined with the Markovian property that the value of the current predictor is sufficient in relating to the next level variable, i.e., the relationship is independent of the specific value of the preceding-level variables to the current predictor, given the current value. Principle 2 resembles the multiple regression principle in the way multiple predictors are considered simultaneously. Specifically, the relationship of the first-level predictors (such as Time and irradiance etc) to the outcome variable (such as, module degradation or yellowing)  is fit by a supervised additive model. Then each significant intermediate variable is taken as the new outcome variable and the other variables (except the final outcome variable) as the predictors in investigating the next-level multivariate relationship by a supervised additive model. This fitting process is continued until all sensible models are investigated."
"escalation",3,1,3,"2020-04-14 17:10","Methods for working with dose-finding clinical trials. We start by     providing a common interface to various dose-finding methodologies like the    continual reassessment method (CRM) by O'Quigley et al. (1990)     <doi:10.2307/2531628>, the Bayesian optimal interval design (BOIN) by Liu &     Yuan (2015) <doi:10.1111/rssc.12089>, and the 3+3 described by Korn et al.     (1994) <doi:10.1002/sim.4780131802>. We then add optional embellishments to     provide extra desirable behaviour, like avoiding skipping doses, stopping     after n patients have been treated at the recommended dose, or demanding     that n patients are treated before stopping is allowed. By daisy-chaining     together these embellishments using the pipe operator from 'magrittr', it is    simple to tailor the behaviour of dose-finding designs so that they do what     you want. Furthermore, using this flexible interface for creating     dose-finding designs, it is simple to run simulations or calculate     dose-pathways for future cohorts of patients."
"cohorttools",0,1,3,"2020-03-02 12:30","Functions to make lifetables and to calculate hazard function estimate using Poisson regression model with splines. Includes function to draw simple flowchart of cohort study. Function boxesLx() makes boxes of transition rates between states. It utilizes 'Epi' package 'Lexis' data."
"bsem",2,1,1,"2020-08-14","Flexible routines to allow structural equation modeling particular cases using 'rstan' integration. 'bsem' includes Bayesian semi Confirmatory Factor Analysis, Confirmatory Factor Analysis, and Structural Equation Model.  VD Mayrink (2013) <doi:10.1214/12-AOAS607>."
"BayesianFROC",2,1,11,"2020-07-02 12:00","Execute BayesianFROC::fit_GUI_Shiny() (or fit_GUI_Shiny_MRMC()) for a graphical user interface via Shiny.  Provides new methods for the so-called Free-response Receiver Operating Characteristic (FROC) analysis. The ultimate aim of FROC analysis is to compare observer performances, which means comparing characteristics, such as area under the curve (AUC) or figure of merit (FOM). In this package, we only use the notion of AUC for modality comparison, where by ""modality"",  we mean imaging methods such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET), ..., etc. So there is a problem that which imaging method is better to detect lesions from shadows in radiographs. To solve modality comparison issues, this package provides new methods using hierarchical Bayesian models proposed by the author of this package. Using this package, one can obtain at least one conclusion that which imaging methods are better for finding lesions in radiographs with the case of your data. Fitting FROC statistical models is sometimes not so good, it can easily confirm by drawing FROC curves and comparing these curves and the points constructed by False Positive fractions (FPFs) and True Positive Fractions (TPFs), we can validate the goodness of fit intuitively. Such validation is also implemented by the Chi square goodness of fit statistics in the Bayesian context which means that the parameter is not deterministic, thus by integrating it with the posterior predictive measure, we get a desired value. To compare modalities (imaging methods: MRI, CT, PET, ... , etc),  we evaluate AUCs for each modality. FROC is developed by Dev Chakraborty, his FROC model in his 1989 paper relies on the maximal likelihood methodology. The author modified and provided the alternative Bayesian FROC model. Strictly speaking, his model does not coincide with models in this package. In FROC context, we means by multiple reader and multiple case (MRMC) the case of the number of reader or modality is two or more. The MRMC data is available for functions of this package. I hope that medical researchers use not only the frequentist method but also alternative Bayesian methods. In medical research, many problems are considered under only frequentist methods, such as the notion of p-values. But p-value is sometimes misunderstood. Bayesian methods provide very simple, direct, intuitive answer for research questions. Combining frequentist methods with Bayesian methods, we can obtain more reliable answer for research questions. Please execute the following R scripts from the R (R studio) console, demo(demo_MRMC, package = ""BayesianFROC""); demo(demo_srsc, package = ""BayesianFROC""); demo(demo_stan, package = ""BayesianFROC""); demo(demo_drawcurves_srsc, package = ""BayesianFROC""); demo_Bayesian_FROC(); demo_Bayesian_FROC_without_pause(). References: Dev Chakraborty (1989) <doi:10.1118/1.596358> Maximum likelihood analysis of free - response receiver operating characteristic (FROC) data. Pre-print: Issei Tsunoda; Bayesian Models for free-response receiver operating characteristic analysis."
"actel",17,1,3,"2020-08-01 13:10","Designed for studies where animals tagged with acoustic tags are expected    to move through receiver arrays. This package combines the advantages of automatic sorting and checking     of animal movements with the possibility for user intervention on tags that deviate from expected     behaviour. The three analysis functions (explore(), migration() and residency())     allow the users to analyse their data in a systematic way, making it easy to compare results from     different studies.    CJS calculations are based on Perry et al. (2012) <https://www.researchgate.net/publication/256443823_Using_mark-recapture_models_to_estimate_survival_from_telemetry_data>."
"sbfc",0,1,3,"2016-07-16 00:01","An MCMC algorithm for simultaneous feature selection and classification,     and visualization of the selected features and feature interactions.     An implementation of SBFC by Krakovna, Du and Liu (2015), <arXiv:1506.02371>."
"integr",1,1,1,"2019-05-24","Generates a 'Graphviz' graph of the most significant 3-way     interaction gains (i.e. conditional information gains) based on a provided     discrete data frame. Various output formats are supported ('Graphviz', SVG,     PNG, PDF, PS). For references, see the webpage of Aleks Jakulin     <http://stat.columbia.edu/~jakulin/Int/>."
"tmt",1,1,2,"2019-04-01 11:10","Provides conditional maximum likelihood (CML) estimation of item parameters in multistage designs (Zwitser & Maris, 2013, <doi:10.1007/s11336-013-9369-6>) and CML estimation for conventional designs. Additional features are the likelihood ratio test (Andersen, 1973, <doi:10.1007/BF02291180>) and simulation of multistage designs. "
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"RWsearch",4,1,5,"2020-02-15 23:40","Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the whole documentation of packages (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies and CRAN checks. Explore CRAN archive. Use the above functions for task view maintenance. Use quick links and 70 web search engines to explore the web. A lazy evaluation of non-standard content is available throughout the package and eases the use of many functions."
"LexisNexisTools",1,1,7,"2020-01-10 00:00","My PhD supervisor once told me that everyone doing newspaper    analysis starts by writing code to read in files from the 'LexisNexis' newspaper    archive (retrieved e.g., from <http://www.nexis.com/> or any of the partner    sites). However, while this is a nice exercise I do recommend, not everyone has    the time. This package takes files downloaded from the newspaper archive of    'LexisNexis', reads them into R and offers functions for further processing."
"LexisNexisTools",1,1,7,"2020-01-10 00:00","My PhD supervisor once told me that everyone doing newspaper    analysis starts by writing code to read in files from the 'LexisNexis' newspaper    archive (retrieved e.g., from <http://www.nexis.com/> or any of the partner    sites). However, while this is a nice exercise I do recommend, not everyone has    the time. This package takes files downloaded from the newspaper archive of    'LexisNexis', reads them into R and offers functions for further processing."
"LexisNexisTools",1,1,7,"2020-01-10 00:00","My PhD supervisor once told me that everyone doing newspaper    analysis starts by writing code to read in files from the 'LexisNexis' newspaper    archive (retrieved e.g., from <http://www.nexis.com/> or any of the partner    sites). However, while this is a nice exercise I do recommend, not everyone has    the time. This package takes files downloaded from the newspaper archive of    'LexisNexis', reads them into R and offers functions for further processing."
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"VineCopula",0,11,26,"2019-11-26 23:50","Provides tools for the statistical analysis of vine copula models.    The package includes tools for parameter estimation, model selection,    simulation, goodness-of-fit tests, and visualization. Tools for estimation,    selection and exploratory data analysis of bivariate copula models are also    provided."
"ESGtoolkit",0,1,2,"2014-06-13 06:17","A toolkit for Monte Carlo Simulations in Finance, Economics, Insurance, Physics. Multiple simulation models can be created by combining building blocks provided in the package.   "
"texreg",1,20,28,"2020-06-15 14:50","Converts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented. (If the Zelig package, which this package enhances, cannot be found on CRAN, you can find it at <https://github.com/IQSS/Zelig>.)"
"broom",6,158,19,"2020-07-09 14:30","Summarizes key information about statistical    objects in tidy tibbles. This makes it easy to report results, create    plots and consistently work with large numbers of models at once.    Broom provides three verbs that each provide different types of    information about a model. tidy() summarizes information about model    components such as coefficients of a regression. glance() reports    information about an entire model, such as goodness of fit measures    like AIC and BIC. augment() adds information about individual    observations to a dataset, such as fitted values or influence    measures."
"ergMargins",0,1,2,"2019-07-26 13:00","Calculates marginal effects and conducts process analysis in exponential family random graph models (ERGM).    Includes functions to conduct mediation and moderation analyses and to diagnose    multicollinearity.    URL: <http://github.com/sduxbury/ergMargins>.    BugReports: <http://github.com/sduxbury/ergMargins/issues>.    Duxbury, Scott W (2019) <doi:10.31235/osf.io/9bs4u>.    Long, J. Scott, and Sarah Mustillo (2018) <doi:10.1177/0049124118799374>.    Mize, Trenton D. (2019) <doi:10.15195/v6.a4>.    Karlson, Kristian Bernt, Anders Holm, and Richard Breen (2012) <doi:10.1177/0081175012444861>.    Duxbury, Scott W (2018) <doi:10.1177/0049124118782543>."
"texreg",1,20,28,"2020-06-15 14:50","Converts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented. (If the Zelig package, which this package enhances, cannot be found on CRAN, you can find it at <https://github.com/IQSS/Zelig>.)"
"broom",6,158,19,"2020-07-09 14:30","Summarizes key information about statistical    objects in tidy tibbles. This makes it easy to report results, create    plots and consistently work with large numbers of models at once.    Broom provides three verbs that each provide different types of    information about a model. tidy() summarizes information about model    components such as coefficients of a regression. glance() reports    information about an entire model, such as goodness of fit measures    like AIC and BIC. augment() adds information about individual    observations to a dataset, such as fitted values or influence    measures."
"ergMargins",0,1,2,"2019-07-26 13:00","Calculates marginal effects and conducts process analysis in exponential family random graph models (ERGM).    Includes functions to conduct mediation and moderation analyses and to diagnose    multicollinearity.    URL: <http://github.com/sduxbury/ergMargins>.    BugReports: <http://github.com/sduxbury/ergMargins/issues>.    Duxbury, Scott W (2019) <doi:10.31235/osf.io/9bs4u>.    Long, J. Scott, and Sarah Mustillo (2018) <doi:10.1177/0049124118799374>.    Mize, Trenton D. (2019) <doi:10.15195/v6.a4>.    Karlson, Kristian Bernt, Anders Holm, and Richard Breen (2012) <doi:10.1177/0081175012444861>.    Duxbury, Scott W (2018) <doi:10.1177/0049124118782543>."
"texreg",1,20,28,"2020-06-15 14:50","Converts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented. (If the Zelig package, which this package enhances, cannot be found on CRAN, you can find it at <https://github.com/IQSS/Zelig>.)"
"broom",6,158,19,"2020-07-09 14:30","Summarizes key information about statistical    objects in tidy tibbles. This makes it easy to report results, create    plots and consistently work with large numbers of models at once.    Broom provides three verbs that each provide different types of    information about a model. tidy() summarizes information about model    components such as coefficients of a regression. glance() reports    information about an entire model, such as goodness of fit measures    like AIC and BIC. augment() adds information about individual    observations to a dataset, such as fitted values or influence    measures."
"ergMargins",0,1,2,"2019-07-26 13:00","Calculates marginal effects and conducts process analysis in exponential family random graph models (ERGM).    Includes functions to conduct mediation and moderation analyses and to diagnose    multicollinearity.    URL: <http://github.com/sduxbury/ergMargins>.    BugReports: <http://github.com/sduxbury/ergMargins/issues>.    Duxbury, Scott W (2019) <doi:10.31235/osf.io/9bs4u>.    Long, J. Scott, and Sarah Mustillo (2018) <doi:10.1177/0049124118799374>.    Mize, Trenton D. (2019) <doi:10.15195/v6.a4>.    Karlson, Kristian Bernt, Anders Holm, and Richard Breen (2012) <doi:10.1177/0081175012444861>.    Duxbury, Scott W (2018) <doi:10.1177/0049124118782543>."
"texreg",1,20,28,"2020-06-15 14:50","Converts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented. (If the Zelig package, which this package enhances, cannot be found on CRAN, you can find it at <https://github.com/IQSS/Zelig>.)"
"broom",6,158,19,"2020-07-09 14:30","Summarizes key information about statistical    objects in tidy tibbles. This makes it easy to report results, create    plots and consistently work with large numbers of models at once.    Broom provides three verbs that each provide different types of    information about a model. tidy() summarizes information about model    components such as coefficients of a regression. glance() reports    information about an entire model, such as goodness of fit measures    like AIC and BIC. augment() adds information about individual    observations to a dataset, such as fitted values or influence    measures."
"ergMargins",0,1,2,"2019-07-26 13:00","Calculates marginal effects and conducts process analysis in exponential family random graph models (ERGM).    Includes functions to conduct mediation and moderation analyses and to diagnose    multicollinearity.    URL: <http://github.com/sduxbury/ergMargins>.    BugReports: <http://github.com/sduxbury/ergMargins/issues>.    Duxbury, Scott W (2019) <doi:10.31235/osf.io/9bs4u>.    Long, J. Scott, and Sarah Mustillo (2018) <doi:10.1177/0049124118799374>.    Mize, Trenton D. (2019) <doi:10.15195/v6.a4>.    Karlson, Kristian Bernt, Anders Holm, and Richard Breen (2012) <doi:10.1177/0081175012444861>.    Duxbury, Scott W (2018) <doi:10.1177/0049124118782543>."
"texreg",1,20,28,"2020-06-15 14:50","Converts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented. (If the Zelig package, which this package enhances, cannot be found on CRAN, you can find it at <https://github.com/IQSS/Zelig>.)"
"broom",6,158,19,"2020-07-09 14:30","Summarizes key information about statistical    objects in tidy tibbles. This makes it easy to report results, create    plots and consistently work with large numbers of models at once.    Broom provides three verbs that each provide different types of    information about a model. tidy() summarizes information about model    components such as coefficients of a regression. glance() reports    information about an entire model, such as goodness of fit measures    like AIC and BIC. augment() adds information about individual    observations to a dataset, such as fitted values or influence    measures."
"ergMargins",0,1,2,"2019-07-26 13:00","Calculates marginal effects and conducts process analysis in exponential family random graph models (ERGM).    Includes functions to conduct mediation and moderation analyses and to diagnose    multicollinearity.    URL: <http://github.com/sduxbury/ergMargins>.    BugReports: <http://github.com/sduxbury/ergMargins/issues>.    Duxbury, Scott W (2019) <doi:10.31235/osf.io/9bs4u>.    Long, J. Scott, and Sarah Mustillo (2018) <doi:10.1177/0049124118799374>.    Mize, Trenton D. (2019) <doi:10.15195/v6.a4>.    Karlson, Kristian Bernt, Anders Holm, and Richard Breen (2012) <doi:10.1177/0081175012444861>.    Duxbury, Scott W (2018) <doi:10.1177/0049124118782543>."
"BoolFilter",1,1,1,"2017-01-09","Tools for optimal and approximate state estimation as well as    network inference of Partially-Observed Boolean Dynamical Systems."
"BoolFilter",1,1,1,"2017-01-09","Tools for optimal and approximate state estimation as well as    network inference of Partially-Observed Boolean Dynamical Systems."
"BoolFilter",1,1,1,"2017-01-09","Tools for optimal and approximate state estimation as well as    network inference of Partially-Observed Boolean Dynamical Systems."
"aaSEA",1,1,2,"2019-08-01 11:10","Given a protein multiple sequence alignment, it is daunting task to assess the effects of substitutions along sequence length. 'aaSEA' package is intended to help researchers to rapidly analyse property changes caused by single, multiple and correlated amino acid substitutions in proteins. Methods for identification of co-evolving positions from multiple sequence alignment are as described in :  Pelé et al., (2017) <doi:10.4172/2379-1764.1000250>."
"pubmedR",1,1,3,"2020-04-03 08:50","A set of tools to extract bibliographic content from 'PubMed' database using 'NCBI' REST API <https://www.ncbi.nlm.nih.gov/home/develop/api/>."
"dimensionsR",1,1,2,"2020-03-20 11:10","A set of tools to extract bibliographic content from 'Digital Science Dimensions' using 'DSL' API <https://www.dimensions.ai/dimensions-apis/>."
"pubmedR",1,1,3,"2020-04-03 08:50","A set of tools to extract bibliographic content from 'PubMed' database using 'NCBI' REST API <https://www.ncbi.nlm.nih.gov/home/develop/api/>."
"dimensionsR",1,1,2,"2020-03-20 11:10","A set of tools to extract bibliographic content from 'Digital Science Dimensions' using 'DSL' API <https://www.dimensions.ai/dimensions-apis/>."
"bmixture",0,1,14,"2020-05-13 12:50","Provides statistical tools for Bayesian estimation of finite mixture of distributions, mainly mixture of Gamma, Normal and t-distributions. The package is implemented the Bayesian literature for the finite mixture of distributions, including Mohammadi and et al. (2013) <doi:10.1007/s00180-012-0323-3> and Mohammadi and Salehi-Rad (2012) <doi:10.1080/03610918.2011.588358>."
"ssgraph",0,1,13,"2020-05-14 12:10","Bayesian estimation for undirected graphical models using spike-and-slab priors. The package handles continuous, discrete, and mixed data. "
"bmixture",0,1,14,"2020-05-13 12:50","Provides statistical tools for Bayesian estimation of finite mixture of distributions, mainly mixture of Gamma, Normal and t-distributions. The package is implemented the Bayesian literature for the finite mixture of distributions, including Mohammadi and et al. (2013) <doi:10.1007/s00180-012-0323-3> and Mohammadi and Salehi-Rad (2012) <doi:10.1080/03610918.2011.588358>."
"ssgraph",0,1,13,"2020-05-14 12:10","Bayesian estimation for undirected graphical models using spike-and-slab priors. The package handles continuous, discrete, and mixed data. "
"bmixture",0,1,14,"2020-05-13 12:50","Provides statistical tools for Bayesian estimation of finite mixture of distributions, mainly mixture of Gamma, Normal and t-distributions. The package is implemented the Bayesian literature for the finite mixture of distributions, including Mohammadi and et al. (2013) <doi:10.1007/s00180-012-0323-3> and Mohammadi and Salehi-Rad (2012) <doi:10.1080/03610918.2011.588358>."
"ssgraph",0,1,13,"2020-05-14 12:10","Bayesian estimation for undirected graphical models using spike-and-slab priors. The package handles continuous, discrete, and mixed data. "
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"CompareCausalNetworks",0,1,9,"2019-06-18 10:10","Unified interface for the estimation of causal networks, including    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate    additive noise model), 'bivariateCAM' (bivariate causal additive model),    'CAM' (causal additive model) (from package 'CAM'; the package is     temporarily unavailable on the CRAN repository; formerly available versions     can be obtained from the archive), 'hiddenICP' (invariant    causal prediction with hidden variables), 'ICP' (invariant causal prediction)    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC    Algorithm), 'FCI' (fast causal inference),     'RFCI' (really fast causal inference) (all from package 'pcalg') and    regression."
"rattle",1,5,14,"2019-12-16 07:20","The R Analytic Tool To Learn Easily (Rattle) provides a   collection of utilities functions for the data scientist. A  Gnome (RGtk2) based graphical interface is included with   the aim to provide a simple and intuitive introduction to R   for data science, allowing a user to quickly load data from a CSV file   (or via ODBC), transform and explore the data,   build and evaluate models, and export models as PMML (predictive  modelling markup language) or as scores. A key aspect of the GUI  is that all R commands are logged and commented through the log tab.  This can be saved as a standalone R script file and as  an aid for the user to   learn R or to copy-and-paste directly into R itself. "
"arules",1,29,71,"2020-04-04 07:30","Provides the infrastructure for representing,    manipulating and analyzing transaction data and patterns (frequent    itemsets and association rules). Also provides    C implementations of the association mining algorithms Apriori and Eclat.     Hahsler, Gruen and Hornik (2005) <doi:10.18637/jss.v014.i15>."
"TELP",0,1,1,"2016-04-24","Using The Free Evocation of Words Technique method with some functions, this package will make a    social representation and other analysis. The Free Evocation of Words Technique consists of collecting a number of words evoked by a subject facing exposure to an inducer term. The purpose of this technique is to understand the relationships created between words evoked by the individual and the inducer term. This technique is included in the theory of social representations, therefore, on the information transmitted by an individual, seeks to create a profile that define a social group."
"ASSOCShiny",0,1,1,"2019-05-14","An interactive document on  the topic of association rule mining analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyabolar.shinyapps.io/ASSOCShiny/>."
"rattle",1,5,14,"2019-12-16 07:20","The R Analytic Tool To Learn Easily (Rattle) provides a   collection of utilities functions for the data scientist. A  Gnome (RGtk2) based graphical interface is included with   the aim to provide a simple and intuitive introduction to R   for data science, allowing a user to quickly load data from a CSV file   (or via ODBC), transform and explore the data,   build and evaluate models, and export models as PMML (predictive  modelling markup language) or as scores. A key aspect of the GUI  is that all R commands are logged and commented through the log tab.  This can be saved as a standalone R script file and as  an aid for the user to   learn R or to copy-and-paste directly into R itself. "
"arules",1,29,71,"2020-04-04 07:30","Provides the infrastructure for representing,    manipulating and analyzing transaction data and patterns (frequent    itemsets and association rules). Also provides    C implementations of the association mining algorithms Apriori and Eclat.     Hahsler, Gruen and Hornik (2005) <doi:10.18637/jss.v014.i15>."
"TELP",0,1,1,"2016-04-24","Using The Free Evocation of Words Technique method with some functions, this package will make a    social representation and other analysis. The Free Evocation of Words Technique consists of collecting a number of words evoked by a subject facing exposure to an inducer term. The purpose of this technique is to understand the relationships created between words evoked by the individual and the inducer term. This technique is included in the theory of social representations, therefore, on the information transmitted by an individual, seeks to create a profile that define a social group."
"ASSOCShiny",0,1,1,"2019-05-14","An interactive document on  the topic of association rule mining analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyabolar.shinyapps.io/ASSOCShiny/>."
"rattle",1,5,14,"2019-12-16 07:20","The R Analytic Tool To Learn Easily (Rattle) provides a   collection of utilities functions for the data scientist. A  Gnome (RGtk2) based graphical interface is included with   the aim to provide a simple and intuitive introduction to R   for data science, allowing a user to quickly load data from a CSV file   (or via ODBC), transform and explore the data,   build and evaluate models, and export models as PMML (predictive  modelling markup language) or as scores. A key aspect of the GUI  is that all R commands are logged and commented through the log tab.  This can be saved as a standalone R script file and as  an aid for the user to   learn R or to copy-and-paste directly into R itself. "
"arules",1,29,71,"2020-04-04 07:30","Provides the infrastructure for representing,    manipulating and analyzing transaction data and patterns (frequent    itemsets and association rules). Also provides    C implementations of the association mining algorithms Apriori and Eclat.     Hahsler, Gruen and Hornik (2005) <doi:10.18637/jss.v014.i15>."
"TELP",0,1,1,"2016-04-24","Using The Free Evocation of Words Technique method with some functions, this package will make a    social representation and other analysis. The Free Evocation of Words Technique consists of collecting a number of words evoked by a subject facing exposure to an inducer term. The purpose of this technique is to understand the relationships created between words evoked by the individual and the inducer term. This technique is included in the theory of social representations, therefore, on the information transmitted by an individual, seeks to create a profile that define a social group."
"ASSOCShiny",0,1,1,"2019-05-14","An interactive document on  the topic of association rule mining analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyabolar.shinyapps.io/ASSOCShiny/>."
"rattle",1,5,14,"2019-12-16 07:20","The R Analytic Tool To Learn Easily (Rattle) provides a   collection of utilities functions for the data scientist. A  Gnome (RGtk2) based graphical interface is included with   the aim to provide a simple and intuitive introduction to R   for data science, allowing a user to quickly load data from a CSV file   (or via ODBC), transform and explore the data,   build and evaluate models, and export models as PMML (predictive  modelling markup language) or as scores. A key aspect of the GUI  is that all R commands are logged and commented through the log tab.  This can be saved as a standalone R script file and as  an aid for the user to   learn R or to copy-and-paste directly into R itself. "
"arules",1,29,71,"2020-04-04 07:30","Provides the infrastructure for representing,    manipulating and analyzing transaction data and patterns (frequent    itemsets and association rules). Also provides    C implementations of the association mining algorithms Apriori and Eclat.     Hahsler, Gruen and Hornik (2005) <doi:10.18637/jss.v014.i15>."
"TELP",0,1,1,"2016-04-24","Using The Free Evocation of Words Technique method with some functions, this package will make a    social representation and other analysis. The Free Evocation of Words Technique consists of collecting a number of words evoked by a subject facing exposure to an inducer term. The purpose of this technique is to understand the relationships created between words evoked by the individual and the inducer term. This technique is included in the theory of social representations, therefore, on the information transmitted by an individual, seeks to create a profile that define a social group."
"ASSOCShiny",0,1,1,"2019-05-14","An interactive document on  the topic of association rule mining analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyabolar.shinyapps.io/ASSOCShiny/>."
"tigger",1,1,10,"2019-07-19 07:40","Infers the V genotype of an individual from immunoglobulin (Ig)    repertoire sequencing data (AIRR-Seq, Rep-Seq). Includes detection of     any novel alleles. This information is then used to correct existing V     allele calls from among the sample sequences.    Citations:    Gadala-Maria, et al (2015) <doi:10.1073/pnas.1417683112>.    Gadala-Maria, et al (2019) <doi:10.3389/fimmu.2019.00129>."
"scoper",1,1,5,"2020-05-25 21:40","Provides a computational framework for identification of B cell clones from              Adaptive Immune Receptor Repertoire sequencing (AIRR-Seq) data. Three main              functions are included (identicalClones, hierarchicalClones, and spectralClones)              that perform clustering among sequences of BCRs/IGs (B cell receptors/immunoglobulins)              which share the same V gene, J gene and junction length.             Nouri N and Kleinstein SH (2018) <doi:10.1093/bioinformatics/bty235>.             Nouri N and Kleinstein SH (2019) <doi:10.1101/788620>.             Gupta NT, et al. (2017) <doi:10.4049/jimmunol.1601850>."
"rabhit",1,1,4,"2020-01-29 21:20","Infers V-D-J haplotypes and gene deletions from AIRR-seq data,     based on IGHJ, IGHD or IGHV as anchor, by adapting a Bayesian framework.    It also calculates a Bayes factor, a number that indicates the certainty level of the inference, for each haplotyped gene.    Citation:    Gidoni, et al (2019) <doi:10.1038/s41467-019-08489-3>.    Peres and Gidoni, et al (2019) <doi:10.1093/bioinformatics/btz481>."
"tigger",1,1,10,"2019-07-19 07:40","Infers the V genotype of an individual from immunoglobulin (Ig)    repertoire sequencing data (AIRR-Seq, Rep-Seq). Includes detection of     any novel alleles. This information is then used to correct existing V     allele calls from among the sample sequences.    Citations:    Gadala-Maria, et al (2015) <doi:10.1073/pnas.1417683112>.    Gadala-Maria, et al (2019) <doi:10.3389/fimmu.2019.00129>."
"scoper",1,1,5,"2020-05-25 21:40","Provides a computational framework for identification of B cell clones from              Adaptive Immune Receptor Repertoire sequencing (AIRR-Seq) data. Three main              functions are included (identicalClones, hierarchicalClones, and spectralClones)              that perform clustering among sequences of BCRs/IGs (B cell receptors/immunoglobulins)              which share the same V gene, J gene and junction length.             Nouri N and Kleinstein SH (2018) <doi:10.1093/bioinformatics/bty235>.             Nouri N and Kleinstein SH (2019) <doi:10.1101/788620>.             Gupta NT, et al. (2017) <doi:10.4049/jimmunol.1601850>."
"rabhit",1,1,4,"2020-01-29 21:20","Infers V-D-J haplotypes and gene deletions from AIRR-seq data,     based on IGHJ, IGHD or IGHV as anchor, by adapting a Bayesian framework.    It also calculates a Bayes factor, a number that indicates the certainty level of the inference, for each haplotyped gene.    Citation:    Gidoni, et al (2019) <doi:10.1038/s41467-019-08489-3>.    Peres and Gidoni, et al (2019) <doi:10.1093/bioinformatics/btz481>."
"shadow",1,1,21,"2020-06-13 10:20","Functions for calculating: (1) shadow height, (2) logical shadow flag, (3) shadow footprint, (4) Sky View Factor and (5) radiation load. Basic required inputs include a polygonal layer of obstacle outlines along with their heights (i.e. ""extruded polygons""), sun azimuth and sun elevation. The package also provides functions for related preliminary calculations: breaking polygons into line segments, determining azimuth of line segments, shifting segments by azimuth and distance, constructing the footprint of a line-of-sight between an observer and the sun, and creating a 3D grid covering the surface area of extruded polygons."
"shinystan",2,8,8,"2017-08-02 07:43","A graphical user interface for interactive Markov chain Monte    Carlo (MCMC) diagnostics and plots and tables helpful for analyzing a    posterior sample. The interface is powered by the 'Shiny' web    application framework from 'RStudio' and works with the output of MCMC     programs written in any programming language (and has extended     functionality for 'Stan' models fit using the 'rstan' and 'rstanarm'     packages)."
"BNSP",0,1,22,"2020-02-23 14:30","MCMC algorithms & processing functions for: 1. multivariate (and univariate) regression, with nonparametric models for the means, the variances and the correlation matrix, with variable selection, and 2. Dirichlet process mixtures."
"shadow",1,1,21,"2020-06-13 10:20","Functions for calculating: (1) shadow height, (2) logical shadow flag, (3) shadow footprint, (4) Sky View Factor and (5) radiation load. Basic required inputs include a polygonal layer of obstacle outlines along with their heights (i.e. ""extruded polygons""), sun azimuth and sun elevation. The package also provides functions for related preliminary calculations: breaking polygons into line segments, determining azimuth of line segments, shifting segments by azimuth and distance, constructing the footprint of a line-of-sight between an observer and the sun, and creating a 3D grid covering the surface area of extruded polygons."
"shinystan",2,8,8,"2017-08-02 07:43","A graphical user interface for interactive Markov chain Monte    Carlo (MCMC) diagnostics and plots and tables helpful for analyzing a    posterior sample. The interface is powered by the 'Shiny' web    application framework from 'RStudio' and works with the output of MCMC     programs written in any programming language (and has extended     functionality for 'Stan' models fit using the 'rstan' and 'rstanarm'     packages)."
"BNSP",0,1,22,"2020-02-23 14:30","MCMC algorithms & processing functions for: 1. multivariate (and univariate) regression, with nonparametric models for the means, the variances and the correlation matrix, with variable selection, and 2. Dirichlet process mixtures."
"kmeRs",1,1,1,"2018-11-03","Contains tools to calculate similarity score matrix for DNA k-mers. The pairwise            similarity score is calculated using PAM or BLOSUM substitution matrix. The             results are evaluated by similarity score calculated by Needleman-Wunsch             (1970) <doi:10.1016/0022-2836(70)90057-4> global or Smith-Waterman             (1981) <doi:10.1016/0022-2836(81)90087-5> local alignment. Higher similarity            score indicates more similar sequences for BLOSUM and less similar sequences            for PAM matrix; 30, 40, 70, 120, 250 and 62, 45, 50, 62, 80, 100 matrix             versions are available for PAM and BLOSUM, respectively. "
"GenomicMating",0,1,5,"2017-10-02 20:57","Implements the genomic mating approach in the recently published article: Akdemir, D., & Sanchez, J. I. (2016). Efficient Breeding by Genomic Mating. Frontiers in Genetics, 7. <doi:10.3389/fgene.2016.00210>. "
"GenomicMating",0,1,5,"2017-10-02 20:57","Implements the genomic mating approach in the recently published article: Akdemir, D., & Sanchez, J. I. (2016). Efficient Breeding by Genomic Mating. Frontiers in Genetics, 7. <doi:10.3389/fgene.2016.00210>. "
"GenomicMating",0,1,5,"2017-10-02 20:57","Implements the genomic mating approach in the recently published article: Akdemir, D., & Sanchez, J. I. (2016). Efficient Breeding by Genomic Mating. Frontiers in Genetics, 7. <doi:10.3389/fgene.2016.00210>. "
"GenomicMating",0,1,5,"2017-10-02 20:57","Implements the genomic mating approach in the recently published article: Akdemir, D., & Sanchez, J. I. (2016). Efficient Breeding by Genomic Mating. Frontiers in Genetics, 7. <doi:10.3389/fgene.2016.00210>. "
"GenomicMating",0,1,5,"2017-10-02 20:57","Implements the genomic mating approach in the recently published article: Akdemir, D., & Sanchez, J. I. (2016). Efficient Breeding by Genomic Mating. Frontiers in Genetics, 7. <doi:10.3389/fgene.2016.00210>. "
"GenomicMating",0,1,5,"2017-10-02 20:57","Implements the genomic mating approach in the recently published article: Akdemir, D., & Sanchez, J. I. (2016). Efficient Breeding by Genomic Mating. Frontiers in Genetics, 7. <doi:10.3389/fgene.2016.00210>. "
"GenomicMating",0,1,5,"2017-10-02 20:57","Implements the genomic mating approach in the recently published article: Akdemir, D., & Sanchez, J. I. (2016). Efficient Breeding by Genomic Mating. Frontiers in Genetics, 7. <doi:10.3389/fgene.2016.00210>. "
"secrdesign",0,1,16,"2020-01-31 10:00","Tools for designing spatially explicit capture-recapture studies of animal populations. This is primarily a simulation manager for package 'secr'. Extensions in version 2.5.0 include costing and evaluation of detector spacing."
"secrdesign",0,1,16,"2020-01-31 10:00","Tools for designing spatially explicit capture-recapture studies of animal populations. This is primarily a simulation manager for package 'secr'. Extensions in version 2.5.0 include costing and evaluation of detector spacing."
"secrdesign",0,1,16,"2020-01-31 10:00","Tools for designing spatially explicit capture-recapture studies of animal populations. This is primarily a simulation manager for package 'secr'. Extensions in version 2.5.0 include costing and evaluation of detector spacing."
"secrdesign",0,1,16,"2020-01-31 10:00","Tools for designing spatially explicit capture-recapture studies of animal populations. This is primarily a simulation manager for package 'secr'. Extensions in version 2.5.0 include costing and evaluation of detector spacing."
"secrdesign",0,1,16,"2020-01-31 10:00","Tools for designing spatially explicit capture-recapture studies of animal populations. This is primarily a simulation manager for package 'secr'. Extensions in version 2.5.0 include costing and evaluation of detector spacing."
"secrdesign",0,1,16,"2020-01-31 10:00","Tools for designing spatially explicit capture-recapture studies of animal populations. This is primarily a simulation manager for package 'secr'. Extensions in version 2.5.0 include costing and evaluation of detector spacing."
"evoper",0,1,5,"2017-03-14 14:21","The EvoPER, Evolutionary Parameter Estimation for Individual-based Models is an extensible       package providing optimization driven parameter estimation methods using metaheuristics  and       evolutionary computation techniques (Particle Swarm Optimization, Simulated Annealing, Ant Colony Optimization       for continuous domains, Tabu Search, Evolutionary Strategies, ...)  which could be more efficient and require,       in some cases, fewer model evaluations than alternatives relying on experimental design.  Currently there       are built in support for models developed with 'Repast Simphony'  Agent-Based framework (<https://repast.github.io/>)        and with NetLogo (<https://ccl.northwestern.edu/netlogo/>) which are the most used frameworks for Agent-based modeling. "
"OSDR",0,1,3,"2018-10-08 14:40","Provides routines for finding an Optimal System of Distinct Representatives (OSDR), as defined by D.Gale (1968) <doi:10.1016/S0021-9800(68)80039-0>."
"OSDR",0,1,3,"2018-10-08 14:40","Provides routines for finding an Optimal System of Distinct Representatives (OSDR), as defined by D.Gale (1968) <doi:10.1016/S0021-9800(68)80039-0>."
"OSDR",0,1,3,"2018-10-08 14:40","Provides routines for finding an Optimal System of Distinct Representatives (OSDR), as defined by D.Gale (1968) <doi:10.1016/S0021-9800(68)80039-0>."
"OSDR",0,1,3,"2018-10-08 14:40","Provides routines for finding an Optimal System of Distinct Representatives (OSDR), as defined by D.Gale (1968) <doi:10.1016/S0021-9800(68)80039-0>."
"OSDR",0,1,3,"2018-10-08 14:40","Provides routines for finding an Optimal System of Distinct Representatives (OSDR), as defined by D.Gale (1968) <doi:10.1016/S0021-9800(68)80039-0>."
"OSDR",0,1,3,"2018-10-08 14:40","Provides routines for finding an Optimal System of Distinct Representatives (OSDR), as defined by D.Gale (1968) <doi:10.1016/S0021-9800(68)80039-0>."
"CA3variants",0,1,3,"2019-08-01 13:50","Provides four variants of three-way correspondence analysis (ca): three-way symmetrical ca, three-way non-symmetrical ca, three-way ordered symmetrical ca and three-way ordered non-symmetrical ca."
"CA3variants",0,1,3,"2019-08-01 13:50","Provides four variants of three-way correspondence analysis (ca): three-way symmetrical ca, three-way non-symmetrical ca, three-way ordered symmetrical ca and three-way ordered non-symmetrical ca."
"CA3variants",0,1,3,"2019-08-01 13:50","Provides four variants of three-way correspondence analysis (ca): three-way symmetrical ca, three-way non-symmetrical ca, three-way ordered symmetrical ca and three-way ordered non-symmetrical ca."
"CA3variants",0,1,3,"2019-08-01 13:50","Provides four variants of three-way correspondence analysis (ca): three-way symmetrical ca, three-way non-symmetrical ca, three-way ordered symmetrical ca and three-way ordered non-symmetrical ca."
"mlrCPO",8,2,8,"2018-07-03 00:00","Toolset that enriches 'mlr' with a diverse set of preprocessing    operators. Composable Preprocessing Operators (""CPO""s) are first-class    R objects that can be applied to data.frames and 'mlr' ""Task""s to modify    data, can be attached to 'mlr' ""Learner""s to add preprocessing to machine    learning algorithms, and can be composed to form preprocessing pipelines."
"mlr",1,33,21,"2020-01-10 21:00","Interface to a large number of classification and    regression techniques, including machine-readable parameter    descriptions. There is also an experimental extension for survival    analysis, clustering and general, example-specific cost-sensitive    learning. Generic resampling, including cross-validation,    bootstrapping and subsampling. Hyperparameter tuning with modern    optimization techniques, for single- and multi-objective problems.    Filter and wrapper methods for feature selection. Extension of basic    learners with additional operations common in machine learning, also    allowing for easy nested resampling. Most operations can be    parallelized."
"FRESA.CAD",0,1,10,"2018-11-27 09:50","Contains a set of utilities for building and testing statistical models (linear, logistic,ordinal or COX) for Computer Aided Diagnosis/Prognosis applications. Utilities include data adjustment, univariate analysis, model building, model-validation, longitudinal analysis, reporting and visualization."
"fgm",0,1,1,"2019-10-22","Estimates a functional graphical model and a partially separable Karhunen-Loève decomposition for a multivariate Gaussian process. See Zapata J., Oh S. and Petersen A. (2019) <arXiv:1910.03134>."
"pGMGM",0,1,1,"2016-06-28","This is an R and C code implementation of the New-SP and New-JGL method of Gao et al. (2016) <doi:10.1214/16-EJS1135> to perform model-based clustering and multiple graph estimation."
"fgm",0,1,1,"2019-10-22","Estimates a functional graphical model and a partially separable Karhunen-Loève decomposition for a multivariate Gaussian process. See Zapata J., Oh S. and Petersen A. (2019) <arXiv:1910.03134>."
"pGMGM",0,1,1,"2016-06-28","This is an R and C code implementation of the New-SP and New-JGL method of Gao et al. (2016) <doi:10.1214/16-EJS1135> to perform model-based clustering and multiple graph estimation."
"fgm",0,1,1,"2019-10-22","Estimates a functional graphical model and a partially separable Karhunen-Loève decomposition for a multivariate Gaussian process. See Zapata J., Oh S. and Petersen A. (2019) <arXiv:1910.03134>."
"pGMGM",0,1,1,"2016-06-28","This is an R and C code implementation of the New-SP and New-JGL method of Gao et al. (2016) <doi:10.1214/16-EJS1135> to perform model-based clustering and multiple graph estimation."
"DWLasso",0,1,2,"2017-08-25 10:52","Infers networks with hubs using degree weighted Lasso method."
"DWLasso",0,1,2,"2017-08-25 10:52","Infers networks with hubs using degree weighted Lasso method."
"DWLasso",0,1,2,"2017-08-25 10:52","Infers networks with hubs using degree weighted Lasso method."
"DWLasso",0,1,2,"2017-08-25 10:52","Infers networks with hubs using degree weighted Lasso method."
"oneclust",1,1,1,"2020-09-01","Maximum homogeneity clustering algorithm for one-dimensional data    described in W. D. Fisher (1958) <doi:10.1080/01621459.1958.10501479>    via dynamic programming."
"WLasso",1,1,1,"2020-08-13","It proposes a novel variable selection approach taking into account the correlations that may exist between the predictors of the design matrix in a high-dimensional linear model. Our approach consists in rewriting the initial high-dimensional linear model to remove the correlation between the predictors and in applying the generalized Lasso criterion. For further details we refer the reader to the paper <arXiv:2007.10768> (Zhu et al., 2020). "
"oneclust",1,1,1,"2020-09-01","Maximum homogeneity clustering algorithm for one-dimensional data    described in W. D. Fisher (1958) <doi:10.1080/01621459.1958.10501479>    via dynamic programming."
"WLasso",1,1,1,"2020-08-13","It proposes a novel variable selection approach taking into account the correlations that may exist between the predictors of the design matrix in a high-dimensional linear model. Our approach consists in rewriting the initial high-dimensional linear model to remove the correlation between the predictors and in applying the generalized Lasso criterion. For further details we refer the reader to the paper <arXiv:2007.10768> (Zhu et al., 2020). "
"oneclust",1,1,1,"2020-09-01","Maximum homogeneity clustering algorithm for one-dimensional data    described in W. D. Fisher (1958) <doi:10.1080/01621459.1958.10501479>    via dynamic programming."
"WLasso",1,1,1,"2020-08-13","It proposes a novel variable selection approach taking into account the correlations that may exist between the predictors of the design matrix in a high-dimensional linear model. Our approach consists in rewriting the initial high-dimensional linear model to remove the correlation between the predictors and in applying the generalized Lasso criterion. For further details we refer the reader to the paper <arXiv:2007.10768> (Zhu et al., 2020). "
"oneclust",1,1,1,"2020-09-01","Maximum homogeneity clustering algorithm for one-dimensional data    described in W. D. Fisher (1958) <doi:10.1080/01621459.1958.10501479>    via dynamic programming."
"WLasso",1,1,1,"2020-08-13","It proposes a novel variable selection approach taking into account the correlations that may exist between the predictors of the design matrix in a high-dimensional linear model. Our approach consists in rewriting the initial high-dimensional linear model to remove the correlation between the predictors and in applying the generalized Lasso criterion. For further details we refer the reader to the paper <arXiv:2007.10768> (Zhu et al., 2020). "
"secr",2,6,43,"2020-07-14 11:10","Functions to estimate the density and size of a spatially distributed animal population sampled with an array of passive detectors, such as traps, or by searching polygons or transects. Models incorporating distance-dependent detection are fitted by maximizing the likelihood. Tools are included for data manipulation and model selection."
"phylin",2,1,5,"2019-03-13 14:50","The spatial interpolation of genetic distances between	     samples is based on a modified kriging method that	     accepts a genetic distance matrix and generates a map of	     probability of lineage presence. This package also offers	     tools to generate a map of  potential contact zones	     between groups with user-defined thresholds in the tree	     to account for old and recent divergence. Additionally,	     it has functions for IDW interpolation using genetic data	     and midpoints."
"GmAMisc",0,1,3,"2020-02-22 08:20","Contains many functions useful for univariate outlier detection, permutation-based t-test, permutation-based chi-square test, visualization of residuals, and bootstrap 'Cramer V', plotting of the results of the 'Mann-Whitney' and 'Kruskal-Wallis' test, calculation of 'Brainerd-Robinson' similarity coefficient and subsequent clustering, validation of logistic regression models, optimism-corrected AUC, robust 'Bland-Altman' plot, calculation of posterior probability for different chronological relationships between two Bayesian radiocarbon phases, point pattern analysis, clustering of spatial features."
"actel",17,1,3,"2020-08-01 13:10","Designed for studies where animals tagged with acoustic tags are expected    to move through receiver arrays. This package combines the advantages of automatic sorting and checking     of animal movements with the possibility for user intervention on tags that deviate from expected     behaviour. The three analysis functions (explore(), migration() and residency())     allow the users to analyse their data in a systematic way, making it easy to compare results from     different studies.    CJS calculations are based on Perry et al. (2012) <https://www.researchgate.net/publication/256443823_Using_mark-recapture_models_to_estimate_survival_from_telemetry_data>."
"VTrack",0,1,3,"2015-07-04 09:06","Designed to facilitate the assimilation, analysis and synthesis of animal location and movement data collected by the VEMCO suite of acoustic transmitters and receivers. As well as database and geographic information capabilities the principal feature of VTrack is the qualification and identification of ecologically relevant events from the acoustic detection and sensor data. This procedure condenses the acoustic detection database by orders of magnitude, greatly enhancing the synthesis of acoustic detection data."
"samc",7,1,3,"2020-01-08 08:00","An implementation of the framework described in ""Toward a unified    framework for connectivity that disentangles movement and mortality in space    and time"" by Fletcher et al. (2019) <doi:10.1111/ele.13333>.     Incorporates both resistance and absorption with spatial absorbing Markov     chains (SAMC) to provide several short-term and long-term predictions for     metrics related to connectivity in landscapes."
"rWind",1,1,9,"2019-07-24 13:20","Tools for download and manage surface wind and sea currents data from the Global Forecasting System <https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/global-forcast-system-gfs> and to compute connectivity between locations."
"movecost",0,1,5,"2020-05-09 14:00","Provides the facility to calculate non-isotropic accumulated cost surface and least-cost paths using a number of human-movement-related cost functions that can be selected by the user. It just requires a Digital Terrain Model, a start location and (optionally) destination locations. See Alberti (2019) <doi:10.1016/j.softx.2019.100331>."
"marmap",3,3,16,"2018-04-12 12:53","Import xyz data from the NOAA (National Oceanic and Atmospheric Administration, <http://www.noaa.gov>), GEBCO (General Bathymetric Chart of the Oceans, <http://www.gebco.net>) and other sources, plot xyz data to prepare publication-ready figures, analyze xyz data to extract transects, get depth / altitude based on geographical coordinates, or calculate z-constrained least-cost paths."
"leastcostpath",2,1,4,"2020-05-15 07:00","Provides functionality to calculate cost surfaces based on slope (e.g. Herzog, 2010; Llobera and Sluckin, 2007 <doi:10.1016/j.jtbi.2007.07.020>; París Roche, 2002; Tobler, 1993), traversing slope (Bell and Lock, 2000), and landscape features (Llobera, 2000) to be used when modelling pathways and movement potential within a landscape (e.g. Llobera, 2015; Verhagen, 2013; White and Barber, 2012 <doi:10.1016/j.jas.2012.04.017>). "
"gen3sis",4,1,2,"2020-07-10 18:40","Contains an engine for spatially-explicit eco-evolutionary mechanistic models with a modular implementation and several support functions. It allows exploring the consequences of ecological and macroevolutionary processes across realistic or theoretical spatio-temporal landscapes on biodiversity patterns as a general term."
"crawl",0,2,19,"2018-08-25 06:44","Fit continuous-time correlated random walk models with time indexed    covariates to animal telemetry data. The model is fit using the Kalman-filter on    a state space version of the continuous-time stochastic movement process."
"ipdw",1,1,11,"2014-06-07 21:11","Functions are provided to interpolate geo-referenced point data via    Inverse Path Distance Weighting. Useful for coastal marine applications where    barriers in the landscape preclude interpolation with Euclidean distances."
"ctmcmove",0,1,11,"2016-02-20 08:55","Software to facilitates taking movement data in xyt format and pairing it with raster covariates within a continuous time Markov chain (CTMC) framework.  As described in Hanks et al. (2015) <doi:10.1214/14-AOAS803> , this allows flexible modeling of movement in response to covariates (or covariate gradients) with model fitting possible within a Poisson GLM framework. "
"SHT",0,1,4,"2019-12-04 10:20","We provide a collection of statistical hypothesis testing procedures ranging from classical to modern methods for non-trivial settings such as high-dimensional scenario. For the general treatment of statistical hypothesis testing, see the book by Lehmann and Romano (2005) <doi:10.1007/0-387-27605-X>."
"HDtest",0,1,2,"2016-12-30 10:12","High dimensional testing procedures on mean, covariance and white noises."
"SHT",0,1,4,"2019-12-04 10:20","We provide a collection of statistical hypothesis testing procedures ranging from classical to modern methods for non-trivial settings such as high-dimensional scenario. For the general treatment of statistical hypothesis testing, see the book by Lehmann and Romano (2005) <doi:10.1007/0-387-27605-X>."
"HDtest",0,1,2,"2016-12-30 10:12","High dimensional testing procedures on mean, covariance and white noises."
"SHT",0,1,4,"2019-12-04 10:20","We provide a collection of statistical hypothesis testing procedures ranging from classical to modern methods for non-trivial settings such as high-dimensional scenario. For the general treatment of statistical hypothesis testing, see the book by Lehmann and Romano (2005) <doi:10.1007/0-387-27605-X>."
"HDtest",0,1,2,"2016-12-30 10:12","High dimensional testing procedures on mean, covariance and white noises."
"deducorrect",1,1,20,"2014-10-01 11:03","A collection of methods for automated data cleaning where all actions are logged."
"deducorrect",1,1,20,"2014-10-01 11:03","A collection of methods for automated data cleaning where all actions are logged."
"deducorrect",1,1,20,"2014-10-01 11:03","A collection of methods for automated data cleaning where all actions are logged."
"deducorrect",1,1,20,"2014-10-01 11:03","A collection of methods for automated data cleaning where all actions are logged."
"BoltzMM",0,1,4,"2019-01-10 08:40","Provides probability computation, data generation,    and model estimation for fully-visible Boltzmann machines. It follows the methods    described in Nguyen and Wood (2016a) <doi:10.1162/NECO_a_00813>     and Nguyen and Wood (2016b) <doi:10.1109/TNNLS.2015.2425898>. "
"TGS",1,1,2,"2018-11-16 17:10","Rapid advancements in high-throughput gene sequencing    technologies have resulted in genome-scale time-series datasets.     Uncovering the underlying temporal sequence of gene regulatory events     in the form of time-varying gene regulatory networks demands     accurate and computationally efficient algorithms. Such an    algorithm is 'TGS'. It is proposed in Saptarshi Pyne, Alok Ranjan     Kumar, and Ashish Anand. Rapid reconstruction of time-varying     gene regulatory networks. IEEE/ACM Transactions on Computational     Biology and Bioinformatics, 17(1):278{291, Jan-Feb 2020. The TGS     algorithm is shown to consume only 29 minutes for a microarray     dataset with 4028 genes. This package provides an implementation     of the TGS algorithm and its variants."
"BoltzMM",0,1,4,"2019-01-10 08:40","Provides probability computation, data generation,    and model estimation for fully-visible Boltzmann machines. It follows the methods    described in Nguyen and Wood (2016a) <doi:10.1162/NECO_a_00813>     and Nguyen and Wood (2016b) <doi:10.1109/TNNLS.2015.2425898>. "
"TGS",1,1,2,"2018-11-16 17:10","Rapid advancements in high-throughput gene sequencing    technologies have resulted in genome-scale time-series datasets.     Uncovering the underlying temporal sequence of gene regulatory events     in the form of time-varying gene regulatory networks demands     accurate and computationally efficient algorithms. Such an    algorithm is 'TGS'. It is proposed in Saptarshi Pyne, Alok Ranjan     Kumar, and Ashish Anand. Rapid reconstruction of time-varying     gene regulatory networks. IEEE/ACM Transactions on Computational     Biology and Bioinformatics, 17(1):278{291, Jan-Feb 2020. The TGS     algorithm is shown to consume only 29 minutes for a microarray     dataset with 4028 genes. This package provides an implementation     of the TGS algorithm and its variants."
"optpart",0,1,6,"2016-09-20 18:46","Contains a set of algorithms for creating        partitions and coverings of objects largely based on operations        on (dis)similarity relations (or matrices). There are several        iterative re-assignment algorithms optimizing different        goodness-of-clustering criteria.  In addition, there are        covering algorithms 'clique' which derives maximal cliques, and        'maxpact' which creates a covering of maximally compact sets.        Graphical analyses and conversion routines are also included."
"fso",0,1,4,"2013-02-26 17:21","Fuzzy set ordination is a multivariate analysis used in ecology to    relate the composition of samples to possible explanatory variables.  While    differing in theory and method, in practice, the use is similar to 'constrained    ordination.'  The package contains plotting and summary functions as well as    the analyses.  "
"optpart",0,1,6,"2016-09-20 18:46","Contains a set of algorithms for creating        partitions and coverings of objects largely based on operations        on (dis)similarity relations (or matrices). There are several        iterative re-assignment algorithms optimizing different        goodness-of-clustering criteria.  In addition, there are        covering algorithms 'clique' which derives maximal cliques, and        'maxpact' which creates a covering of maximally compact sets.        Graphical analyses and conversion routines are also included."
"fso",0,1,4,"2013-02-26 17:21","Fuzzy set ordination is a multivariate analysis used in ecology to    relate the composition of samples to possible explanatory variables.  While    differing in theory and method, in practice, the use is similar to 'constrained    ordination.'  The package contains plotting and summary functions as well as    the analyses.  "
"optpart",0,1,6,"2016-09-20 18:46","Contains a set of algorithms for creating        partitions and coverings of objects largely based on operations        on (dis)similarity relations (or matrices). There are several        iterative re-assignment algorithms optimizing different        goodness-of-clustering criteria.  In addition, there are        covering algorithms 'clique' which derives maximal cliques, and        'maxpact' which creates a covering of maximally compact sets.        Graphical analyses and conversion routines are also included."
"fso",0,1,4,"2013-02-26 17:21","Fuzzy set ordination is a multivariate analysis used in ecology to    relate the composition of samples to possible explanatory variables.  While    differing in theory and method, in practice, the use is similar to 'constrained    ordination.'  The package contains plotting and summary functions as well as    the analyses.  "
"LICORS",0,3,4,"2012-11-08 07:05","Estimates predictive states from spatio-temporal data and    consequently can provide provably optimal forecasts.    Currently this implementation    supports an N-dimensional spatial grid observed over equally spaced time    intervals. E.g. a video is a 2D spatial systems observed over time. This    package implements mixed LICORS, has plotting tools (for (1+1)D and (2+1)D    systems), and methods for optimal forecasting.  Due to memory limitations    it is recommend to only analyze (1+1)D systems."
"iml",2,3,12,"2020-03-26 11:20","Interpretability methods to analyze the behavior    and predictions of any machine learning model.  Implemented methods    are: Feature importance described by Fisher et al. (2018)    <arXiv:1801.01489>, accumulated local effects plots described by Apley    (2018) <arXiv:1612.08468>, partial dependence plots described by    Friedman (2001) <www.jstor.org/stable/2699986>, individual    conditional expectation ('ice') plots described by Goldstein et al.    (2013) <doi:10.1080/10618600.2014.907095>, local models (variant of    'lime') described by Ribeiro et. al (2016) <arXiv:1602.04938>, the    Shapley Value described by Strumbelj et. al (2014)    <doi:10.1007/s10115-013-0679-x>, feature interactions described by    Friedman et. al <doi:10.1214/07-AOAS148> and tree surrogate models."
"stlplus",0,2,1,"2016-01-06","Decompose a time series into seasonal, trend, and remainder    components using an implementation of Seasonal Decomposition of Time    Series by Loess (STL) that provides several enhancements over the STL    method in the stats package.  These enhancements include handling missing    values, providing higher order (quadratic) loess smoothing with automated    parameter choices, frequency component smoothing beyond the seasonal and    trend components, and some basic plot methods for diagnostics."
"neuroim",3,1,3,"2015-07-02 16:07","A collection of data structures that represent    volumetric brain imaging data. The focus is on basic data handling for 3D    and 4D neuroimaging data. In addition, there are function to read and write    NIFTI files and limited support for reading AFNI files."
"NCSampling",0,1,1,"2017-06-27","Provides functionality for performing Nearest Centroid (NC) Sampling. The NC sampling procedure was developed for forestry applications and selects plots for ground measurement so as to maximize the efficiency of imputation estimates. It uses multiple auxiliary variables and multivariate clustering to search for an optimal sample. Further details are given in Melville G. & Stone C. (2016) <doi:10.1080/00049158.2016.1218265>. "
"ALEPlot",1,2,2,"2017-11-13 12:25","Visualizes the main effects of individual predictor variables and their second-order interaction effects in black-box supervised learning models. The package creates either Accumulated Local Effects (ALE) plots and/or Partial Dependence (PD) plots, given a fitted supervised learning model."
"intrinsicDimension",1,1,2,"2017-11-26 17:42","A variety of methods for estimating intrinsic dimension of data sets (i.e the manifold or Hausdorff dimension of the support of the distribution that generated the data) as reviewed in Johnsson, K. (2016, ISBN:978-91-7623-921-6) and Johnsson, K., Soneson, C. and Fontes, M. (2015) <doi:10.1109/TPAMI.2014.2343220>. Furthermore, to evaluate the performance of these estimators, functions for generating data sets with given intrinsic dimensions are provided."
"seqhandbook",1,1,1,"2020-06-29","It provides miscellaneous sequence analysis functions for describing episodes in individual sequences, measuring association between domains in multidimensional sequence analysis (see Piccarreta (2017) <doi:10.1177/0049124115591013>), heat maps of sequence data, Globally Interdependent Multidimensional Sequence Analysis (see Robette et al (2015) <doi:10.1177/0081175015570976>), smoothing sequences for index plots (see Piccarreta (2012) <doi:10.1177/0049124112452394>), coding sequences for Qualitative Harmonic Analysis (see Deville (1982)), measuring stress from multidimensional scaling factors (see Piccarreta and Lior (2010) <doi:10.1111/j.1467-985X.2009.00606.x>), symmetrical (or canonical) Partial Least Squares (see Bry (1996))."
"GDAtools",0,3,5,"2017-03-07 14:46","Contains functions for 'specific' Multiple Correspondence Analysis, 	Class Specific Analysis, Multiple Factor Analysis, 'standardized' MCA, computing and plotting structuring factors and concentration ellipses, 	inductive tests and others tools for Geometric Data Analysis (Le Roux & Rouanet (2005) <doi:10.1007/1-4020-2236-0>). It also provides functions	for the translation of logit models coefficients into percentages (Deauvieau (2010) <doi:10.1177/0759106309352586>), weighted contingency tables, an association   measure for contingency tables (""Percentages of Maximum Deviation from Independence"", aka PEM, see Cibois (1993) <doi:10.1177/075910639304000103>) and some tools to measure bivariate associations between variables  (phi, Cram<c3><a9>r V, correlation coefficient, eta-squared...)."
"MEDseq",1,1,4,"2020-03-31 17:40","Implements a model-based clustering method for categorical life-course sequences relying on mixtures of exponential-distance models introduced by Murphy et al. (2019) <arXiv:1908.07963>. A range of flexible precision parameter settings corresponding to weighted generalisations of the Hamming distance metric are considered, along with the potential inclusion of a noise component. Gating covariates can be supplied in order to relate sequences to baseline characteristics. Sampling weights are also accommodated. The models are fitted using the EM algorithm and tools for visualising the results are also provided."
"ClusterStability",0,1,4,"2015-10-14 23:41","Allows one to assess the stability of individual objects, clusters     and whole clustering solutions based on repeated runs of the K-means and K-medoids     partitioning algorithms."
"taxlist",1,1,10,"2020-04-29 19:20","Handling taxonomic lists through objects of class 'taxlist'.    This package provides functions to import species lists from 'Turboveg'    (<https://www.synbiosys.alterra.nl/turboveg>) and the possibility to create    backups from resulting R-objects.    Also quick displays are implemented as summary-methods."
"taxlist",1,1,10,"2020-04-29 19:20","Handling taxonomic lists through objects of class 'taxlist'.    This package provides functions to import species lists from 'Turboveg'    (<https://www.synbiosys.alterra.nl/turboveg>) and the possibility to create    backups from resulting R-objects.    Also quick displays are implemented as summary-methods."
"rerddap",2,3,10,"2019-07-01 22:20","General purpose R client for 'ERDDAP' servers. Includes    functions to search for 'datasets', get summary information on    'datasets', and fetch 'datasets', in either 'csv' or 'netCDF' format.    'ERDDAP' information:     <https://upwell.pfeg.noaa.gov/erddap/information.html>."
"rnoaa",10,2,22,"2020-06-13 06:50","Client for many 'NOAA' data sources including the 'NCDC' climate    'API' at <https://www.ncdc.noaa.gov/cdo-web/webservices/v2>, with functions for    each of the 'API' 'endpoints': data, data categories, data sets, data types,    locations, location categories, and stations. In addition, we have an interface    for 'NOAA' sea ice data, the 'NOAA' severe weather inventory, 'NOAA' Historical    Observing 'Metadata' Repository ('HOMR') data, 'NOAA' storm data via 'IBTrACS',    tornado data via the 'NOAA' storm prediction center, and more."
"rbison",2,1,12,"2019-05-03 22:51","Interface to the 'USGS' 'BISON' (<https://bison.usgs.gov/>)    API, a 'database' for species occurrence data. Data comes from    species in the United States from participating data providers. You can get    data via 'taxonomic' and location based queries. A simple function    is provided to help visualize data."
"mapr",1,1,5,"2018-03-21 18:27","Utilities for visualizing species occurrence data. Includes    functions to visualize occurrence data from 'spocc', 'rgbif',    and other packages. Mapping options included for base R plots, 'ggplot2',    'ggmap', 'leaflet' and 'GitHub' 'gists'."
"camtrapR",4,1,23,"2020-04-23 02:14","Management of and data extraction from camera trap data in wildlife studies. The package provides a workflow for storing and sorting camera trap photos (and videos), tabulates records of species and individuals, and creates detection/non-detection matrices for occupancy and spatial capture-recapture analyses with great flexibility. In addition, it can visualise species activity data and provides simple mapping functions with GIS export."
"BIOMASS",2,1,8,"2019-03-26 09:56","Contains functions to estimate aboveground biomass/carbon and its uncertainty in tropical forests. 	These functions allow to (1) retrieve and to correct taxonomy, (2) estimate wood density and its uncertainty, 	(3) construct height-diameter models, (4) manage tree and plot coordinates, 	(5) estimate the aboveground biomass/carbon at the stand level with associated uncertainty. 	To cite BIOMASS, please use citation(""BIOMASS""). 	See more in the article of Réjou-Méchain et al. (2017) <doi:10.1111/2041-210X.12753>."
"traits",2,1,7,"2019-06-29 07:00","Species trait data from many different sources, including    sequence data from 'NCBI' (<https://www.ncbi.nlm.nih.gov/>),    plant trait data from 'BETYdb', data from 'EOL' 'Traitbank',    'Birdlife' International, and more."
"traitdataform",1,1,6,"2019-10-01 20:20","Assistance for handling ecological trait data and applying the     Ecological Trait-Data Standard terminology (Schneider et al. 2019    <doi:10.1111/2041-210X.13288>). There are two major use cases: (1) preparation of    own trait datasets for upload into public data bases, and (2) harmonizing    trait datasets from different sources by re-formatting them into a unified    format. See 'traitdataform' website for full documentation. "
"TR8",3,1,12,"2014-10-28 08:02","Plant ecologists often need to collect ""traits"" data    about plant species which are often scattered among various    databases: TR8 contains a set of tools which take care of    automatically retrieving some of those functional traits data    for plant species from publicly available databases (Biolflor,    The Ecological Flora of the British Isles, LEDA traitbase, Ellenberg    values for Italian Flora, Mycorrhizal intensity databases, Catminat, BROT,    PLANTS, Jepson Flora Project).    The TR8 name, inspired by ""car plates"" jokes, was chosen since    it both reminds of the main object of the package and is    extremely short to type."
"taxotools",0,1,12,"2019-07-26 01:00","Some tools to work with taxonomic name lists. "
"taxlist",1,1,10,"2020-04-29 19:20","Handling taxonomic lists through objects of class 'taxlist'.    This package provides functions to import species lists from 'Turboveg'    (<https://www.synbiosys.alterra.nl/turboveg>) and the possibility to create    backups from resulting R-objects.    Also quick displays are implemented as summary-methods."
"taxa",1,2,7,"2020-02-25 07:40","Provides taxonomic classes for    groupings of taxonomic names without data, and those    with data. Methods provided are ""taxonomically aware"", in    that they know about ordering of ranks, and methods that    filter based on taxonomy also filter associated data.     This package is described in the publication: ""Taxa: An R     package implementing data standards and methods for     taxonomic data"", Zachary S.L. Foster, Scott Chamberlain,      Niklaus J. Grünwald (2018) <doi:10.12688/f1000research.14013.2>."
"rusda",0,1,2,"2015-11-13 10:20","An interface to the web service methods provided by the United States Department of Agriculture (USDA). The Agricultural Research Service (ARS) provides a large set of databases. The current version of the package holds interfaces to the Systematic Mycology and Microbiology Laboratory (SMML), which consists of four databases: Fungus-Host Distributions, Specimens, Literature and the Nomenclature database. It provides functions for querying these databases. The main function is \code{associations}, which allows searching for fungus-host combinations. "
"originr",0,1,4,"2018-05-01 00:05","Get species origin data (whether species is native/invasive) from the    following sources on the web: Encyclopedia of Life (<http://eol.org>), Flora    'Europaea' (<http://rbg-web2.rbge.org.uk/FE/fe.html>), Global Invasive Species    Database (<http://www.iucngisd.org/gisd>), the Native Species Resolver    (<https://bien.nceas.ucsb.edu/bien/tools/nsr/>), Integrated Taxonomic    Information Service (<https://www.itis.gov/>), and Global Register of    Introduced and Invasive Species (<http://www.griis.org/>)."
"myTAI",6,1,14,"2019-03-11 00:10","Investigate the evolution of biological processes by capturing evolutionary signatures in transcriptomes (Drost et al. (2017) <doi:10.1093/bioinformatics/btx835>). The aim of this tool is to provide a transcriptome analysis environment for answering questions regarding the evolution of biological processes (Drost et al. (2016) <doi:10.1101/051565>)."
"brranching",1,1,6,"2019-07-27 06:30","Includes methods for fetching 'phylogenies' from a variety    of sources, including the 'Phylomatic' web service     (<http://phylodiversity.net/phylomatic>), and 'Phylocom'     (<https://github.com/phylocom/phylocom/>)."
"bdvis",0,1,10,"2018-02-13 05:30","Provides a set of functions to create basic visualizations to quickly    preview different aspects of biodiversity information such as inventory     completeness, extent of coverage (taxonomic, temporal and geographic), gaps    and biases."
"algaeClassify",0,1,2,"2019-05-20 10:51","Verify accepted taxonomic nomenclature of phytoplankton species, assign 	species to functional group classifications, and manipulate taxonomic and functional diversity 	data. Possible functional classifications include Morpho-functional group (MFG; 	Salmaso et al. 2015 <doi:10.1111/fwb.12520>) and CSR (Reynolds 1988; Functional morphology and 	the adaptive strategies of phytoplankton. In C.D. Sandgren (ed). Growth and reproductive 	strategies of freshwater phytoplankton, 388-433. Cambridge University Press, New York). 	Version 1.2.0 also includes functions to query names using the algaebase online taxonomic 	database (<https://www.algaebase.org>; <doi:10.7872/crya.v35.iss2.2014.105>).	The algaeClassify package is a product of the GEISHA (Global Evaluation of the Impacts of 	Storms on freshwater Habitat and Structure of phytoplankton Assemblages), funded by CESAB     (Centre for Synthesis and Analysis of Biodiversity) and the USGS John Wesley Powell Center for	Synthesis and Analysis, with data and other support provided by members of GLEON 	(Global Lake Ecology Observation Network). This software is preliminary or provisional and	is subject to revision. It is being provided to meet the need for timely best science. 	The software has not received final approval by the U.S. Geological Survey (USGS). 	No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the 	functionality of the software and related material nor shall the fact of release constitute 	any such warranty. The software is provided on the condition that neither the USGS nor the U.S. 	Government shall be held liable for any damages resulting from the authorized or unauthorized 	use of the software."
"rerddap",2,3,10,"2019-07-01 22:20","General purpose R client for 'ERDDAP' servers. Includes    functions to search for 'datasets', get summary information on    'datasets', and fetch 'datasets', in either 'csv' or 'netCDF' format.    'ERDDAP' information:     <https://upwell.pfeg.noaa.gov/erddap/information.html>."
"rnoaa",10,2,22,"2020-06-13 06:50","Client for many 'NOAA' data sources including the 'NCDC' climate    'API' at <https://www.ncdc.noaa.gov/cdo-web/webservices/v2>, with functions for    each of the 'API' 'endpoints': data, data categories, data sets, data types,    locations, location categories, and stations. In addition, we have an interface    for 'NOAA' sea ice data, the 'NOAA' severe weather inventory, 'NOAA' Historical    Observing 'Metadata' Repository ('HOMR') data, 'NOAA' storm data via 'IBTrACS',    tornado data via the 'NOAA' storm prediction center, and more."
"rbison",2,1,12,"2019-05-03 22:51","Interface to the 'USGS' 'BISON' (<https://bison.usgs.gov/>)    API, a 'database' for species occurrence data. Data comes from    species in the United States from participating data providers. You can get    data via 'taxonomic' and location based queries. A simple function    is provided to help visualize data."
"mapr",1,1,5,"2018-03-21 18:27","Utilities for visualizing species occurrence data. Includes    functions to visualize occurrence data from 'spocc', 'rgbif',    and other packages. Mapping options included for base R plots, 'ggplot2',    'ggmap', 'leaflet' and 'GitHub' 'gists'."
"camtrapR",4,1,23,"2020-04-23 02:14","Management of and data extraction from camera trap data in wildlife studies. The package provides a workflow for storing and sorting camera trap photos (and videos), tabulates records of species and individuals, and creates detection/non-detection matrices for occupancy and spatial capture-recapture analyses with great flexibility. In addition, it can visualise species activity data and provides simple mapping functions with GIS export."
"BIOMASS",2,1,8,"2019-03-26 09:56","Contains functions to estimate aboveground biomass/carbon and its uncertainty in tropical forests. 	These functions allow to (1) retrieve and to correct taxonomy, (2) estimate wood density and its uncertainty, 	(3) construct height-diameter models, (4) manage tree and plot coordinates, 	(5) estimate the aboveground biomass/carbon at the stand level with associated uncertainty. 	To cite BIOMASS, please use citation(""BIOMASS""). 	See more in the article of Réjou-Méchain et al. (2017) <doi:10.1111/2041-210X.12753>."
"traits",2,1,7,"2019-06-29 07:00","Species trait data from many different sources, including    sequence data from 'NCBI' (<https://www.ncbi.nlm.nih.gov/>),    plant trait data from 'BETYdb', data from 'EOL' 'Traitbank',    'Birdlife' International, and more."
"traitdataform",1,1,6,"2019-10-01 20:20","Assistance for handling ecological trait data and applying the     Ecological Trait-Data Standard terminology (Schneider et al. 2019    <doi:10.1111/2041-210X.13288>). There are two major use cases: (1) preparation of    own trait datasets for upload into public data bases, and (2) harmonizing    trait datasets from different sources by re-formatting them into a unified    format. See 'traitdataform' website for full documentation. "
"TR8",3,1,12,"2014-10-28 08:02","Plant ecologists often need to collect ""traits"" data    about plant species which are often scattered among various    databases: TR8 contains a set of tools which take care of    automatically retrieving some of those functional traits data    for plant species from publicly available databases (Biolflor,    The Ecological Flora of the British Isles, LEDA traitbase, Ellenberg    values for Italian Flora, Mycorrhizal intensity databases, Catminat, BROT,    PLANTS, Jepson Flora Project).    The TR8 name, inspired by ""car plates"" jokes, was chosen since    it both reminds of the main object of the package and is    extremely short to type."
"taxotools",0,1,12,"2019-07-26 01:00","Some tools to work with taxonomic name lists. "
"taxlist",1,1,10,"2020-04-29 19:20","Handling taxonomic lists through objects of class 'taxlist'.    This package provides functions to import species lists from 'Turboveg'    (<https://www.synbiosys.alterra.nl/turboveg>) and the possibility to create    backups from resulting R-objects.    Also quick displays are implemented as summary-methods."
"taxa",1,2,7,"2020-02-25 07:40","Provides taxonomic classes for    groupings of taxonomic names without data, and those    with data. Methods provided are ""taxonomically aware"", in    that they know about ordering of ranks, and methods that    filter based on taxonomy also filter associated data.     This package is described in the publication: ""Taxa: An R     package implementing data standards and methods for     taxonomic data"", Zachary S.L. Foster, Scott Chamberlain,      Niklaus J. Grünwald (2018) <doi:10.12688/f1000research.14013.2>."
"rusda",0,1,2,"2015-11-13 10:20","An interface to the web service methods provided by the United States Department of Agriculture (USDA). The Agricultural Research Service (ARS) provides a large set of databases. The current version of the package holds interfaces to the Systematic Mycology and Microbiology Laboratory (SMML), which consists of four databases: Fungus-Host Distributions, Specimens, Literature and the Nomenclature database. It provides functions for querying these databases. The main function is \code{associations}, which allows searching for fungus-host combinations. "
"originr",0,1,4,"2018-05-01 00:05","Get species origin data (whether species is native/invasive) from the    following sources on the web: Encyclopedia of Life (<http://eol.org>), Flora    'Europaea' (<http://rbg-web2.rbge.org.uk/FE/fe.html>), Global Invasive Species    Database (<http://www.iucngisd.org/gisd>), the Native Species Resolver    (<https://bien.nceas.ucsb.edu/bien/tools/nsr/>), Integrated Taxonomic    Information Service (<https://www.itis.gov/>), and Global Register of    Introduced and Invasive Species (<http://www.griis.org/>)."
"myTAI",6,1,14,"2019-03-11 00:10","Investigate the evolution of biological processes by capturing evolutionary signatures in transcriptomes (Drost et al. (2017) <doi:10.1093/bioinformatics/btx835>). The aim of this tool is to provide a transcriptome analysis environment for answering questions regarding the evolution of biological processes (Drost et al. (2016) <doi:10.1101/051565>)."
"brranching",1,1,6,"2019-07-27 06:30","Includes methods for fetching 'phylogenies' from a variety    of sources, including the 'Phylomatic' web service     (<http://phylodiversity.net/phylomatic>), and 'Phylocom'     (<https://github.com/phylocom/phylocom/>)."
"bdvis",0,1,10,"2018-02-13 05:30","Provides a set of functions to create basic visualizations to quickly    preview different aspects of biodiversity information such as inventory     completeness, extent of coverage (taxonomic, temporal and geographic), gaps    and biases."
"algaeClassify",0,1,2,"2019-05-20 10:51","Verify accepted taxonomic nomenclature of phytoplankton species, assign 	species to functional group classifications, and manipulate taxonomic and functional diversity 	data. Possible functional classifications include Morpho-functional group (MFG; 	Salmaso et al. 2015 <doi:10.1111/fwb.12520>) and CSR (Reynolds 1988; Functional morphology and 	the adaptive strategies of phytoplankton. In C.D. Sandgren (ed). Growth and reproductive 	strategies of freshwater phytoplankton, 388-433. Cambridge University Press, New York). 	Version 1.2.0 also includes functions to query names using the algaebase online taxonomic 	database (<https://www.algaebase.org>; <doi:10.7872/crya.v35.iss2.2014.105>).	The algaeClassify package is a product of the GEISHA (Global Evaluation of the Impacts of 	Storms on freshwater Habitat and Structure of phytoplankton Assemblages), funded by CESAB     (Centre for Synthesis and Analysis of Biodiversity) and the USGS John Wesley Powell Center for	Synthesis and Analysis, with data and other support provided by members of GLEON 	(Global Lake Ecology Observation Network). This software is preliminary or provisional and	is subject to revision. It is being provided to meet the need for timely best science. 	The software has not received final approval by the U.S. Geological Survey (USGS). 	No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the 	functionality of the software and related material nor shall the fact of release constitute 	any such warranty. The software is provided on the condition that neither the USGS nor the U.S. 	Government shall be held liable for any damages resulting from the authorized or unauthorized 	use of the software."
"VdgRsm",0,1,5,"2015-02-01 09:04","Functions for creating variance dispersion graphs, fraction of design space plots, and contour plots of scaled prediction variances for second-order response surface designs in spherical and cuboidal regions. Also, some standard response surface designs can be generated."
"rankdist",0,1,8,"2018-04-08 12:39","Implements distance based probability models for ranking data.     The supported distance metrics include Kendall distance, Spearman distance, Footrule distance, Hamming distance,    Weighted-tau distance and Weighted Kendall distance.    Phi-component model and mixture models are also supported."
"randomizationInference",0,1,4,"2014-09-20 22:04","Allows the user to conduct randomization-based inference for a wide variety of experimental scenarios. The package leverages a potential outcomes framework to output randomization-based p-values and null intervals for test statistics geared toward any estimands of interest, according to the specified null and alternative hypotheses. Users can define custom randomization schemes so that the randomization distributions are accurate for their experimental settings. The package also creates visualizations of randomization distributions and can test multiple test statistics simultaneously."
"pRF",0,1,3,"2015-11-05 08:31","Estimate False Discovery Rates (FDRs) for importance metrics from    random forest runs."
"permuco",1,1,4,"2019-01-25 10:40","Functions to compute p-values based on permutation tests. Regression, ANOVA and ANCOVA, omnibus F-tests, marginal unilateral and bilateral t-tests are available. Several methods to handle nuisance variables are implemented (Kherad-Pajouh, S., & Renaud, O. (2010) <doi:10.1016/j.csda.2010.02.015> ; Kherad-Pajouh, S., & Renaud, O. (2014) <doi:10.1007/s00362-014-0617-3> ; Winkler, A. M., Ridgway, G. R., Webster, M. A., Smith, S. M., & Nichols, T. E. (2014) <doi:10.1016/j.neuroimage.2014.01.060>). An extension for the comparison of signals issued from experimental conditions (e.g. EEG/ERP signals) is provided. Several corrections for multiple testing are possible, including the cluster-mass statistic (Maris, E., & Oostenveld, R. (2007) <doi:10.1016/j.jneumeth.2007.03.024>) and the threshold-free cluster enhancement (Smith, S. M., & Nichols, T. E. (2009) <doi:10.1016/j.neuroimage.2008.03.061>). "
"npcure",0,1,3,"2019-06-17 11:10","Performs nonparametric estimation in mixture cure models, and significance tests for the cure probability. For details, see López-Cheda et al. (2017a) <doi:10.1016/j.csda.2016.08.002> and López-Cheda et al. (2017b) <doi:10.1007/s11749-016-0515-1>."
"NPC",0,1,3,"2015-10-21 10:01","An implementation of nonparametric combination of hypothesis tests.    This package performs nonparametric combination (Pesarin and Salmaso 2010),    a permutation-based procedure for jointly testing multiple hypotheses. The    tests are conducted under the global ""sharp"" null hypothesis of no effects,    and the component tests are combined on the metric of their p-values. A    key feature of nonparametric combination is that it accounts for the    dependence among tests under the null hypothesis. In addition to the    ""NPC"" function, which performs nonparametric combination itself, the    package also contains a number of helper functions, many of which calculate    a test statistic given an input of data."
"mixAR",0,1,2,"2020-06-22 10:40","Model time series using mixture autoregressive (MAR)             models.  Implemented are frequentist (EM) and Bayesian             methods for estimation, prediction and model             evaluation. See Wong and Li (2002)             <doi:10.1111/1467-9868.00222>, Boshnakov (2009)             <doi:10.1016/j.spl.2009.04.009>), and the extensive             references in the documentation."
"cwm",2,1,2,"2013-03-28 08:47","This package estimates gaussian cluster weighted linear regressions by EM algorithm. "
"Storm",0,1,3,"2014-05-22 05:35","Storm is a distributed real-time computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing real-time computation.    Storm includes a ""Multi-Language"" (or ""Multilang"") Protocol to allow implementation of Bolts and Spouts in languages other than Java.  This R extension provides implementations of utility functions to allow an application developer to focus on application-specific functionality rather than Storm/R communications plumbing."
"VdgRsm",0,1,5,"2015-02-01 09:04","Functions for creating variance dispersion graphs, fraction of design space plots, and contour plots of scaled prediction variances for second-order response surface designs in spherical and cuboidal regions. Also, some standard response surface designs can be generated."
"rankdist",0,1,8,"2018-04-08 12:39","Implements distance based probability models for ranking data.     The supported distance metrics include Kendall distance, Spearman distance, Footrule distance, Hamming distance,    Weighted-tau distance and Weighted Kendall distance.    Phi-component model and mixture models are also supported."
"randomizationInference",0,1,4,"2014-09-20 22:04","Allows the user to conduct randomization-based inference for a wide variety of experimental scenarios. The package leverages a potential outcomes framework to output randomization-based p-values and null intervals for test statistics geared toward any estimands of interest, according to the specified null and alternative hypotheses. Users can define custom randomization schemes so that the randomization distributions are accurate for their experimental settings. The package also creates visualizations of randomization distributions and can test multiple test statistics simultaneously."
"pRF",0,1,3,"2015-11-05 08:31","Estimate False Discovery Rates (FDRs) for importance metrics from    random forest runs."
"permuco",1,1,4,"2019-01-25 10:40","Functions to compute p-values based on permutation tests. Regression, ANOVA and ANCOVA, omnibus F-tests, marginal unilateral and bilateral t-tests are available. Several methods to handle nuisance variables are implemented (Kherad-Pajouh, S., & Renaud, O. (2010) <doi:10.1016/j.csda.2010.02.015> ; Kherad-Pajouh, S., & Renaud, O. (2014) <doi:10.1007/s00362-014-0617-3> ; Winkler, A. M., Ridgway, G. R., Webster, M. A., Smith, S. M., & Nichols, T. E. (2014) <doi:10.1016/j.neuroimage.2014.01.060>). An extension for the comparison of signals issued from experimental conditions (e.g. EEG/ERP signals) is provided. Several corrections for multiple testing are possible, including the cluster-mass statistic (Maris, E., & Oostenveld, R. (2007) <doi:10.1016/j.jneumeth.2007.03.024>) and the threshold-free cluster enhancement (Smith, S. M., & Nichols, T. E. (2009) <doi:10.1016/j.neuroimage.2008.03.061>). "
"npcure",0,1,3,"2019-06-17 11:10","Performs nonparametric estimation in mixture cure models, and significance tests for the cure probability. For details, see López-Cheda et al. (2017a) <doi:10.1016/j.csda.2016.08.002> and López-Cheda et al. (2017b) <doi:10.1007/s11749-016-0515-1>."
"NPC",0,1,3,"2015-10-21 10:01","An implementation of nonparametric combination of hypothesis tests.    This package performs nonparametric combination (Pesarin and Salmaso 2010),    a permutation-based procedure for jointly testing multiple hypotheses. The    tests are conducted under the global ""sharp"" null hypothesis of no effects,    and the component tests are combined on the metric of their p-values. A    key feature of nonparametric combination is that it accounts for the    dependence among tests under the null hypothesis. In addition to the    ""NPC"" function, which performs nonparametric combination itself, the    package also contains a number of helper functions, many of which calculate    a test statistic given an input of data."
"mixAR",0,1,2,"2020-06-22 10:40","Model time series using mixture autoregressive (MAR)             models.  Implemented are frequentist (EM) and Bayesian             methods for estimation, prediction and model             evaluation. See Wong and Li (2002)             <doi:10.1111/1467-9868.00222>, Boshnakov (2009)             <doi:10.1016/j.spl.2009.04.009>), and the extensive             references in the documentation."
"cwm",2,1,2,"2013-03-28 08:47","This package estimates gaussian cluster weighted linear regressions by EM algorithm. "
"Storm",0,1,3,"2014-05-22 05:35","Storm is a distributed real-time computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing real-time computation.    Storm includes a ""Multi-Language"" (or ""Multilang"") Protocol to allow implementation of Bolts and Spouts in languages other than Java.  This R extension provides implementations of utility functions to allow an application developer to focus on application-specific functionality rather than Storm/R communications plumbing."
"VdgRsm",0,1,5,"2015-02-01 09:04","Functions for creating variance dispersion graphs, fraction of design space plots, and contour plots of scaled prediction variances for second-order response surface designs in spherical and cuboidal regions. Also, some standard response surface designs can be generated."
"rankdist",0,1,8,"2018-04-08 12:39","Implements distance based probability models for ranking data.     The supported distance metrics include Kendall distance, Spearman distance, Footrule distance, Hamming distance,    Weighted-tau distance and Weighted Kendall distance.    Phi-component model and mixture models are also supported."
"randomizationInference",0,1,4,"2014-09-20 22:04","Allows the user to conduct randomization-based inference for a wide variety of experimental scenarios. The package leverages a potential outcomes framework to output randomization-based p-values and null intervals for test statistics geared toward any estimands of interest, according to the specified null and alternative hypotheses. Users can define custom randomization schemes so that the randomization distributions are accurate for their experimental settings. The package also creates visualizations of randomization distributions and can test multiple test statistics simultaneously."
"pRF",0,1,3,"2015-11-05 08:31","Estimate False Discovery Rates (FDRs) for importance metrics from    random forest runs."
"permuco",1,1,4,"2019-01-25 10:40","Functions to compute p-values based on permutation tests. Regression, ANOVA and ANCOVA, omnibus F-tests, marginal unilateral and bilateral t-tests are available. Several methods to handle nuisance variables are implemented (Kherad-Pajouh, S., & Renaud, O. (2010) <doi:10.1016/j.csda.2010.02.015> ; Kherad-Pajouh, S., & Renaud, O. (2014) <doi:10.1007/s00362-014-0617-3> ; Winkler, A. M., Ridgway, G. R., Webster, M. A., Smith, S. M., & Nichols, T. E. (2014) <doi:10.1016/j.neuroimage.2014.01.060>). An extension for the comparison of signals issued from experimental conditions (e.g. EEG/ERP signals) is provided. Several corrections for multiple testing are possible, including the cluster-mass statistic (Maris, E., & Oostenveld, R. (2007) <doi:10.1016/j.jneumeth.2007.03.024>) and the threshold-free cluster enhancement (Smith, S. M., & Nichols, T. E. (2009) <doi:10.1016/j.neuroimage.2008.03.061>). "
"npcure",0,1,3,"2019-06-17 11:10","Performs nonparametric estimation in mixture cure models, and significance tests for the cure probability. For details, see López-Cheda et al. (2017a) <doi:10.1016/j.csda.2016.08.002> and López-Cheda et al. (2017b) <doi:10.1007/s11749-016-0515-1>."
"NPC",0,1,3,"2015-10-21 10:01","An implementation of nonparametric combination of hypothesis tests.    This package performs nonparametric combination (Pesarin and Salmaso 2010),    a permutation-based procedure for jointly testing multiple hypotheses. The    tests are conducted under the global ""sharp"" null hypothesis of no effects,    and the component tests are combined on the metric of their p-values. A    key feature of nonparametric combination is that it accounts for the    dependence among tests under the null hypothesis. In addition to the    ""NPC"" function, which performs nonparametric combination itself, the    package also contains a number of helper functions, many of which calculate    a test statistic given an input of data."
"mixAR",0,1,2,"2020-06-22 10:40","Model time series using mixture autoregressive (MAR)             models.  Implemented are frequentist (EM) and Bayesian             methods for estimation, prediction and model             evaluation. See Wong and Li (2002)             <doi:10.1111/1467-9868.00222>, Boshnakov (2009)             <doi:10.1016/j.spl.2009.04.009>), and the extensive             references in the documentation."
"cwm",2,1,2,"2013-03-28 08:47","This package estimates gaussian cluster weighted linear regressions by EM algorithm. "
"Storm",0,1,3,"2014-05-22 05:35","Storm is a distributed real-time computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing real-time computation.    Storm includes a ""Multi-Language"" (or ""Multilang"") Protocol to allow implementation of Bolts and Spouts in languages other than Java.  This R extension provides implementations of utility functions to allow an application developer to focus on application-specific functionality rather than Storm/R communications plumbing."
"VdgRsm",0,1,5,"2015-02-01 09:04","Functions for creating variance dispersion graphs, fraction of design space plots, and contour plots of scaled prediction variances for second-order response surface designs in spherical and cuboidal regions. Also, some standard response surface designs can be generated."
"rankdist",0,1,8,"2018-04-08 12:39","Implements distance based probability models for ranking data.     The supported distance metrics include Kendall distance, Spearman distance, Footrule distance, Hamming distance,    Weighted-tau distance and Weighted Kendall distance.    Phi-component model and mixture models are also supported."
"randomizationInference",0,1,4,"2014-09-20 22:04","Allows the user to conduct randomization-based inference for a wide variety of experimental scenarios. The package leverages a potential outcomes framework to output randomization-based p-values and null intervals for test statistics geared toward any estimands of interest, according to the specified null and alternative hypotheses. Users can define custom randomization schemes so that the randomization distributions are accurate for their experimental settings. The package also creates visualizations of randomization distributions and can test multiple test statistics simultaneously."
"pRF",0,1,3,"2015-11-05 08:31","Estimate False Discovery Rates (FDRs) for importance metrics from    random forest runs."
"permuco",1,1,4,"2019-01-25 10:40","Functions to compute p-values based on permutation tests. Regression, ANOVA and ANCOVA, omnibus F-tests, marginal unilateral and bilateral t-tests are available. Several methods to handle nuisance variables are implemented (Kherad-Pajouh, S., & Renaud, O. (2010) <doi:10.1016/j.csda.2010.02.015> ; Kherad-Pajouh, S., & Renaud, O. (2014) <doi:10.1007/s00362-014-0617-3> ; Winkler, A. M., Ridgway, G. R., Webster, M. A., Smith, S. M., & Nichols, T. E. (2014) <doi:10.1016/j.neuroimage.2014.01.060>). An extension for the comparison of signals issued from experimental conditions (e.g. EEG/ERP signals) is provided. Several corrections for multiple testing are possible, including the cluster-mass statistic (Maris, E., & Oostenveld, R. (2007) <doi:10.1016/j.jneumeth.2007.03.024>) and the threshold-free cluster enhancement (Smith, S. M., & Nichols, T. E. (2009) <doi:10.1016/j.neuroimage.2008.03.061>). "
"npcure",0,1,3,"2019-06-17 11:10","Performs nonparametric estimation in mixture cure models, and significance tests for the cure probability. For details, see López-Cheda et al. (2017a) <doi:10.1016/j.csda.2016.08.002> and López-Cheda et al. (2017b) <doi:10.1007/s11749-016-0515-1>."
"NPC",0,1,3,"2015-10-21 10:01","An implementation of nonparametric combination of hypothesis tests.    This package performs nonparametric combination (Pesarin and Salmaso 2010),    a permutation-based procedure for jointly testing multiple hypotheses. The    tests are conducted under the global ""sharp"" null hypothesis of no effects,    and the component tests are combined on the metric of their p-values. A    key feature of nonparametric combination is that it accounts for the    dependence among tests under the null hypothesis. In addition to the    ""NPC"" function, which performs nonparametric combination itself, the    package also contains a number of helper functions, many of which calculate    a test statistic given an input of data."
"mixAR",0,1,2,"2020-06-22 10:40","Model time series using mixture autoregressive (MAR)             models.  Implemented are frequentist (EM) and Bayesian             methods for estimation, prediction and model             evaluation. See Wong and Li (2002)             <doi:10.1111/1467-9868.00222>, Boshnakov (2009)             <doi:10.1016/j.spl.2009.04.009>), and the extensive             references in the documentation."
"cwm",2,1,2,"2013-03-28 08:47","This package estimates gaussian cluster weighted linear regressions by EM algorithm. "
"Storm",0,1,3,"2014-05-22 05:35","Storm is a distributed real-time computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing real-time computation.    Storm includes a ""Multi-Language"" (or ""Multilang"") Protocol to allow implementation of Bolts and Spouts in languages other than Java.  This R extension provides implementations of utility functions to allow an application developer to focus on application-specific functionality rather than Storm/R communications plumbing."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"butcher",3,1,3,"2020-01-08 01:50","Provides a set of five S3 generics to axe components of fitted model objects and help reduce the size of model objects saved to disk."
"UPMASK",0,1,3,"2017-04-03 22:19","An implementation of the UPMASK method for performing membership    assignment in stellar clusters in R. It is prepared to use photometry and    spatial positions, but it can take into account other types of data. The    method is able to take into account arbitrary error models, and it is    unsupervised, data-driven, physical-model-free and relies on as few    assumptions as possible. The approach followed for membership assessment is    based on an iterative process, dimensionality reduction, a clustering    algorithm and a kernel density estimation."
"fPortfolio",0,2,18,"2017-11-16 22:55","Provides a collection	of functions to optimize portfolios and to analyze them from    different points of view."
"phenopix",0,1,5,"2020-01-28 16:00","A collection of functions to process digital images, depict greenness index trajectories and extract relevant phenological stages. "
"PCRedux",2,1,8,"2020-01-07 23:40","Extracts features from amplification curve data of quantitative Polymerase Chain Reactions (qPCR) (Pabinger S. et al. (2014) <doi:10.1016/j.bdq.2014.08.002>) for machine learning purposes. Helper functions prepare the amplification curve data for processing as functional data (e.g., Hausdorff distance) or enable the plotting of amplification curve classes (negative, ambiguous, positive). The hookreg() and hookregNL() functions (Burdukiewicz M. et al. (2018) <doi:10.1016/j.bdq.2018.08.001>) can be used to predict amplification curves with an hook effect-like curvature. The pcrfit_single() function can be used to extract features from an amplification curve."
"ppclust",2,1,5,"2019-07-23 14:10","Partitioning clustering divides the objects in a data set into non-overlapping subsets or clusters by using the prototype-based probabilistic and possibilistic clustering algorithms. This package covers a set of the functions for Fuzzy C-Means (Bezdek, 1974) <doi:10.1080/01969727308546047>, Possibilistic C-Means (Krishnapuram & Keller, 1993) <doi:10.1109/91.227387>, Possibilistic Fuzzy C-Means (Pal et al, 2005) <doi:10.1109/TFUZZ.2004.840099>, Possibilistic Clustering Algorithm (Yang et al, 2006) <doi:10.1016/j.patcog.2005.07.005>, Possibilistic C-Means with Repulsion (Wachs et al, 2006) <doi:10.1007/3-540-31662-0_6> and the other variants of hard and soft clustering algorithms. The cluster prototypes and membership matrices required by these partitioning algorithms are initialized with different initialization techniques that are available in the package 'inaparc'. As the distance metrics, not only the Euclidean distance but also a set of the commonly used distance metrics are available to use with some of the algorithms in the package. "
"ppclust",2,1,5,"2019-07-23 14:10","Partitioning clustering divides the objects in a data set into non-overlapping subsets or clusters by using the prototype-based probabilistic and possibilistic clustering algorithms. This package covers a set of the functions for Fuzzy C-Means (Bezdek, 1974) <doi:10.1080/01969727308546047>, Possibilistic C-Means (Krishnapuram & Keller, 1993) <doi:10.1109/91.227387>, Possibilistic Fuzzy C-Means (Pal et al, 2005) <doi:10.1109/TFUZZ.2004.840099>, Possibilistic Clustering Algorithm (Yang et al, 2006) <doi:10.1016/j.patcog.2005.07.005>, Possibilistic C-Means with Repulsion (Wachs et al, 2006) <doi:10.1007/3-540-31662-0_6> and the other variants of hard and soft clustering algorithms. The cluster prototypes and membership matrices required by these partitioning algorithms are initialized with different initialization techniques that are available in the package 'inaparc'. As the distance metrics, not only the Euclidean distance but also a set of the commonly used distance metrics are available to use with some of the algorithms in the package. "
"ppclust",2,1,5,"2019-07-23 14:10","Partitioning clustering divides the objects in a data set into non-overlapping subsets or clusters by using the prototype-based probabilistic and possibilistic clustering algorithms. This package covers a set of the functions for Fuzzy C-Means (Bezdek, 1974) <doi:10.1080/01969727308546047>, Possibilistic C-Means (Krishnapuram & Keller, 1993) <doi:10.1109/91.227387>, Possibilistic Fuzzy C-Means (Pal et al, 2005) <doi:10.1109/TFUZZ.2004.840099>, Possibilistic Clustering Algorithm (Yang et al, 2006) <doi:10.1016/j.patcog.2005.07.005>, Possibilistic C-Means with Repulsion (Wachs et al, 2006) <doi:10.1007/3-540-31662-0_6> and the other variants of hard and soft clustering algorithms. The cluster prototypes and membership matrices required by these partitioning algorithms are initialized with different initialization techniques that are available in the package 'inaparc'. As the distance metrics, not only the Euclidean distance but also a set of the commonly used distance metrics are available to use with some of the algorithms in the package. "
"ppclust",2,1,5,"2019-07-23 14:10","Partitioning clustering divides the objects in a data set into non-overlapping subsets or clusters by using the prototype-based probabilistic and possibilistic clustering algorithms. This package covers a set of the functions for Fuzzy C-Means (Bezdek, 1974) <doi:10.1080/01969727308546047>, Possibilistic C-Means (Krishnapuram & Keller, 1993) <doi:10.1109/91.227387>, Possibilistic Fuzzy C-Means (Pal et al, 2005) <doi:10.1109/TFUZZ.2004.840099>, Possibilistic Clustering Algorithm (Yang et al, 2006) <doi:10.1016/j.patcog.2005.07.005>, Possibilistic C-Means with Repulsion (Wachs et al, 2006) <doi:10.1007/3-540-31662-0_6> and the other variants of hard and soft clustering algorithms. The cluster prototypes and membership matrices required by these partitioning algorithms are initialized with different initialization techniques that are available in the package 'inaparc'. As the distance metrics, not only the Euclidean distance but also a set of the commonly used distance metrics are available to use with some of the algorithms in the package. "
"ppclust",2,1,5,"2019-07-23 14:10","Partitioning clustering divides the objects in a data set into non-overlapping subsets or clusters by using the prototype-based probabilistic and possibilistic clustering algorithms. This package covers a set of the functions for Fuzzy C-Means (Bezdek, 1974) <doi:10.1080/01969727308546047>, Possibilistic C-Means (Krishnapuram & Keller, 1993) <doi:10.1109/91.227387>, Possibilistic Fuzzy C-Means (Pal et al, 2005) <doi:10.1109/TFUZZ.2004.840099>, Possibilistic Clustering Algorithm (Yang et al, 2006) <doi:10.1016/j.patcog.2005.07.005>, Possibilistic C-Means with Repulsion (Wachs et al, 2006) <doi:10.1007/3-540-31662-0_6> and the other variants of hard and soft clustering algorithms. The cluster prototypes and membership matrices required by these partitioning algorithms are initialized with different initialization techniques that are available in the package 'inaparc'. As the distance metrics, not only the Euclidean distance but also a set of the commonly used distance metrics are available to use with some of the algorithms in the package. "
"SimRVSequences",1,1,8,"2020-06-15 14:20","Methods to simulate genetic sequence data for pedigrees, with     functionality to simulate genetic heterogeneity among pedigrees.    Christina Nieuwoudt, Angela Brooks-Wilson,     and Jinko Graham (2019) <doi:10.1101/534552>."
"rehh",2,1,15,"2020-06-19 12:00","Population genetic data such as 'Single Nucleotide        Polymorphisms' (SNPs) is often used to identify genomic regions        that have been under recent natural or artificial selection        and might provide clues about the molecular mechanisms of adaptation.         One approach, the concept of an 'Extended Haplotype Homozygosity' (EHH),         introduced by (Sabeti 2002) <doi:10.1038/nature01140>, has given rise to         several statistics designed for whole genome scans.         The package provides functions to compute three of these,        namely: 'iHS' (Voight 2006) <doi:10.1371/journal.pbio.0040072> for        detecting positive or 'Darwinian' selection within a single population as well as        'Rsb' (Tang 2007) <doi:10.1371/journal.pbio.0050171> and         'XP-EHH' (Sabeti 2007) <doi:10.1038/nature06250>, targeted        at differential selection between two populations.         Various plotting functions are also included to facilitate        visualization and interpretation of these statistics.         Due to changes in the API, albeit mostly minor,         versions 3.X are not compatible with versions 2.0.X.        Note: optionally, vcf files can be imported using package vcfR. That package        is currently removed from CRAN, but can still be installed from         <https://github.com/knausb/vcfR> following instructions there."
"rdiversity",0,1,4,"2018-06-12 16:23","Provides a framework for the measurement and partitioning of    the (similarity-sensitive) biodiversity of a metacommunity and its    constituent subcommunities. Richard Reeve, et al. (2016)     <arXiv:1404.6520v3>."
"perfectphyloR",1,1,2,"2019-07-18 08:36","Reconstructs perfect phylogeny at a user-given focal point and to depict and test association in a genomic region based on the reconstructed partitions. Charith B Karunarathna and Jinko Graham (2019) <bioRxiv:10.1101/674523>."
"pcadapt",0,1,17,"2020-02-28 23:50","Methods to detect genetic markers involved in biological    adaptation. 'pcadapt' provides statistical tools for outlier detection based on    Principal Component Analysis. Implements the method described in (Luu, 2016)    <doi:10.1111/1755-0998.12592>."
"onemap",4,1,12,"2019-09-19 15:20","Analysis of molecular marker data from model (backcrosses,    F2 and recombinant inbred lines) and non-model systems (i. e.    outcrossing species). For the later, it allows statistical    analysis by simultaneously estimating linkage and linkage    phases (genetic map construction) according to Wu et al. (2002)    <doi:10.1006/tpbi.2002.1577>. All analysis are based on multipoint     approaches using hidden Markov models."
"MoBPS",0,1,1,"2020-03-28","Framework for the simulation framework for the simulation of complex breeding programs and compare their economic and genetic impact. The package is also used as the background simulator for our a web-based interface <http:www.mobps.de>. Associated publication: Pook et al (2019) <doi:10.1101/829333>."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"aqp",0,4,42,"2019-11-09 06:30","The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."
"adjclust",3,1,5,"2019-12-10 14:30","Implements a constrained version of hierarchical agglomerative     clustering, in which each observation is associated to a position, and     only adjacent clusters can be merged. Typical application fields in     bioinformatics include Genome-Wide Association Studies or Hi-C data     analysis, where the similarity between items is a decreasing function of     their genomic distance. Taking advantage of this feature, the implemented     algorithm is time and memory efficient. This algorithm is described in     Ambroise et al (2019) <https://almob.biomedcentral.com/articles/10.1186/s13015-019-0157-4>."
"adjclust",3,1,5,"2019-12-10 14:30","Implements a constrained version of hierarchical agglomerative     clustering, in which each observation is associated to a position, and     only adjacent clusters can be merged. Typical application fields in     bioinformatics include Genome-Wide Association Studies or Hi-C data     analysis, where the similarity between items is a decreasing function of     their genomic distance. Taking advantage of this feature, the implemented     algorithm is time and memory efficient. This algorithm is described in     Ambroise et al (2019) <https://almob.biomedcentral.com/articles/10.1186/s13015-019-0157-4>."
"adjclust",3,1,5,"2019-12-10 14:30","Implements a constrained version of hierarchical agglomerative     clustering, in which each observation is associated to a position, and     only adjacent clusters can be merged. Typical application fields in     bioinformatics include Genome-Wide Association Studies or Hi-C data     analysis, where the similarity between items is a decreasing function of     their genomic distance. Taking advantage of this feature, the implemented     algorithm is time and memory efficient. This algorithm is described in     Ambroise et al (2019) <https://almob.biomedcentral.com/articles/10.1186/s13015-019-0157-4>."
"adjclust",3,1,5,"2019-12-10 14:30","Implements a constrained version of hierarchical agglomerative     clustering, in which each observation is associated to a position, and     only adjacent clusters can be merged. Typical application fields in     bioinformatics include Genome-Wide Association Studies or Hi-C data     analysis, where the similarity between items is a decreasing function of     their genomic distance. Taking advantage of this feature, the implemented     algorithm is time and memory efficient. This algorithm is described in     Ambroise et al (2019) <https://almob.biomedcentral.com/articles/10.1186/s13015-019-0157-4>."
"adjclust",3,1,5,"2019-12-10 14:30","Implements a constrained version of hierarchical agglomerative     clustering, in which each observation is associated to a position, and     only adjacent clusters can be merged. Typical application fields in     bioinformatics include Genome-Wide Association Studies or Hi-C data     analysis, where the similarity between items is a decreasing function of     their genomic distance. Taking advantage of this feature, the implemented     algorithm is time and memory efficient. This algorithm is described in     Ambroise et al (2019) <https://almob.biomedcentral.com/articles/10.1186/s13015-019-0157-4>."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"DatabionicSwarm",1,2,9,"2019-12-11 18:30","Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: ""Swarm Intelligence for Self-Organized Clustering"" (2020), Artificial Intelligence, <doi:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: ""Projection Based Clustering through Self-Organization and Swarm Intelligence"" (2018) <doi:10.1007/978-3-658-20540-9>. A comparison to 26 common clustering algorithms on 15 datasets is presented on the website."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"DatabionicSwarm",1,2,9,"2019-12-11 18:30","Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: ""Swarm Intelligence for Self-Organized Clustering"" (2020), Artificial Intelligence, <doi:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: ""Projection Based Clustering through Self-Organization and Swarm Intelligence"" (2018) <doi:10.1007/978-3-658-20540-9>. A comparison to 26 common clustering algorithms on 15 datasets is presented on the website."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"DatabionicSwarm",1,2,9,"2019-12-11 18:30","Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: ""Swarm Intelligence for Self-Organized Clustering"" (2020), Artificial Intelligence, <doi:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: ""Projection Based Clustering through Self-Organization and Swarm Intelligence"" (2018) <doi:10.1007/978-3-658-20540-9>. A comparison to 26 common clustering algorithms on 15 datasets is presented on the website."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"DatabionicSwarm",1,2,9,"2019-12-11 18:30","Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: ""Swarm Intelligence for Self-Organized Clustering"" (2020), Artificial Intelligence, <doi:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: ""Projection Based Clustering through Self-Organization and Swarm Intelligence"" (2018) <doi:10.1007/978-3-658-20540-9>. A comparison to 26 common clustering algorithms on 15 datasets is presented on the website."
"FCPS",1,1,6,"2020-06-26 14:40","Many conventional clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the the mirrored density plot (MD-plot) of clusterability are implemented. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."
"DatabionicSwarm",1,2,9,"2019-12-11 18:30","Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: ""Swarm Intelligence for Self-Organized Clustering"" (2020), Artificial Intelligence, <doi:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: ""Projection Based Clustering through Self-Organization and Swarm Intelligence"" (2018) <doi:10.1007/978-3-658-20540-9>. A comparison to 26 common clustering algorithms on 15 datasets is presented on the website."
"CopyDetect",0,1,4,"2016-04-27 19:28","Contains several IRT and non-IRT based response similarity indices proposed in the literature for multiple-choice examinations such as the Omega index, Wollack (1997) <doi:10.1177/01466216970214002>; Generalized Binomial Test, van der Linden & Sotaridona (2006) <doi:10.3102/10769986031003283>; K index, K1 and K2 indices, Sotaridona & Meijer (2002) <doi:10.1111/j.1745-3984.2002.tb01138.x>;  and S1 and S2 indices, Sotaridona & Meijer (2003) <doi:10.1111/j.1745-3984.2003.tb01096.x>."
"CDM",0,12,50,"2019-09-05 06:40","    Functions for cognitive diagnosis modeling and multidimensional item response modeling     for dichotomous and polytomous item responses. This package enables the estimation of     the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),    the multiple group (polytomous) GDINA model (de la Torre, 2011,     <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009,     <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008,     <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992,     <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis     (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>).     See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02>     or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)         for further details on estimation and the package structure.    For tutorials on how to use the CDM package see     George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as    Ravand and Robitzsch (2015)."
"TestDesign",2,1,8,"2020-09-21 08:50","Use the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and    van der Linden (2018) <doi:10.1201/9781315117430> in constructing fixed and adaptive tests. Supports the following    mixed-integer programming (MIP) solver packages: 'lpsymphony', 'Rsymphony', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package    is not available from CRAN; see <https://www.gurobi.com/downloads/>. See vignette for installing 'Rsymphony' package    on Mac systems."
"rpf",3,4,42,"2020-05-07 13:20","The purpose of this package is to factor out logic    and math common to Item Factor Analysis fitting, diagnostics, and    analysis. It is envisioned as core support code suitable for more    specialized IRT packages to build upon. Complete access to optimized C    functions are made available with R_RegisterCCallable().    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>."
"psychotools",0,14,12,"2018-12-19 23:48","Infrastructure for psychometric modeling such as data classes (for  item response data and paired comparisons), basic model fitting functions (for  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,  rating scale, multinomial processing tree models), extractor functions for  different types of parameters (item, person, threshold, discrimination,  guessing, upper asymptotes), unified inference and visualizations, and various  datasets for illustration.  Intended as a common lightweight and efficient  toolbox for psychometric modeling and a common building block for fitting  psychometric mixture models in package ""psychomix"" and trees based on  psychometric models in package ""psychotree""."
"nonnest2",1,2,11,"2017-12-15 13:22","Testing non-nested models via theory supplied by Vuong (1989) <doi:10.2307/1912557>.    Includes tests of model distinguishability and of model fit that can be applied    to both nested and non-nested models. Also includes functionality to obtain    confidence intervals associated with AIC and BIC. This material is partially based on    work supported by the National Science Foundation under Grant Number SES-1061334."
"LSAmitR",0,2,4,"2018-01-19 08:56","Dieses R-Paket stellt Zusatzmaterial in Form von Daten, Funktionen             und R-Hilfe-Seiten für den Herausgeberband Breit, S. und Schreiner,              C. (Hrsg.). (2016). ""Large-Scale Assessment mit R: Methodische Grundlagen              der österreichischen Bildungsstandardüberprüfung."" Wien: facultas.              (ISBN: 978-3-7089-1343-8, <https://www.bifie.at/node/3770>) zur              Verfügung."
"irtplay",0,1,10,"2020-07-15 15:00","Fit unidimensional item response theory (IRT) models to mixture     of dichotomous and polytomous data, calibrate online item parameters     (i.e., pretest and operational items), estimate examinees abilities,     and examine the IRT model-data fit on item-level in different ways     as well as provide useful functions related to unidimensional IRT models.     For the item parameter estimation, marginal maximum likelihood estimation     with expectation-maximization (MMLE-EM) algorithm     (Bock & Aitkin (1981) <doi:10.1007/BF02294168>) is used.     For the online calibration, Stocking's Method A     (Ban, Hanson, Wang, Yi, & Harris (2011) <doi:10.1111/j.1745-3984.2001.tb01123.x>)     and the fixed item parameter calibration (FIPC) method     (Kim (2006) <doi:10.1111/j.1745-3984.2006.00021.x>) are provided.     For the ability estimation, several popular scoring methods     (e.g., MLE, EAP, and MAP) are implemented. In terms of assessing the IRT     model-data fit, one of distinguished features of this package is that it     gives not only well-known item fit statistics (e.g., chi-square (X2),     likelihood ratio chi-square (G2), infit and oufit statistics, and     S-X2 statistic (Ames & Penfield (2015) <doi:10.1111/emip.12067>))     but also graphical displays to look at residuals between the observed     data and model-based predictions     (Hambleton, Swaminathan, & Rogers (1991, ISBN:9780803936478)).     In addition, there are many useful functions such as computing asymptotic     variance-covariance matrices of item parameter estimates (Li & Lissitz (2004)     <doi:10.1111/j.1745-3984.2004.tb01109.x>), importing item and/or ability     parameters from popular IRT software, running 'flexMIRT' (Cai, 2017)     through R, generating simulated data, computing the conditional     distribution of observed scores using the Lord-Wingersky recursion     formula (Lord & Wingersky (1984) <doi:10.1177/014662168400800409>),     computing the loglikelihood of individual items, computing the loglikelihood     of abilities, computing item and test information functions, computing item     and test characteristic curve functions, and plotting item and test     characteristic curves and item and test information functions."
"ShinyItemAnalysis",0,1,14,"2020-05-04 17:20","Interactive shiny application for analysis of educational tests and    their items."
"scDIFtest",1,1,2,"2020-06-26 11:10","Detection of item-wise Differential Item Functioning (DIF)              in fitted 'mirt', 'multipleGroup' or 'bfactor' models               using score-based structural change tests. Under the hood              the sctest() function from the 'strucchange' package is used."
"PROsetta",1,1,2,"2020-07-01 13:00","Perform scale linking to establish relationships between instruments    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>."
"mstDIF",1,1,2,"2020-07-20 10:40","A collection of statistical tests for the detection of differential    item functioning (DIF) in multistage tests. Methods entail logistic regression,    an adaptation of the simultaneous item bias test (SIBTEST), and various score-based tests.    The presented tests provide itemwise test for DIF along categorical, ordinal or metric covariates. Methods for uniform and non-uniform     DIF effects are available depending on which method is used."
"kequate",2,1,12,"2019-05-31 19:30","Implements the kernel method of test equating as defined in von Davier, A. A., Holland, P. W. and Thayer, D. T. (2004) <doi:10.1007/b97446> and Andersson, B. and Wiberg, M. (2017) <doi:10.1007/s11336-016-9528-7> using the CB, EG, SG, NEAT CE/PSE and NEC designs, supporting Gaussian, logistic and uniform kernels and unsmoothed and pre-smoothed input data."
"jrt",1,1,2,"2019-01-04 12:20","Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225>. A convenience function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions."
"irtreliability",0,1,1,"2018-02-22","Estimation of reliability coefficients for ability estimates and sum scores from item response theory models as defined in Cheng, Y., Yuan, K.-H. and Liu, C. (2012) <doi:10.1177/0013164411407315> and Kim, S. and Feldt, L. S. (2010) <doi:10.1007/s12564-009-9062-8>. The package supports the 3-PL and generalized partial credit models and includes estimates of the standard errors of the reliability coefficient estimators, derived in Andersson, B. and Xin, T. (2018) <doi:10.1177/0013164417713570>."
"GPCMlasso",0,1,4,"2019-02-21 09:30","Provides a framework to detect Differential Item Functioning (DIF) in Generalized Partial Credit Models (GPCM) and special cases of the GPCM as proposed by Schauberger and Mair (2019) <doi:10.3758/s13428-019-01224-2>. A joint model is set up where DIF is explicitly parametrized and penalized likelihood estimation is used for parameter selection. The big advantage of the method called GPCMlasso is that several variables can be treated simultaneously and that both continuous and categorical variables can be used to detect DIF."
"faoutlier",0,1,10,"2017-07-22 07:31","Tools for detecting and summarize influential cases that    can affect exploratory and confirmatory factor analysis models as well as    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>;     Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <10.3389/fpsyg.2012.00055>)."
"equateIRT",2,2,14,"2016-03-30 20:35","Computation of direct, chain and average (bisector) equating coefficients with   standard errors using Item Response Theory (IRT) methods for dichotomous items   (Battauz (2013) <doi:10.1007/s11336-012-9316-y>,   Battauz (2015) <doi:10.18637/jss.v068.i07>).   Test scoring can be performed by true score equating and observed score equating methods.   DIF detection can be performed using a Wald-type test    (Battauz (2018) <doi:10.1007/s10260-018-00442-w>)."
"difR",0,2,20,"2018-05-14 15:38","Provides a collection of standard methods to detect differential item functioning among dichotomously scored items. Methods for uniform and non-uniform DIF, based on test-score or IRT methods, for comparing two or more than two groups of respondents, are available (Magis, Beland, Tuerlinckx and De Boeck,A General Framework and an R Package for the Detection of Dichotomous Differential Item Functioning, Behavior Research Methods, 42, 2010, 847-862 <doi:10.3758/BRM.42.3.847>)."
"ctgdist",0,1,1,"2020-08-10","It is assumed that psychological distances between the categories are equal for the measurement instruments consisted of polytomously scored items. According to Muraki, this assumption must be tested. In the examination process of this assumption, the fit indexes are obtained and evaluated. This package provides that this assumption is removed. By with this package, the converted scale values of all items in a measurement instrument can be calculated by estimating a category parameter set for each item. Thus, the calculations can be made without any need to usage of the common category parameter set. Through this package, the psychological distances of the items are scaled. The scaling of a category parameter set for each item cause differentiation of score of the categories will be got from items. Also, the total measurement instrument score of an individual can be calculated according to the scaling of item score categories by with this package.This package provides that the place of individuals related to the structure to be measured with a measurement instrument consisted of polytomously scored items can be reveal more accurately. In this way, it is thought that the results obtained about individuals can be made more sensitive, and the differences between individuals can be revealed more accurately. On the other hand, it can be argued that more accurate evidences can be obtained regarding the psychometric properties of the measurement instruments."
"airt",1,1,1,"2020-05-12","An evaluation framework for algorithm portfolios using Item Response    Theory (IRT). We use polytomous IRT models to evaluate algorithms and introduce    algorithm characteristics such as stability, effectiveness and anomalousness     (Kandanaarachchi, Smith-Miles 2020) <doi:10.13140/RG.2.2.11363.09760>."
"PerFit",0,1,7,"2015-07-20 09:59","Several person-fit statistics (PFSs) are offered. These statistics allow assessing whether  individual response patterns to tests or questionnaires are (im)plausible given   the other respondents in the sample or given a specified item response theory model. Some PFSs apply to   dichotomous data, such as the likelihood-based PFSs (lz, lz*) and the group-based PFSs   (personal biserial correlation, caution index, (normed) number of Guttman errors,   agreement/disagreement/dependability statistics, U3, ZU3, NCI, Ht). PFSs suitable to polytomous data include  extensions of lz, U3, and (normed) number of Guttman errors."
"OrdinalLogisticBiplot",0,1,4,"2014-05-03 00:36","Analysis of a matrix of polytomous items using Ordinal Logistic Biplots (OLB)  The OLB procedure extends the binary logistic biplot to ordinal (polytomous) data.   The  individuals are represented as points on a plane and the variables are represented   as lines rather than vectors as in a classical or binary biplot, specifying the points  for each of the categories of the variable.   The set of prediction regions is established by stripes perpendicular to the line   between the category points, in such a way that the prediction for each individual is given  by its projection into the line of the variable."
"NominalLogisticBiplot",0,1,2,"2013-09-17 14:47","Analysis of a matrix of polytomous items using Nominal Logistic Biplots (NLB)  according to Hernandez-Sanchez and Vicente-Villardon (2013).   The NLB procedure extends the binary logistic biplot to nominal (polytomous) data.   The  individuals are represented as points on a plane and the  variables are represented   as convex prediction regions rather than vectors as in a classical or binary biplot.   Using the methods from Computational Geometry, the set of prediction regions is converted to a set of points   in such a way that the prediction for each individual is established by its closest   ""category point"". Then interpretation is based on distances rather than on projections.   In this package we implement the geometry of such a representation and construct computational algorithms   for the estimation of parameters and the calculation of prediction regions."
"lordif",0,1,14,"2015-09-18 08:58","Analysis of Differential Item Functioning (DIF) for        dichotomous and polytomous items using an iterative hybrid of        ordinal logistic regression and item response theory (IRT)."
"CopyDetect",0,1,4,"2016-04-27 19:28","Contains several IRT and non-IRT based response similarity indices proposed in the literature for multiple-choice examinations such as the Omega index, Wollack (1997) <doi:10.1177/01466216970214002>; Generalized Binomial Test, van der Linden & Sotaridona (2006) <doi:10.3102/10769986031003283>; K index, K1 and K2 indices, Sotaridona & Meijer (2002) <doi:10.1111/j.1745-3984.2002.tb01138.x>;  and S1 and S2 indices, Sotaridona & Meijer (2003) <doi:10.1111/j.1745-3984.2003.tb01096.x>."
"CDM",0,12,50,"2019-09-05 06:40","    Functions for cognitive diagnosis modeling and multidimensional item response modeling     for dichotomous and polytomous item responses. This package enables the estimation of     the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),    the multiple group (polytomous) GDINA model (de la Torre, 2011,     <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009,     <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008,     <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992,     <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis     (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>).     See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02>     or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)         for further details on estimation and the package structure.    For tutorials on how to use the CDM package see     George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as    Ravand and Robitzsch (2015)."
"TestDesign",2,1,8,"2020-09-21 08:50","Use the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and    van der Linden (2018) <doi:10.1201/9781315117430> in constructing fixed and adaptive tests. Supports the following    mixed-integer programming (MIP) solver packages: 'lpsymphony', 'Rsymphony', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package    is not available from CRAN; see <https://www.gurobi.com/downloads/>. See vignette for installing 'Rsymphony' package    on Mac systems."
"rpf",3,4,42,"2020-05-07 13:20","The purpose of this package is to factor out logic    and math common to Item Factor Analysis fitting, diagnostics, and    analysis. It is envisioned as core support code suitable for more    specialized IRT packages to build upon. Complete access to optimized C    functions are made available with R_RegisterCCallable().    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>."
"psychotools",0,14,12,"2018-12-19 23:48","Infrastructure for psychometric modeling such as data classes (for  item response data and paired comparisons), basic model fitting functions (for  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,  rating scale, multinomial processing tree models), extractor functions for  different types of parameters (item, person, threshold, discrimination,  guessing, upper asymptotes), unified inference and visualizations, and various  datasets for illustration.  Intended as a common lightweight and efficient  toolbox for psychometric modeling and a common building block for fitting  psychometric mixture models in package ""psychomix"" and trees based on  psychometric models in package ""psychotree""."
"nonnest2",1,2,11,"2017-12-15 13:22","Testing non-nested models via theory supplied by Vuong (1989) <doi:10.2307/1912557>.    Includes tests of model distinguishability and of model fit that can be applied    to both nested and non-nested models. Also includes functionality to obtain    confidence intervals associated with AIC and BIC. This material is partially based on    work supported by the National Science Foundation under Grant Number SES-1061334."
"LSAmitR",0,2,4,"2018-01-19 08:56","Dieses R-Paket stellt Zusatzmaterial in Form von Daten, Funktionen             und R-Hilfe-Seiten für den Herausgeberband Breit, S. und Schreiner,              C. (Hrsg.). (2016). ""Large-Scale Assessment mit R: Methodische Grundlagen              der österreichischen Bildungsstandardüberprüfung."" Wien: facultas.              (ISBN: 978-3-7089-1343-8, <https://www.bifie.at/node/3770>) zur              Verfügung."
"irtplay",0,1,10,"2020-07-15 15:00","Fit unidimensional item response theory (IRT) models to mixture     of dichotomous and polytomous data, calibrate online item parameters     (i.e., pretest and operational items), estimate examinees abilities,     and examine the IRT model-data fit on item-level in different ways     as well as provide useful functions related to unidimensional IRT models.     For the item parameter estimation, marginal maximum likelihood estimation     with expectation-maximization (MMLE-EM) algorithm     (Bock & Aitkin (1981) <doi:10.1007/BF02294168>) is used.     For the online calibration, Stocking's Method A     (Ban, Hanson, Wang, Yi, & Harris (2011) <doi:10.1111/j.1745-3984.2001.tb01123.x>)     and the fixed item parameter calibration (FIPC) method     (Kim (2006) <doi:10.1111/j.1745-3984.2006.00021.x>) are provided.     For the ability estimation, several popular scoring methods     (e.g., MLE, EAP, and MAP) are implemented. In terms of assessing the IRT     model-data fit, one of distinguished features of this package is that it     gives not only well-known item fit statistics (e.g., chi-square (X2),     likelihood ratio chi-square (G2), infit and oufit statistics, and     S-X2 statistic (Ames & Penfield (2015) <doi:10.1111/emip.12067>))     but also graphical displays to look at residuals between the observed     data and model-based predictions     (Hambleton, Swaminathan, & Rogers (1991, ISBN:9780803936478)).     In addition, there are many useful functions such as computing asymptotic     variance-covariance matrices of item parameter estimates (Li & Lissitz (2004)     <doi:10.1111/j.1745-3984.2004.tb01109.x>), importing item and/or ability     parameters from popular IRT software, running 'flexMIRT' (Cai, 2017)     through R, generating simulated data, computing the conditional     distribution of observed scores using the Lord-Wingersky recursion     formula (Lord & Wingersky (1984) <doi:10.1177/014662168400800409>),     computing the loglikelihood of individual items, computing the loglikelihood     of abilities, computing item and test information functions, computing item     and test characteristic curve functions, and plotting item and test     characteristic curves and item and test information functions."
"ShinyItemAnalysis",0,1,14,"2020-05-04 17:20","Interactive shiny application for analysis of educational tests and    their items."
"scDIFtest",1,1,2,"2020-06-26 11:10","Detection of item-wise Differential Item Functioning (DIF)              in fitted 'mirt', 'multipleGroup' or 'bfactor' models               using score-based structural change tests. Under the hood              the sctest() function from the 'strucchange' package is used."
"PROsetta",1,1,2,"2020-07-01 13:00","Perform scale linking to establish relationships between instruments    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>."
"mstDIF",1,1,2,"2020-07-20 10:40","A collection of statistical tests for the detection of differential    item functioning (DIF) in multistage tests. Methods entail logistic regression,    an adaptation of the simultaneous item bias test (SIBTEST), and various score-based tests.    The presented tests provide itemwise test for DIF along categorical, ordinal or metric covariates. Methods for uniform and non-uniform     DIF effects are available depending on which method is used."
"kequate",2,1,12,"2019-05-31 19:30","Implements the kernel method of test equating as defined in von Davier, A. A., Holland, P. W. and Thayer, D. T. (2004) <doi:10.1007/b97446> and Andersson, B. and Wiberg, M. (2017) <doi:10.1007/s11336-016-9528-7> using the CB, EG, SG, NEAT CE/PSE and NEC designs, supporting Gaussian, logistic and uniform kernels and unsmoothed and pre-smoothed input data."
"jrt",1,1,2,"2019-01-04 12:20","Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225>. A convenience function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions."
"irtreliability",0,1,1,"2018-02-22","Estimation of reliability coefficients for ability estimates and sum scores from item response theory models as defined in Cheng, Y., Yuan, K.-H. and Liu, C. (2012) <doi:10.1177/0013164411407315> and Kim, S. and Feldt, L. S. (2010) <doi:10.1007/s12564-009-9062-8>. The package supports the 3-PL and generalized partial credit models and includes estimates of the standard errors of the reliability coefficient estimators, derived in Andersson, B. and Xin, T. (2018) <doi:10.1177/0013164417713570>."
"GPCMlasso",0,1,4,"2019-02-21 09:30","Provides a framework to detect Differential Item Functioning (DIF) in Generalized Partial Credit Models (GPCM) and special cases of the GPCM as proposed by Schauberger and Mair (2019) <doi:10.3758/s13428-019-01224-2>. A joint model is set up where DIF is explicitly parametrized and penalized likelihood estimation is used for parameter selection. The big advantage of the method called GPCMlasso is that several variables can be treated simultaneously and that both continuous and categorical variables can be used to detect DIF."
"faoutlier",0,1,10,"2017-07-22 07:31","Tools for detecting and summarize influential cases that    can affect exploratory and confirmatory factor analysis models as well as    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>;     Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <10.3389/fpsyg.2012.00055>)."
"equateIRT",2,2,14,"2016-03-30 20:35","Computation of direct, chain and average (bisector) equating coefficients with   standard errors using Item Response Theory (IRT) methods for dichotomous items   (Battauz (2013) <doi:10.1007/s11336-012-9316-y>,   Battauz (2015) <doi:10.18637/jss.v068.i07>).   Test scoring can be performed by true score equating and observed score equating methods.   DIF detection can be performed using a Wald-type test    (Battauz (2018) <doi:10.1007/s10260-018-00442-w>)."
"difR",0,2,20,"2018-05-14 15:38","Provides a collection of standard methods to detect differential item functioning among dichotomously scored items. Methods for uniform and non-uniform DIF, based on test-score or IRT methods, for comparing two or more than two groups of respondents, are available (Magis, Beland, Tuerlinckx and De Boeck,A General Framework and an R Package for the Detection of Dichotomous Differential Item Functioning, Behavior Research Methods, 42, 2010, 847-862 <doi:10.3758/BRM.42.3.847>)."
"ctgdist",0,1,1,"2020-08-10","It is assumed that psychological distances between the categories are equal for the measurement instruments consisted of polytomously scored items. According to Muraki, this assumption must be tested. In the examination process of this assumption, the fit indexes are obtained and evaluated. This package provides that this assumption is removed. By with this package, the converted scale values of all items in a measurement instrument can be calculated by estimating a category parameter set for each item. Thus, the calculations can be made without any need to usage of the common category parameter set. Through this package, the psychological distances of the items are scaled. The scaling of a category parameter set for each item cause differentiation of score of the categories will be got from items. Also, the total measurement instrument score of an individual can be calculated according to the scaling of item score categories by with this package.This package provides that the place of individuals related to the structure to be measured with a measurement instrument consisted of polytomously scored items can be reveal more accurately. In this way, it is thought that the results obtained about individuals can be made more sensitive, and the differences between individuals can be revealed more accurately. On the other hand, it can be argued that more accurate evidences can be obtained regarding the psychometric properties of the measurement instruments."
"airt",1,1,1,"2020-05-12","An evaluation framework for algorithm portfolios using Item Response    Theory (IRT). We use polytomous IRT models to evaluate algorithms and introduce    algorithm characteristics such as stability, effectiveness and anomalousness     (Kandanaarachchi, Smith-Miles 2020) <doi:10.13140/RG.2.2.11363.09760>."
"PerFit",0,1,7,"2015-07-20 09:59","Several person-fit statistics (PFSs) are offered. These statistics allow assessing whether  individual response patterns to tests or questionnaires are (im)plausible given   the other respondents in the sample or given a specified item response theory model. Some PFSs apply to   dichotomous data, such as the likelihood-based PFSs (lz, lz*) and the group-based PFSs   (personal biserial correlation, caution index, (normed) number of Guttman errors,   agreement/disagreement/dependability statistics, U3, ZU3, NCI, Ht). PFSs suitable to polytomous data include  extensions of lz, U3, and (normed) number of Guttman errors."
"OrdinalLogisticBiplot",0,1,4,"2014-05-03 00:36","Analysis of a matrix of polytomous items using Ordinal Logistic Biplots (OLB)  The OLB procedure extends the binary logistic biplot to ordinal (polytomous) data.   The  individuals are represented as points on a plane and the variables are represented   as lines rather than vectors as in a classical or binary biplot, specifying the points  for each of the categories of the variable.   The set of prediction regions is established by stripes perpendicular to the line   between the category points, in such a way that the prediction for each individual is given  by its projection into the line of the variable."
"NominalLogisticBiplot",0,1,2,"2013-09-17 14:47","Analysis of a matrix of polytomous items using Nominal Logistic Biplots (NLB)  according to Hernandez-Sanchez and Vicente-Villardon (2013).   The NLB procedure extends the binary logistic biplot to nominal (polytomous) data.   The  individuals are represented as points on a plane and the  variables are represented   as convex prediction regions rather than vectors as in a classical or binary biplot.   Using the methods from Computational Geometry, the set of prediction regions is converted to a set of points   in such a way that the prediction for each individual is established by its closest   ""category point"". Then interpretation is based on distances rather than on projections.   In this package we implement the geometry of such a representation and construct computational algorithms   for the estimation of parameters and the calculation of prediction regions."
"lordif",0,1,14,"2015-09-18 08:58","Analysis of Differential Item Functioning (DIF) for        dichotomous and polytomous items using an iterative hybrid of        ordinal logistic regression and item response theory (IRT)."
"CopyDetect",0,1,4,"2016-04-27 19:28","Contains several IRT and non-IRT based response similarity indices proposed in the literature for multiple-choice examinations such as the Omega index, Wollack (1997) <doi:10.1177/01466216970214002>; Generalized Binomial Test, van der Linden & Sotaridona (2006) <doi:10.3102/10769986031003283>; K index, K1 and K2 indices, Sotaridona & Meijer (2002) <doi:10.1111/j.1745-3984.2002.tb01138.x>;  and S1 and S2 indices, Sotaridona & Meijer (2003) <doi:10.1111/j.1745-3984.2003.tb01096.x>."
"CDM",0,12,50,"2019-09-05 06:40","    Functions for cognitive diagnosis modeling and multidimensional item response modeling     for dichotomous and polytomous item responses. This package enables the estimation of     the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),    the multiple group (polytomous) GDINA model (de la Torre, 2011,     <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009,     <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008,     <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992,     <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis     (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>).     See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02>     or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)         for further details on estimation and the package structure.    For tutorials on how to use the CDM package see     George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as    Ravand and Robitzsch (2015)."
"TestDesign",2,1,8,"2020-09-21 08:50","Use the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and    van der Linden (2018) <doi:10.1201/9781315117430> in constructing fixed and adaptive tests. Supports the following    mixed-integer programming (MIP) solver packages: 'lpsymphony', 'Rsymphony', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package    is not available from CRAN; see <https://www.gurobi.com/downloads/>. See vignette for installing 'Rsymphony' package    on Mac systems."
"rpf",3,4,42,"2020-05-07 13:20","The purpose of this package is to factor out logic    and math common to Item Factor Analysis fitting, diagnostics, and    analysis. It is envisioned as core support code suitable for more    specialized IRT packages to build upon. Complete access to optimized C    functions are made available with R_RegisterCCallable().    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>."
"psychotools",0,14,12,"2018-12-19 23:48","Infrastructure for psychometric modeling such as data classes (for  item response data and paired comparisons), basic model fitting functions (for  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,  rating scale, multinomial processing tree models), extractor functions for  different types of parameters (item, person, threshold, discrimination,  guessing, upper asymptotes), unified inference and visualizations, and various  datasets for illustration.  Intended as a common lightweight and efficient  toolbox for psychometric modeling and a common building block for fitting  psychometric mixture models in package ""psychomix"" and trees based on  psychometric models in package ""psychotree""."
"nonnest2",1,2,11,"2017-12-15 13:22","Testing non-nested models via theory supplied by Vuong (1989) <doi:10.2307/1912557>.    Includes tests of model distinguishability and of model fit that can be applied    to both nested and non-nested models. Also includes functionality to obtain    confidence intervals associated with AIC and BIC. This material is partially based on    work supported by the National Science Foundation under Grant Number SES-1061334."
"LSAmitR",0,2,4,"2018-01-19 08:56","Dieses R-Paket stellt Zusatzmaterial in Form von Daten, Funktionen             und R-Hilfe-Seiten für den Herausgeberband Breit, S. und Schreiner,              C. (Hrsg.). (2016). ""Large-Scale Assessment mit R: Methodische Grundlagen              der österreichischen Bildungsstandardüberprüfung."" Wien: facultas.              (ISBN: 978-3-7089-1343-8, <https://www.bifie.at/node/3770>) zur              Verfügung."
"irtplay",0,1,10,"2020-07-15 15:00","Fit unidimensional item response theory (IRT) models to mixture     of dichotomous and polytomous data, calibrate online item parameters     (i.e., pretest and operational items), estimate examinees abilities,     and examine the IRT model-data fit on item-level in different ways     as well as provide useful functions related to unidimensional IRT models.     For the item parameter estimation, marginal maximum likelihood estimation     with expectation-maximization (MMLE-EM) algorithm     (Bock & Aitkin (1981) <doi:10.1007/BF02294168>) is used.     For the online calibration, Stocking's Method A     (Ban, Hanson, Wang, Yi, & Harris (2011) <doi:10.1111/j.1745-3984.2001.tb01123.x>)     and the fixed item parameter calibration (FIPC) method     (Kim (2006) <doi:10.1111/j.1745-3984.2006.00021.x>) are provided.     For the ability estimation, several popular scoring methods     (e.g., MLE, EAP, and MAP) are implemented. In terms of assessing the IRT     model-data fit, one of distinguished features of this package is that it     gives not only well-known item fit statistics (e.g., chi-square (X2),     likelihood ratio chi-square (G2), infit and oufit statistics, and     S-X2 statistic (Ames & Penfield (2015) <doi:10.1111/emip.12067>))     but also graphical displays to look at residuals between the observed     data and model-based predictions     (Hambleton, Swaminathan, & Rogers (1991, ISBN:9780803936478)).     In addition, there are many useful functions such as computing asymptotic     variance-covariance matrices of item parameter estimates (Li & Lissitz (2004)     <doi:10.1111/j.1745-3984.2004.tb01109.x>), importing item and/or ability     parameters from popular IRT software, running 'flexMIRT' (Cai, 2017)     through R, generating simulated data, computing the conditional     distribution of observed scores using the Lord-Wingersky recursion     formula (Lord & Wingersky (1984) <doi:10.1177/014662168400800409>),     computing the loglikelihood of individual items, computing the loglikelihood     of abilities, computing item and test information functions, computing item     and test characteristic curve functions, and plotting item and test     characteristic curves and item and test information functions."
"ShinyItemAnalysis",0,1,14,"2020-05-04 17:20","Interactive shiny application for analysis of educational tests and    their items."
"scDIFtest",1,1,2,"2020-06-26 11:10","Detection of item-wise Differential Item Functioning (DIF)              in fitted 'mirt', 'multipleGroup' or 'bfactor' models               using score-based structural change tests. Under the hood              the sctest() function from the 'strucchange' package is used."
"PROsetta",1,1,2,"2020-07-01 13:00","Perform scale linking to establish relationships between instruments    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>."
"mstDIF",1,1,2,"2020-07-20 10:40","A collection of statistical tests for the detection of differential    item functioning (DIF) in multistage tests. Methods entail logistic regression,    an adaptation of the simultaneous item bias test (SIBTEST), and various score-based tests.    The presented tests provide itemwise test for DIF along categorical, ordinal or metric covariates. Methods for uniform and non-uniform     DIF effects are available depending on which method is used."
"kequate",2,1,12,"2019-05-31 19:30","Implements the kernel method of test equating as defined in von Davier, A. A., Holland, P. W. and Thayer, D. T. (2004) <doi:10.1007/b97446> and Andersson, B. and Wiberg, M. (2017) <doi:10.1007/s11336-016-9528-7> using the CB, EG, SG, NEAT CE/PSE and NEC designs, supporting Gaussian, logistic and uniform kernels and unsmoothed and pre-smoothed input data."
"jrt",1,1,2,"2019-01-04 12:20","Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225>. A convenience function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions."
"irtreliability",0,1,1,"2018-02-22","Estimation of reliability coefficients for ability estimates and sum scores from item response theory models as defined in Cheng, Y., Yuan, K.-H. and Liu, C. (2012) <doi:10.1177/0013164411407315> and Kim, S. and Feldt, L. S. (2010) <doi:10.1007/s12564-009-9062-8>. The package supports the 3-PL and generalized partial credit models and includes estimates of the standard errors of the reliability coefficient estimators, derived in Andersson, B. and Xin, T. (2018) <doi:10.1177/0013164417713570>."
"GPCMlasso",0,1,4,"2019-02-21 09:30","Provides a framework to detect Differential Item Functioning (DIF) in Generalized Partial Credit Models (GPCM) and special cases of the GPCM as proposed by Schauberger and Mair (2019) <doi:10.3758/s13428-019-01224-2>. A joint model is set up where DIF is explicitly parametrized and penalized likelihood estimation is used for parameter selection. The big advantage of the method called GPCMlasso is that several variables can be treated simultaneously and that both continuous and categorical variables can be used to detect DIF."
"faoutlier",0,1,10,"2017-07-22 07:31","Tools for detecting and summarize influential cases that    can affect exploratory and confirmatory factor analysis models as well as    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>;     Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <10.3389/fpsyg.2012.00055>)."
"equateIRT",2,2,14,"2016-03-30 20:35","Computation of direct, chain and average (bisector) equating coefficients with   standard errors using Item Response Theory (IRT) methods for dichotomous items   (Battauz (2013) <doi:10.1007/s11336-012-9316-y>,   Battauz (2015) <doi:10.18637/jss.v068.i07>).   Test scoring can be performed by true score equating and observed score equating methods.   DIF detection can be performed using a Wald-type test    (Battauz (2018) <doi:10.1007/s10260-018-00442-w>)."
"difR",0,2,20,"2018-05-14 15:38","Provides a collection of standard methods to detect differential item functioning among dichotomously scored items. Methods for uniform and non-uniform DIF, based on test-score or IRT methods, for comparing two or more than two groups of respondents, are available (Magis, Beland, Tuerlinckx and De Boeck,A General Framework and an R Package for the Detection of Dichotomous Differential Item Functioning, Behavior Research Methods, 42, 2010, 847-862 <doi:10.3758/BRM.42.3.847>)."
"ctgdist",0,1,1,"2020-08-10","It is assumed that psychological distances between the categories are equal for the measurement instruments consisted of polytomously scored items. According to Muraki, this assumption must be tested. In the examination process of this assumption, the fit indexes are obtained and evaluated. This package provides that this assumption is removed. By with this package, the converted scale values of all items in a measurement instrument can be calculated by estimating a category parameter set for each item. Thus, the calculations can be made without any need to usage of the common category parameter set. Through this package, the psychological distances of the items are scaled. The scaling of a category parameter set for each item cause differentiation of score of the categories will be got from items. Also, the total measurement instrument score of an individual can be calculated according to the scaling of item score categories by with this package.This package provides that the place of individuals related to the structure to be measured with a measurement instrument consisted of polytomously scored items can be reveal more accurately. In this way, it is thought that the results obtained about individuals can be made more sensitive, and the differences between individuals can be revealed more accurately. On the other hand, it can be argued that more accurate evidences can be obtained regarding the psychometric properties of the measurement instruments."
"airt",1,1,1,"2020-05-12","An evaluation framework for algorithm portfolios using Item Response    Theory (IRT). We use polytomous IRT models to evaluate algorithms and introduce    algorithm characteristics such as stability, effectiveness and anomalousness     (Kandanaarachchi, Smith-Miles 2020) <doi:10.13140/RG.2.2.11363.09760>."
"PerFit",0,1,7,"2015-07-20 09:59","Several person-fit statistics (PFSs) are offered. These statistics allow assessing whether  individual response patterns to tests or questionnaires are (im)plausible given   the other respondents in the sample or given a specified item response theory model. Some PFSs apply to   dichotomous data, such as the likelihood-based PFSs (lz, lz*) and the group-based PFSs   (personal biserial correlation, caution index, (normed) number of Guttman errors,   agreement/disagreement/dependability statistics, U3, ZU3, NCI, Ht). PFSs suitable to polytomous data include  extensions of lz, U3, and (normed) number of Guttman errors."
"OrdinalLogisticBiplot",0,1,4,"2014-05-03 00:36","Analysis of a matrix of polytomous items using Ordinal Logistic Biplots (OLB)  The OLB procedure extends the binary logistic biplot to ordinal (polytomous) data.   The  individuals are represented as points on a plane and the variables are represented   as lines rather than vectors as in a classical or binary biplot, specifying the points  for each of the categories of the variable.   The set of prediction regions is established by stripes perpendicular to the line   between the category points, in such a way that the prediction for each individual is given  by its projection into the line of the variable."
"NominalLogisticBiplot",0,1,2,"2013-09-17 14:47","Analysis of a matrix of polytomous items using Nominal Logistic Biplots (NLB)  according to Hernandez-Sanchez and Vicente-Villardon (2013).   The NLB procedure extends the binary logistic biplot to nominal (polytomous) data.   The  individuals are represented as points on a plane and the  variables are represented   as convex prediction regions rather than vectors as in a classical or binary biplot.   Using the methods from Computational Geometry, the set of prediction regions is converted to a set of points   in such a way that the prediction for each individual is established by its closest   ""category point"". Then interpretation is based on distances rather than on projections.   In this package we implement the geometry of such a representation and construct computational algorithms   for the estimation of parameters and the calculation of prediction regions."
"lordif",0,1,14,"2015-09-18 08:58","Analysis of Differential Item Functioning (DIF) for        dichotomous and polytomous items using an iterative hybrid of        ordinal logistic regression and item response theory (IRT)."
"CopyDetect",0,1,4,"2016-04-27 19:28","Contains several IRT and non-IRT based response similarity indices proposed in the literature for multiple-choice examinations such as the Omega index, Wollack (1997) <doi:10.1177/01466216970214002>; Generalized Binomial Test, van der Linden & Sotaridona (2006) <doi:10.3102/10769986031003283>; K index, K1 and K2 indices, Sotaridona & Meijer (2002) <doi:10.1111/j.1745-3984.2002.tb01138.x>;  and S1 and S2 indices, Sotaridona & Meijer (2003) <doi:10.1111/j.1745-3984.2003.tb01096.x>."
"CDM",0,12,50,"2019-09-05 06:40","    Functions for cognitive diagnosis modeling and multidimensional item response modeling     for dichotomous and polytomous item responses. This package enables the estimation of     the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),    the multiple group (polytomous) GDINA model (de la Torre, 2011,     <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009,     <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008,     <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992,     <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis     (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>).     See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02>     or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)         for further details on estimation and the package structure.    For tutorials on how to use the CDM package see     George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as    Ravand and Robitzsch (2015)."
"TestDesign",2,1,8,"2020-09-21 08:50","Use the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and    van der Linden (2018) <doi:10.1201/9781315117430> in constructing fixed and adaptive tests. Supports the following    mixed-integer programming (MIP) solver packages: 'lpsymphony', 'Rsymphony', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package    is not available from CRAN; see <https://www.gurobi.com/downloads/>. See vignette for installing 'Rsymphony' package    on Mac systems."
"rpf",3,4,42,"2020-05-07 13:20","The purpose of this package is to factor out logic    and math common to Item Factor Analysis fitting, diagnostics, and    analysis. It is envisioned as core support code suitable for more    specialized IRT packages to build upon. Complete access to optimized C    functions are made available with R_RegisterCCallable().    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>."
"psychotools",0,14,12,"2018-12-19 23:48","Infrastructure for psychometric modeling such as data classes (for  item response data and paired comparisons), basic model fitting functions (for  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,  rating scale, multinomial processing tree models), extractor functions for  different types of parameters (item, person, threshold, discrimination,  guessing, upper asymptotes), unified inference and visualizations, and various  datasets for illustration.  Intended as a common lightweight and efficient  toolbox for psychometric modeling and a common building block for fitting  psychometric mixture models in package ""psychomix"" and trees based on  psychometric models in package ""psychotree""."
"nonnest2",1,2,11,"2017-12-15 13:22","Testing non-nested models via theory supplied by Vuong (1989) <doi:10.2307/1912557>.    Includes tests of model distinguishability and of model fit that can be applied    to both nested and non-nested models. Also includes functionality to obtain    confidence intervals associated with AIC and BIC. This material is partially based on    work supported by the National Science Foundation under Grant Number SES-1061334."
"LSAmitR",0,2,4,"2018-01-19 08:56","Dieses R-Paket stellt Zusatzmaterial in Form von Daten, Funktionen             und R-Hilfe-Seiten für den Herausgeberband Breit, S. und Schreiner,              C. (Hrsg.). (2016). ""Large-Scale Assessment mit R: Methodische Grundlagen              der österreichischen Bildungsstandardüberprüfung."" Wien: facultas.              (ISBN: 978-3-7089-1343-8, <https://www.bifie.at/node/3770>) zur              Verfügung."
"irtplay",0,1,10,"2020-07-15 15:00","Fit unidimensional item response theory (IRT) models to mixture     of dichotomous and polytomous data, calibrate online item parameters     (i.e., pretest and operational items), estimate examinees abilities,     and examine the IRT model-data fit on item-level in different ways     as well as provide useful functions related to unidimensional IRT models.     For the item parameter estimation, marginal maximum likelihood estimation     with expectation-maximization (MMLE-EM) algorithm     (Bock & Aitkin (1981) <doi:10.1007/BF02294168>) is used.     For the online calibration, Stocking's Method A     (Ban, Hanson, Wang, Yi, & Harris (2011) <doi:10.1111/j.1745-3984.2001.tb01123.x>)     and the fixed item parameter calibration (FIPC) method     (Kim (2006) <doi:10.1111/j.1745-3984.2006.00021.x>) are provided.     For the ability estimation, several popular scoring methods     (e.g., MLE, EAP, and MAP) are implemented. In terms of assessing the IRT     model-data fit, one of distinguished features of this package is that it     gives not only well-known item fit statistics (e.g., chi-square (X2),     likelihood ratio chi-square (G2), infit and oufit statistics, and     S-X2 statistic (Ames & Penfield (2015) <doi:10.1111/emip.12067>))     but also graphical displays to look at residuals between the observed     data and model-based predictions     (Hambleton, Swaminathan, & Rogers (1991, ISBN:9780803936478)).     In addition, there are many useful functions such as computing asymptotic     variance-covariance matrices of item parameter estimates (Li & Lissitz (2004)     <doi:10.1111/j.1745-3984.2004.tb01109.x>), importing item and/or ability     parameters from popular IRT software, running 'flexMIRT' (Cai, 2017)     through R, generating simulated data, computing the conditional     distribution of observed scores using the Lord-Wingersky recursion     formula (Lord & Wingersky (1984) <doi:10.1177/014662168400800409>),     computing the loglikelihood of individual items, computing the loglikelihood     of abilities, computing item and test information functions, computing item     and test characteristic curve functions, and plotting item and test     characteristic curves and item and test information functions."
"ShinyItemAnalysis",0,1,14,"2020-05-04 17:20","Interactive shiny application for analysis of educational tests and    their items."
"scDIFtest",1,1,2,"2020-06-26 11:10","Detection of item-wise Differential Item Functioning (DIF)              in fitted 'mirt', 'multipleGroup' or 'bfactor' models               using score-based structural change tests. Under the hood              the sctest() function from the 'strucchange' package is used."
"PROsetta",1,1,2,"2020-07-01 13:00","Perform scale linking to establish relationships between instruments    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>."
"mstDIF",1,1,2,"2020-07-20 10:40","A collection of statistical tests for the detection of differential    item functioning (DIF) in multistage tests. Methods entail logistic regression,    an adaptation of the simultaneous item bias test (SIBTEST), and various score-based tests.    The presented tests provide itemwise test for DIF along categorical, ordinal or metric covariates. Methods for uniform and non-uniform     DIF effects are available depending on which method is used."
"kequate",2,1,12,"2019-05-31 19:30","Implements the kernel method of test equating as defined in von Davier, A. A., Holland, P. W. and Thayer, D. T. (2004) <doi:10.1007/b97446> and Andersson, B. and Wiberg, M. (2017) <doi:10.1007/s11336-016-9528-7> using the CB, EG, SG, NEAT CE/PSE and NEC designs, supporting Gaussian, logistic and uniform kernels and unsmoothed and pre-smoothed input data."
"jrt",1,1,2,"2019-01-04 12:20","Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225>. A convenience function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions."
"irtreliability",0,1,1,"2018-02-22","Estimation of reliability coefficients for ability estimates and sum scores from item response theory models as defined in Cheng, Y., Yuan, K.-H. and Liu, C. (2012) <doi:10.1177/0013164411407315> and Kim, S. and Feldt, L. S. (2010) <doi:10.1007/s12564-009-9062-8>. The package supports the 3-PL and generalized partial credit models and includes estimates of the standard errors of the reliability coefficient estimators, derived in Andersson, B. and Xin, T. (2018) <doi:10.1177/0013164417713570>."
"GPCMlasso",0,1,4,"2019-02-21 09:30","Provides a framework to detect Differential Item Functioning (DIF) in Generalized Partial Credit Models (GPCM) and special cases of the GPCM as proposed by Schauberger and Mair (2019) <doi:10.3758/s13428-019-01224-2>. A joint model is set up where DIF is explicitly parametrized and penalized likelihood estimation is used for parameter selection. The big advantage of the method called GPCMlasso is that several variables can be treated simultaneously and that both continuous and categorical variables can be used to detect DIF."
"faoutlier",0,1,10,"2017-07-22 07:31","Tools for detecting and summarize influential cases that    can affect exploratory and confirmatory factor analysis models as well as    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>;     Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <10.3389/fpsyg.2012.00055>)."
"equateIRT",2,2,14,"2016-03-30 20:35","Computation of direct, chain and average (bisector) equating coefficients with   standard errors using Item Response Theory (IRT) methods for dichotomous items   (Battauz (2013) <doi:10.1007/s11336-012-9316-y>,   Battauz (2015) <doi:10.18637/jss.v068.i07>).   Test scoring can be performed by true score equating and observed score equating methods.   DIF detection can be performed using a Wald-type test    (Battauz (2018) <doi:10.1007/s10260-018-00442-w>)."
"difR",0,2,20,"2018-05-14 15:38","Provides a collection of standard methods to detect differential item functioning among dichotomously scored items. Methods for uniform and non-uniform DIF, based on test-score or IRT methods, for comparing two or more than two groups of respondents, are available (Magis, Beland, Tuerlinckx and De Boeck,A General Framework and an R Package for the Detection of Dichotomous Differential Item Functioning, Behavior Research Methods, 42, 2010, 847-862 <doi:10.3758/BRM.42.3.847>)."
"ctgdist",0,1,1,"2020-08-10","It is assumed that psychological distances between the categories are equal for the measurement instruments consisted of polytomously scored items. According to Muraki, this assumption must be tested. In the examination process of this assumption, the fit indexes are obtained and evaluated. This package provides that this assumption is removed. By with this package, the converted scale values of all items in a measurement instrument can be calculated by estimating a category parameter set for each item. Thus, the calculations can be made without any need to usage of the common category parameter set. Through this package, the psychological distances of the items are scaled. The scaling of a category parameter set for each item cause differentiation of score of the categories will be got from items. Also, the total measurement instrument score of an individual can be calculated according to the scaling of item score categories by with this package.This package provides that the place of individuals related to the structure to be measured with a measurement instrument consisted of polytomously scored items can be reveal more accurately. In this way, it is thought that the results obtained about individuals can be made more sensitive, and the differences between individuals can be revealed more accurately. On the other hand, it can be argued that more accurate evidences can be obtained regarding the psychometric properties of the measurement instruments."
"airt",1,1,1,"2020-05-12","An evaluation framework for algorithm portfolios using Item Response    Theory (IRT). We use polytomous IRT models to evaluate algorithms and introduce    algorithm characteristics such as stability, effectiveness and anomalousness     (Kandanaarachchi, Smith-Miles 2020) <doi:10.13140/RG.2.2.11363.09760>."
"PerFit",0,1,7,"2015-07-20 09:59","Several person-fit statistics (PFSs) are offered. These statistics allow assessing whether  individual response patterns to tests or questionnaires are (im)plausible given   the other respondents in the sample or given a specified item response theory model. Some PFSs apply to   dichotomous data, such as the likelihood-based PFSs (lz, lz*) and the group-based PFSs   (personal biserial correlation, caution index, (normed) number of Guttman errors,   agreement/disagreement/dependability statistics, U3, ZU3, NCI, Ht). PFSs suitable to polytomous data include  extensions of lz, U3, and (normed) number of Guttman errors."
"OrdinalLogisticBiplot",0,1,4,"2014-05-03 00:36","Analysis of a matrix of polytomous items using Ordinal Logistic Biplots (OLB)  The OLB procedure extends the binary logistic biplot to ordinal (polytomous) data.   The  individuals are represented as points on a plane and the variables are represented   as lines rather than vectors as in a classical or binary biplot, specifying the points  for each of the categories of the variable.   The set of prediction regions is established by stripes perpendicular to the line   between the category points, in such a way that the prediction for each individual is given  by its projection into the line of the variable."
"NominalLogisticBiplot",0,1,2,"2013-09-17 14:47","Analysis of a matrix of polytomous items using Nominal Logistic Biplots (NLB)  according to Hernandez-Sanchez and Vicente-Villardon (2013).   The NLB procedure extends the binary logistic biplot to nominal (polytomous) data.   The  individuals are represented as points on a plane and the  variables are represented   as convex prediction regions rather than vectors as in a classical or binary biplot.   Using the methods from Computational Geometry, the set of prediction regions is converted to a set of points   in such a way that the prediction for each individual is established by its closest   ""category point"". Then interpretation is based on distances rather than on projections.   In this package we implement the geometry of such a representation and construct computational algorithms   for the estimation of parameters and the calculation of prediction regions."
"lordif",0,1,14,"2015-09-18 08:58","Analysis of Differential Item Functioning (DIF) for        dichotomous and polytomous items using an iterative hybrid of        ordinal logistic regression and item response theory (IRT)."
"CopyDetect",0,1,4,"2016-04-27 19:28","Contains several IRT and non-IRT based response similarity indices proposed in the literature for multiple-choice examinations such as the Omega index, Wollack (1997) <doi:10.1177/01466216970214002>; Generalized Binomial Test, van der Linden & Sotaridona (2006) <doi:10.3102/10769986031003283>; K index, K1 and K2 indices, Sotaridona & Meijer (2002) <doi:10.1111/j.1745-3984.2002.tb01138.x>;  and S1 and S2 indices, Sotaridona & Meijer (2003) <doi:10.1111/j.1745-3984.2003.tb01096.x>."
"CDM",0,12,50,"2019-09-05 06:40","    Functions for cognitive diagnosis modeling and multidimensional item response modeling     for dichotomous and polytomous item responses. This package enables the estimation of     the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),    the multiple group (polytomous) GDINA model (de la Torre, 2011,     <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009,     <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008,     <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992,     <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis     (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>).     See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02>     or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)         for further details on estimation and the package structure.    For tutorials on how to use the CDM package see     George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as    Ravand and Robitzsch (2015)."
"TestDesign",2,1,8,"2020-09-21 08:50","Use the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and    van der Linden (2018) <doi:10.1201/9781315117430> in constructing fixed and adaptive tests. Supports the following    mixed-integer programming (MIP) solver packages: 'lpsymphony', 'Rsymphony', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package    is not available from CRAN; see <https://www.gurobi.com/downloads/>. See vignette for installing 'Rsymphony' package    on Mac systems."
"rpf",3,4,42,"2020-05-07 13:20","The purpose of this package is to factor out logic    and math common to Item Factor Analysis fitting, diagnostics, and    analysis. It is envisioned as core support code suitable for more    specialized IRT packages to build upon. Complete access to optimized C    functions are made available with R_RegisterCCallable().    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>."
"psychotools",0,14,12,"2018-12-19 23:48","Infrastructure for psychometric modeling such as data classes (for  item response data and paired comparisons), basic model fitting functions (for  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,  rating scale, multinomial processing tree models), extractor functions for  different types of parameters (item, person, threshold, discrimination,  guessing, upper asymptotes), unified inference and visualizations, and various  datasets for illustration.  Intended as a common lightweight and efficient  toolbox for psychometric modeling and a common building block for fitting  psychometric mixture models in package ""psychomix"" and trees based on  psychometric models in package ""psychotree""."
"nonnest2",1,2,11,"2017-12-15 13:22","Testing non-nested models via theory supplied by Vuong (1989) <doi:10.2307/1912557>.    Includes tests of model distinguishability and of model fit that can be applied    to both nested and non-nested models. Also includes functionality to obtain    confidence intervals associated with AIC and BIC. This material is partially based on    work supported by the National Science Foundation under Grant Number SES-1061334."
"LSAmitR",0,2,4,"2018-01-19 08:56","Dieses R-Paket stellt Zusatzmaterial in Form von Daten, Funktionen             und R-Hilfe-Seiten für den Herausgeberband Breit, S. und Schreiner,              C. (Hrsg.). (2016). ""Large-Scale Assessment mit R: Methodische Grundlagen              der österreichischen Bildungsstandardüberprüfung."" Wien: facultas.              (ISBN: 978-3-7089-1343-8, <https://www.bifie.at/node/3770>) zur              Verfügung."
"irtplay",0,1,10,"2020-07-15 15:00","Fit unidimensional item response theory (IRT) models to mixture     of dichotomous and polytomous data, calibrate online item parameters     (i.e., pretest and operational items), estimate examinees abilities,     and examine the IRT model-data fit on item-level in different ways     as well as provide useful functions related to unidimensional IRT models.     For the item parameter estimation, marginal maximum likelihood estimation     with expectation-maximization (MMLE-EM) algorithm     (Bock & Aitkin (1981) <doi:10.1007/BF02294168>) is used.     For the online calibration, Stocking's Method A     (Ban, Hanson, Wang, Yi, & Harris (2011) <doi:10.1111/j.1745-3984.2001.tb01123.x>)     and the fixed item parameter calibration (FIPC) method     (Kim (2006) <doi:10.1111/j.1745-3984.2006.00021.x>) are provided.     For the ability estimation, several popular scoring methods     (e.g., MLE, EAP, and MAP) are implemented. In terms of assessing the IRT     model-data fit, one of distinguished features of this package is that it     gives not only well-known item fit statistics (e.g., chi-square (X2),     likelihood ratio chi-square (G2), infit and oufit statistics, and     S-X2 statistic (Ames & Penfield (2015) <doi:10.1111/emip.12067>))     but also graphical displays to look at residuals between the observed     data and model-based predictions     (Hambleton, Swaminathan, & Rogers (1991, ISBN:9780803936478)).     In addition, there are many useful functions such as computing asymptotic     variance-covariance matrices of item parameter estimates (Li & Lissitz (2004)     <doi:10.1111/j.1745-3984.2004.tb01109.x>), importing item and/or ability     parameters from popular IRT software, running 'flexMIRT' (Cai, 2017)     through R, generating simulated data, computing the conditional     distribution of observed scores using the Lord-Wingersky recursion     formula (Lord & Wingersky (1984) <doi:10.1177/014662168400800409>),     computing the loglikelihood of individual items, computing the loglikelihood     of abilities, computing item and test information functions, computing item     and test characteristic curve functions, and plotting item and test     characteristic curves and item and test information functions."
"ShinyItemAnalysis",0,1,14,"2020-05-04 17:20","Interactive shiny application for analysis of educational tests and    their items."
"scDIFtest",1,1,2,"2020-06-26 11:10","Detection of item-wise Differential Item Functioning (DIF)              in fitted 'mirt', 'multipleGroup' or 'bfactor' models               using score-based structural change tests. Under the hood              the sctest() function from the 'strucchange' package is used."
"PROsetta",1,1,2,"2020-07-01 13:00","Perform scale linking to establish relationships between instruments    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>."
"mstDIF",1,1,2,"2020-07-20 10:40","A collection of statistical tests for the detection of differential    item functioning (DIF) in multistage tests. Methods entail logistic regression,    an adaptation of the simultaneous item bias test (SIBTEST), and various score-based tests.    The presented tests provide itemwise test for DIF along categorical, ordinal or metric covariates. Methods for uniform and non-uniform     DIF effects are available depending on which method is used."
"kequate",2,1,12,"2019-05-31 19:30","Implements the kernel method of test equating as defined in von Davier, A. A., Holland, P. W. and Thayer, D. T. (2004) <doi:10.1007/b97446> and Andersson, B. and Wiberg, M. (2017) <doi:10.1007/s11336-016-9528-7> using the CB, EG, SG, NEAT CE/PSE and NEC designs, supporting Gaussian, logistic and uniform kernels and unsmoothed and pre-smoothed input data."
"jrt",1,1,2,"2019-01-04 12:20","Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225>. A convenience function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions."
"irtreliability",0,1,1,"2018-02-22","Estimation of reliability coefficients for ability estimates and sum scores from item response theory models as defined in Cheng, Y., Yuan, K.-H. and Liu, C. (2012) <doi:10.1177/0013164411407315> and Kim, S. and Feldt, L. S. (2010) <doi:10.1007/s12564-009-9062-8>. The package supports the 3-PL and generalized partial credit models and includes estimates of the standard errors of the reliability coefficient estimators, derived in Andersson, B. and Xin, T. (2018) <doi:10.1177/0013164417713570>."
"GPCMlasso",0,1,4,"2019-02-21 09:30","Provides a framework to detect Differential Item Functioning (DIF) in Generalized Partial Credit Models (GPCM) and special cases of the GPCM as proposed by Schauberger and Mair (2019) <doi:10.3758/s13428-019-01224-2>. A joint model is set up where DIF is explicitly parametrized and penalized likelihood estimation is used for parameter selection. The big advantage of the method called GPCMlasso is that several variables can be treated simultaneously and that both continuous and categorical variables can be used to detect DIF."
"faoutlier",0,1,10,"2017-07-22 07:31","Tools for detecting and summarize influential cases that    can affect exploratory and confirmatory factor analysis models as well as    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>;     Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <10.3389/fpsyg.2012.00055>)."
"equateIRT",2,2,14,"2016-03-30 20:35","Computation of direct, chain and average (bisector) equating coefficients with   standard errors using Item Response Theory (IRT) methods for dichotomous items   (Battauz (2013) <doi:10.1007/s11336-012-9316-y>,   Battauz (2015) <doi:10.18637/jss.v068.i07>).   Test scoring can be performed by true score equating and observed score equating methods.   DIF detection can be performed using a Wald-type test    (Battauz (2018) <doi:10.1007/s10260-018-00442-w>)."
"difR",0,2,20,"2018-05-14 15:38","Provides a collection of standard methods to detect differential item functioning among dichotomously scored items. Methods for uniform and non-uniform DIF, based on test-score or IRT methods, for comparing two or more than two groups of respondents, are available (Magis, Beland, Tuerlinckx and De Boeck,A General Framework and an R Package for the Detection of Dichotomous Differential Item Functioning, Behavior Research Methods, 42, 2010, 847-862 <doi:10.3758/BRM.42.3.847>)."
"ctgdist",0,1,1,"2020-08-10","It is assumed that psychological distances between the categories are equal for the measurement instruments consisted of polytomously scored items. According to Muraki, this assumption must be tested. In the examination process of this assumption, the fit indexes are obtained and evaluated. This package provides that this assumption is removed. By with this package, the converted scale values of all items in a measurement instrument can be calculated by estimating a category parameter set for each item. Thus, the calculations can be made without any need to usage of the common category parameter set. Through this package, the psychological distances of the items are scaled. The scaling of a category parameter set for each item cause differentiation of score of the categories will be got from items. Also, the total measurement instrument score of an individual can be calculated according to the scaling of item score categories by with this package.This package provides that the place of individuals related to the structure to be measured with a measurement instrument consisted of polytomously scored items can be reveal more accurately. In this way, it is thought that the results obtained about individuals can be made more sensitive, and the differences between individuals can be revealed more accurately. On the other hand, it can be argued that more accurate evidences can be obtained regarding the psychometric properties of the measurement instruments."
"airt",1,1,1,"2020-05-12","An evaluation framework for algorithm portfolios using Item Response    Theory (IRT). We use polytomous IRT models to evaluate algorithms and introduce    algorithm characteristics such as stability, effectiveness and anomalousness     (Kandanaarachchi, Smith-Miles 2020) <doi:10.13140/RG.2.2.11363.09760>."
"PerFit",0,1,7,"2015-07-20 09:59","Several person-fit statistics (PFSs) are offered. These statistics allow assessing whether  individual response patterns to tests or questionnaires are (im)plausible given   the other respondents in the sample or given a specified item response theory model. Some PFSs apply to   dichotomous data, such as the likelihood-based PFSs (lz, lz*) and the group-based PFSs   (personal biserial correlation, caution index, (normed) number of Guttman errors,   agreement/disagreement/dependability statistics, U3, ZU3, NCI, Ht). PFSs suitable to polytomous data include  extensions of lz, U3, and (normed) number of Guttman errors."
"OrdinalLogisticBiplot",0,1,4,"2014-05-03 00:36","Analysis of a matrix of polytomous items using Ordinal Logistic Biplots (OLB)  The OLB procedure extends the binary logistic biplot to ordinal (polytomous) data.   The  individuals are represented as points on a plane and the variables are represented   as lines rather than vectors as in a classical or binary biplot, specifying the points  for each of the categories of the variable.   The set of prediction regions is established by stripes perpendicular to the line   between the category points, in such a way that the prediction for each individual is given  by its projection into the line of the variable."
"NominalLogisticBiplot",0,1,2,"2013-09-17 14:47","Analysis of a matrix of polytomous items using Nominal Logistic Biplots (NLB)  according to Hernandez-Sanchez and Vicente-Villardon (2013).   The NLB procedure extends the binary logistic biplot to nominal (polytomous) data.   The  individuals are represented as points on a plane and the  variables are represented   as convex prediction regions rather than vectors as in a classical or binary biplot.   Using the methods from Computational Geometry, the set of prediction regions is converted to a set of points   in such a way that the prediction for each individual is established by its closest   ""category point"". Then interpretation is based on distances rather than on projections.   In this package we implement the geometry of such a representation and construct computational algorithms   for the estimation of parameters and the calculation of prediction regions."
"lordif",0,1,14,"2015-09-18 08:58","Analysis of Differential Item Functioning (DIF) for        dichotomous and polytomous items using an iterative hybrid of        ordinal logistic regression and item response theory (IRT)."
"CopyDetect",0,1,4,"2016-04-27 19:28","Contains several IRT and non-IRT based response similarity indices proposed in the literature for multiple-choice examinations such as the Omega index, Wollack (1997) <doi:10.1177/01466216970214002>; Generalized Binomial Test, van der Linden & Sotaridona (2006) <doi:10.3102/10769986031003283>; K index, K1 and K2 indices, Sotaridona & Meijer (2002) <doi:10.1111/j.1745-3984.2002.tb01138.x>;  and S1 and S2 indices, Sotaridona & Meijer (2003) <doi:10.1111/j.1745-3984.2003.tb01096.x>."
"CDM",0,12,50,"2019-09-05 06:40","    Functions for cognitive diagnosis modeling and multidimensional item response modeling     for dichotomous and polytomous item responses. This package enables the estimation of     the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),    the multiple group (polytomous) GDINA model (de la Torre, 2011,     <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009,     <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008,     <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992,     <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis     (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>).     See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02>     or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)         for further details on estimation and the package structure.    For tutorials on how to use the CDM package see     George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as    Ravand and Robitzsch (2015)."
"TestDesign",2,1,8,"2020-09-21 08:50","Use the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and    van der Linden (2018) <doi:10.1201/9781315117430> in constructing fixed and adaptive tests. Supports the following    mixed-integer programming (MIP) solver packages: 'lpsymphony', 'Rsymphony', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package    is not available from CRAN; see <https://www.gurobi.com/downloads/>. See vignette for installing 'Rsymphony' package    on Mac systems."
"rpf",3,4,42,"2020-05-07 13:20","The purpose of this package is to factor out logic    and math common to Item Factor Analysis fitting, diagnostics, and    analysis. It is envisioned as core support code suitable for more    specialized IRT packages to build upon. Complete access to optimized C    functions are made available with R_RegisterCCallable().    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>."
"psychotools",0,14,12,"2018-12-19 23:48","Infrastructure for psychometric modeling such as data classes (for  item response data and paired comparisons), basic model fitting functions (for  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,  rating scale, multinomial processing tree models), extractor functions for  different types of parameters (item, person, threshold, discrimination,  guessing, upper asymptotes), unified inference and visualizations, and various  datasets for illustration.  Intended as a common lightweight and efficient  toolbox for psychometric modeling and a common building block for fitting  psychometric mixture models in package ""psychomix"" and trees based on  psychometric models in package ""psychotree""."
"nonnest2",1,2,11,"2017-12-15 13:22","Testing non-nested models via theory supplied by Vuong (1989) <doi:10.2307/1912557>.    Includes tests of model distinguishability and of model fit that can be applied    to both nested and non-nested models. Also includes functionality to obtain    confidence intervals associated with AIC and BIC. This material is partially based on    work supported by the National Science Foundation under Grant Number SES-1061334."
"LSAmitR",0,2,4,"2018-01-19 08:56","Dieses R-Paket stellt Zusatzmaterial in Form von Daten, Funktionen             und R-Hilfe-Seiten für den Herausgeberband Breit, S. und Schreiner,              C. (Hrsg.). (2016). ""Large-Scale Assessment mit R: Methodische Grundlagen              der österreichischen Bildungsstandardüberprüfung."" Wien: facultas.              (ISBN: 978-3-7089-1343-8, <https://www.bifie.at/node/3770>) zur              Verfügung."
"irtplay",0,1,10,"2020-07-15 15:00","Fit unidimensional item response theory (IRT) models to mixture     of dichotomous and polytomous data, calibrate online item parameters     (i.e., pretest and operational items), estimate examinees abilities,     and examine the IRT model-data fit on item-level in different ways     as well as provide useful functions related to unidimensional IRT models.     For the item parameter estimation, marginal maximum likelihood estimation     with expectation-maximization (MMLE-EM) algorithm     (Bock & Aitkin (1981) <doi:10.1007/BF02294168>) is used.     For the online calibration, Stocking's Method A     (Ban, Hanson, Wang, Yi, & Harris (2011) <doi:10.1111/j.1745-3984.2001.tb01123.x>)     and the fixed item parameter calibration (FIPC) method     (Kim (2006) <doi:10.1111/j.1745-3984.2006.00021.x>) are provided.     For the ability estimation, several popular scoring methods     (e.g., MLE, EAP, and MAP) are implemented. In terms of assessing the IRT     model-data fit, one of distinguished features of this package is that it     gives not only well-known item fit statistics (e.g., chi-square (X2),     likelihood ratio chi-square (G2), infit and oufit statistics, and     S-X2 statistic (Ames & Penfield (2015) <doi:10.1111/emip.12067>))     but also graphical displays to look at residuals between the observed     data and model-based predictions     (Hambleton, Swaminathan, & Rogers (1991, ISBN:9780803936478)).     In addition, there are many useful functions such as computing asymptotic     variance-covariance matrices of item parameter estimates (Li & Lissitz (2004)     <doi:10.1111/j.1745-3984.2004.tb01109.x>), importing item and/or ability     parameters from popular IRT software, running 'flexMIRT' (Cai, 2017)     through R, generating simulated data, computing the conditional     distribution of observed scores using the Lord-Wingersky recursion     formula (Lord & Wingersky (1984) <doi:10.1177/014662168400800409>),     computing the loglikelihood of individual items, computing the loglikelihood     of abilities, computing item and test information functions, computing item     and test characteristic curve functions, and plotting item and test     characteristic curves and item and test information functions."
"ShinyItemAnalysis",0,1,14,"2020-05-04 17:20","Interactive shiny application for analysis of educational tests and    their items."
"scDIFtest",1,1,2,"2020-06-26 11:10","Detection of item-wise Differential Item Functioning (DIF)              in fitted 'mirt', 'multipleGroup' or 'bfactor' models               using score-based structural change tests. Under the hood              the sctest() function from the 'strucchange' package is used."
"PROsetta",1,1,2,"2020-07-01 13:00","Perform scale linking to establish relationships between instruments    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>."
"mstDIF",1,1,2,"2020-07-20 10:40","A collection of statistical tests for the detection of differential    item functioning (DIF) in multistage tests. Methods entail logistic regression,    an adaptation of the simultaneous item bias test (SIBTEST), and various score-based tests.    The presented tests provide itemwise test for DIF along categorical, ordinal or metric covariates. Methods for uniform and non-uniform     DIF effects are available depending on which method is used."
"kequate",2,1,12,"2019-05-31 19:30","Implements the kernel method of test equating as defined in von Davier, A. A., Holland, P. W. and Thayer, D. T. (2004) <doi:10.1007/b97446> and Andersson, B. and Wiberg, M. (2017) <doi:10.1007/s11336-016-9528-7> using the CB, EG, SG, NEAT CE/PSE and NEC designs, supporting Gaussian, logistic and uniform kernels and unsmoothed and pre-smoothed input data."
"jrt",1,1,2,"2019-01-04 12:20","Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225>. A convenience function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions."
"irtreliability",0,1,1,"2018-02-22","Estimation of reliability coefficients for ability estimates and sum scores from item response theory models as defined in Cheng, Y., Yuan, K.-H. and Liu, C. (2012) <doi:10.1177/0013164411407315> and Kim, S. and Feldt, L. S. (2010) <doi:10.1007/s12564-009-9062-8>. The package supports the 3-PL and generalized partial credit models and includes estimates of the standard errors of the reliability coefficient estimators, derived in Andersson, B. and Xin, T. (2018) <doi:10.1177/0013164417713570>."
"GPCMlasso",0,1,4,"2019-02-21 09:30","Provides a framework to detect Differential Item Functioning (DIF) in Generalized Partial Credit Models (GPCM) and special cases of the GPCM as proposed by Schauberger and Mair (2019) <doi:10.3758/s13428-019-01224-2>. A joint model is set up where DIF is explicitly parametrized and penalized likelihood estimation is used for parameter selection. The big advantage of the method called GPCMlasso is that several variables can be treated simultaneously and that both continuous and categorical variables can be used to detect DIF."
"faoutlier",0,1,10,"2017-07-22 07:31","Tools for detecting and summarize influential cases that    can affect exploratory and confirmatory factor analysis models as well as    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>;     Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <10.3389/fpsyg.2012.00055>)."
"equateIRT",2,2,14,"2016-03-30 20:35","Computation of direct, chain and average (bisector) equating coefficients with   standard errors using Item Response Theory (IRT) methods for dichotomous items   (Battauz (2013) <doi:10.1007/s11336-012-9316-y>,   Battauz (2015) <doi:10.18637/jss.v068.i07>).   Test scoring can be performed by true score equating and observed score equating methods.   DIF detection can be performed using a Wald-type test    (Battauz (2018) <doi:10.1007/s10260-018-00442-w>)."
"difR",0,2,20,"2018-05-14 15:38","Provides a collection of standard methods to detect differential item functioning among dichotomously scored items. Methods for uniform and non-uniform DIF, based on test-score or IRT methods, for comparing two or more than two groups of respondents, are available (Magis, Beland, Tuerlinckx and De Boeck,A General Framework and an R Package for the Detection of Dichotomous Differential Item Functioning, Behavior Research Methods, 42, 2010, 847-862 <doi:10.3758/BRM.42.3.847>)."
"ctgdist",0,1,1,"2020-08-10","It is assumed that psychological distances between the categories are equal for the measurement instruments consisted of polytomously scored items. According to Muraki, this assumption must be tested. In the examination process of this assumption, the fit indexes are obtained and evaluated. This package provides that this assumption is removed. By with this package, the converted scale values of all items in a measurement instrument can be calculated by estimating a category parameter set for each item. Thus, the calculations can be made without any need to usage of the common category parameter set. Through this package, the psychological distances of the items are scaled. The scaling of a category parameter set for each item cause differentiation of score of the categories will be got from items. Also, the total measurement instrument score of an individual can be calculated according to the scaling of item score categories by with this package.This package provides that the place of individuals related to the structure to be measured with a measurement instrument consisted of polytomously scored items can be reveal more accurately. In this way, it is thought that the results obtained about individuals can be made more sensitive, and the differences between individuals can be revealed more accurately. On the other hand, it can be argued that more accurate evidences can be obtained regarding the psychometric properties of the measurement instruments."
"airt",1,1,1,"2020-05-12","An evaluation framework for algorithm portfolios using Item Response    Theory (IRT). We use polytomous IRT models to evaluate algorithms and introduce    algorithm characteristics such as stability, effectiveness and anomalousness     (Kandanaarachchi, Smith-Miles 2020) <doi:10.13140/RG.2.2.11363.09760>."
"PerFit",0,1,7,"2015-07-20 09:59","Several person-fit statistics (PFSs) are offered. These statistics allow assessing whether  individual response patterns to tests or questionnaires are (im)plausible given   the other respondents in the sample or given a specified item response theory model. Some PFSs apply to   dichotomous data, such as the likelihood-based PFSs (lz, lz*) and the group-based PFSs   (personal biserial correlation, caution index, (normed) number of Guttman errors,   agreement/disagreement/dependability statistics, U3, ZU3, NCI, Ht). PFSs suitable to polytomous data include  extensions of lz, U3, and (normed) number of Guttman errors."
"OrdinalLogisticBiplot",0,1,4,"2014-05-03 00:36","Analysis of a matrix of polytomous items using Ordinal Logistic Biplots (OLB)  The OLB procedure extends the binary logistic biplot to ordinal (polytomous) data.   The  individuals are represented as points on a plane and the variables are represented   as lines rather than vectors as in a classical or binary biplot, specifying the points  for each of the categories of the variable.   The set of prediction regions is established by stripes perpendicular to the line   between the category points, in such a way that the prediction for each individual is given  by its projection into the line of the variable."
"NominalLogisticBiplot",0,1,2,"2013-09-17 14:47","Analysis of a matrix of polytomous items using Nominal Logistic Biplots (NLB)  according to Hernandez-Sanchez and Vicente-Villardon (2013).   The NLB procedure extends the binary logistic biplot to nominal (polytomous) data.   The  individuals are represented as points on a plane and the  variables are represented   as convex prediction regions rather than vectors as in a classical or binary biplot.   Using the methods from Computational Geometry, the set of prediction regions is converted to a set of points   in such a way that the prediction for each individual is established by its closest   ""category point"". Then interpretation is based on distances rather than on projections.   In this package we implement the geometry of such a representation and construct computational algorithms   for the estimation of parameters and the calculation of prediction regions."
"lordif",0,1,14,"2015-09-18 08:58","Analysis of Differential Item Functioning (DIF) for        dichotomous and polytomous items using an iterative hybrid of        ordinal logistic regression and item response theory (IRT)."
"CopyDetect",0,1,4,"2016-04-27 19:28","Contains several IRT and non-IRT based response similarity indices proposed in the literature for multiple-choice examinations such as the Omega index, Wollack (1997) <doi:10.1177/01466216970214002>; Generalized Binomial Test, van der Linden & Sotaridona (2006) <doi:10.3102/10769986031003283>; K index, K1 and K2 indices, Sotaridona & Meijer (2002) <doi:10.1111/j.1745-3984.2002.tb01138.x>;  and S1 and S2 indices, Sotaridona & Meijer (2003) <doi:10.1111/j.1745-3984.2003.tb01096.x>."
"CDM",0,12,50,"2019-09-05 06:40","    Functions for cognitive diagnosis modeling and multidimensional item response modeling     for dichotomous and polytomous item responses. This package enables the estimation of     the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),    the multiple group (polytomous) GDINA model (de la Torre, 2011,     <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009,     <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008,     <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992,     <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis     (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>).     See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02>     or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)         for further details on estimation and the package structure.    For tutorials on how to use the CDM package see     George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as    Ravand and Robitzsch (2015)."
"TestDesign",2,1,8,"2020-09-21 08:50","Use the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and    van der Linden (2018) <doi:10.1201/9781315117430> in constructing fixed and adaptive tests. Supports the following    mixed-integer programming (MIP) solver packages: 'lpsymphony', 'Rsymphony', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package    is not available from CRAN; see <https://www.gurobi.com/downloads/>. See vignette for installing 'Rsymphony' package    on Mac systems."
"rpf",3,4,42,"2020-05-07 13:20","The purpose of this package is to factor out logic    and math common to Item Factor Analysis fitting, diagnostics, and    analysis. It is envisioned as core support code suitable for more    specialized IRT packages to build upon. Complete access to optimized C    functions are made available with R_RegisterCCallable().    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>."
"psychotools",0,14,12,"2018-12-19 23:48","Infrastructure for psychometric modeling such as data classes (for  item response data and paired comparisons), basic model fitting functions (for  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,  rating scale, multinomial processing tree models), extractor functions for  different types of parameters (item, person, threshold, discrimination,  guessing, upper asymptotes), unified inference and visualizations, and various  datasets for illustration.  Intended as a common lightweight and efficient  toolbox for psychometric modeling and a common building block for fitting  psychometric mixture models in package ""psychomix"" and trees based on  psychometric models in package ""psychotree""."
"nonnest2",1,2,11,"2017-12-15 13:22","Testing non-nested models via theory supplied by Vuong (1989) <doi:10.2307/1912557>.    Includes tests of model distinguishability and of model fit that can be applied    to both nested and non-nested models. Also includes functionality to obtain    confidence intervals associated with AIC and BIC. This material is partially based on    work supported by the National Science Foundation under Grant Number SES-1061334."
"LSAmitR",0,2,4,"2018-01-19 08:56","Dieses R-Paket stellt Zusatzmaterial in Form von Daten, Funktionen             und R-Hilfe-Seiten für den Herausgeberband Breit, S. und Schreiner,              C. (Hrsg.). (2016). ""Large-Scale Assessment mit R: Methodische Grundlagen              der österreichischen Bildungsstandardüberprüfung."" Wien: facultas.              (ISBN: 978-3-7089-1343-8, <https://www.bifie.at/node/3770>) zur              Verfügung."
"irtplay",0,1,10,"2020-07-15 15:00","Fit unidimensional item response theory (IRT) models to mixture     of dichotomous and polytomous data, calibrate online item parameters     (i.e., pretest and operational items), estimate examinees abilities,     and examine the IRT model-data fit on item-level in different ways     as well as provide useful functions related to unidimensional IRT models.     For the item parameter estimation, marginal maximum likelihood estimation     with expectation-maximization (MMLE-EM) algorithm     (Bock & Aitkin (1981) <doi:10.1007/BF02294168>) is used.     For the online calibration, Stocking's Method A     (Ban, Hanson, Wang, Yi, & Harris (2011) <doi:10.1111/j.1745-3984.2001.tb01123.x>)     and the fixed item parameter calibration (FIPC) method     (Kim (2006) <doi:10.1111/j.1745-3984.2006.00021.x>) are provided.     For the ability estimation, several popular scoring methods     (e.g., MLE, EAP, and MAP) are implemented. In terms of assessing the IRT     model-data fit, one of distinguished features of this package is that it     gives not only well-known item fit statistics (e.g., chi-square (X2),     likelihood ratio chi-square (G2), infit and oufit statistics, and     S-X2 statistic (Ames & Penfield (2015) <doi:10.1111/emip.12067>))     but also graphical displays to look at residuals between the observed     data and model-based predictions     (Hambleton, Swaminathan, & Rogers (1991, ISBN:9780803936478)).     In addition, there are many useful functions such as computing asymptotic     variance-covariance matrices of item parameter estimates (Li & Lissitz (2004)     <doi:10.1111/j.1745-3984.2004.tb01109.x>), importing item and/or ability     parameters from popular IRT software, running 'flexMIRT' (Cai, 2017)     through R, generating simulated data, computing the conditional     distribution of observed scores using the Lord-Wingersky recursion     formula (Lord & Wingersky (1984) <doi:10.1177/014662168400800409>),     computing the loglikelihood of individual items, computing the loglikelihood     of abilities, computing item and test information functions, computing item     and test characteristic curve functions, and plotting item and test     characteristic curves and item and test information functions."
"ShinyItemAnalysis",0,1,14,"2020-05-04 17:20","Interactive shiny application for analysis of educational tests and    their items."
"scDIFtest",1,1,2,"2020-06-26 11:10","Detection of item-wise Differential Item Functioning (DIF)              in fitted 'mirt', 'multipleGroup' or 'bfactor' models               using score-based structural change tests. Under the hood              the sctest() function from the 'strucchange' package is used."
"PROsetta",1,1,2,"2020-07-01 13:00","Perform scale linking to establish relationships between instruments    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>."
"mstDIF",1,1,2,"2020-07-20 10:40","A collection of statistical tests for the detection of differential    item functioning (DIF) in multistage tests. Methods entail logistic regression,    an adaptation of the simultaneous item bias test (SIBTEST), and various score-based tests.    The presented tests provide itemwise test for DIF along categorical, ordinal or metric covariates. Methods for uniform and non-uniform     DIF effects are available depending on which method is used."
"kequate",2,1,12,"2019-05-31 19:30","Implements the kernel method of test equating as defined in von Davier, A. A., Holland, P. W. and Thayer, D. T. (2004) <doi:10.1007/b97446> and Andersson, B. and Wiberg, M. (2017) <doi:10.1007/s11336-016-9528-7> using the CB, EG, SG, NEAT CE/PSE and NEC designs, supporting Gaussian, logistic and uniform kernels and unsmoothed and pre-smoothed input data."
"jrt",1,1,2,"2019-01-04 12:20","Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225>. A convenience function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions."
"irtreliability",0,1,1,"2018-02-22","Estimation of reliability coefficients for ability estimates and sum scores from item response theory models as defined in Cheng, Y., Yuan, K.-H. and Liu, C. (2012) <doi:10.1177/0013164411407315> and Kim, S. and Feldt, L. S. (2010) <doi:10.1007/s12564-009-9062-8>. The package supports the 3-PL and generalized partial credit models and includes estimates of the standard errors of the reliability coefficient estimators, derived in Andersson, B. and Xin, T. (2018) <doi:10.1177/0013164417713570>."
"GPCMlasso",0,1,4,"2019-02-21 09:30","Provides a framework to detect Differential Item Functioning (DIF) in Generalized Partial Credit Models (GPCM) and special cases of the GPCM as proposed by Schauberger and Mair (2019) <doi:10.3758/s13428-019-01224-2>. A joint model is set up where DIF is explicitly parametrized and penalized likelihood estimation is used for parameter selection. The big advantage of the method called GPCMlasso is that several variables can be treated simultaneously and that both continuous and categorical variables can be used to detect DIF."
"faoutlier",0,1,10,"2017-07-22 07:31","Tools for detecting and summarize influential cases that    can affect exploratory and confirmatory factor analysis models as well as    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>;     Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <10.3389/fpsyg.2012.00055>)."
"equateIRT",2,2,14,"2016-03-30 20:35","Computation of direct, chain and average (bisector) equating coefficients with   standard errors using Item Response Theory (IRT) methods for dichotomous items   (Battauz (2013) <doi:10.1007/s11336-012-9316-y>,   Battauz (2015) <doi:10.18637/jss.v068.i07>).   Test scoring can be performed by true score equating and observed score equating methods.   DIF detection can be performed using a Wald-type test    (Battauz (2018) <doi:10.1007/s10260-018-00442-w>)."
"difR",0,2,20,"2018-05-14 15:38","Provides a collection of standard methods to detect differential item functioning among dichotomously scored items. Methods for uniform and non-uniform DIF, based on test-score or IRT methods, for comparing two or more than two groups of respondents, are available (Magis, Beland, Tuerlinckx and De Boeck,A General Framework and an R Package for the Detection of Dichotomous Differential Item Functioning, Behavior Research Methods, 42, 2010, 847-862 <doi:10.3758/BRM.42.3.847>)."
"ctgdist",0,1,1,"2020-08-10","It is assumed that psychological distances between the categories are equal for the measurement instruments consisted of polytomously scored items. According to Muraki, this assumption must be tested. In the examination process of this assumption, the fit indexes are obtained and evaluated. This package provides that this assumption is removed. By with this package, the converted scale values of all items in a measurement instrument can be calculated by estimating a category parameter set for each item. Thus, the calculations can be made without any need to usage of the common category parameter set. Through this package, the psychological distances of the items are scaled. The scaling of a category parameter set for each item cause differentiation of score of the categories will be got from items. Also, the total measurement instrument score of an individual can be calculated according to the scaling of item score categories by with this package.This package provides that the place of individuals related to the structure to be measured with a measurement instrument consisted of polytomously scored items can be reveal more accurately. In this way, it is thought that the results obtained about individuals can be made more sensitive, and the differences between individuals can be revealed more accurately. On the other hand, it can be argued that more accurate evidences can be obtained regarding the psychometric properties of the measurement instruments."
"airt",1,1,1,"2020-05-12","An evaluation framework for algorithm portfolios using Item Response    Theory (IRT). We use polytomous IRT models to evaluate algorithms and introduce    algorithm characteristics such as stability, effectiveness and anomalousness     (Kandanaarachchi, Smith-Miles 2020) <doi:10.13140/RG.2.2.11363.09760>."
"PerFit",0,1,7,"2015-07-20 09:59","Several person-fit statistics (PFSs) are offered. These statistics allow assessing whether  individual response patterns to tests or questionnaires are (im)plausible given   the other respondents in the sample or given a specified item response theory model. Some PFSs apply to   dichotomous data, such as the likelihood-based PFSs (lz, lz*) and the group-based PFSs   (personal biserial correlation, caution index, (normed) number of Guttman errors,   agreement/disagreement/dependability statistics, U3, ZU3, NCI, Ht). PFSs suitable to polytomous data include  extensions of lz, U3, and (normed) number of Guttman errors."
"OrdinalLogisticBiplot",0,1,4,"2014-05-03 00:36","Analysis of a matrix of polytomous items using Ordinal Logistic Biplots (OLB)  The OLB procedure extends the binary logistic biplot to ordinal (polytomous) data.   The  individuals are represented as points on a plane and the variables are represented   as lines rather than vectors as in a classical or binary biplot, specifying the points  for each of the categories of the variable.   The set of prediction regions is established by stripes perpendicular to the line   between the category points, in such a way that the prediction for each individual is given  by its projection into the line of the variable."
"NominalLogisticBiplot",0,1,2,"2013-09-17 14:47","Analysis of a matrix of polytomous items using Nominal Logistic Biplots (NLB)  according to Hernandez-Sanchez and Vicente-Villardon (2013).   The NLB procedure extends the binary logistic biplot to nominal (polytomous) data.   The  individuals are represented as points on a plane and the  variables are represented   as convex prediction regions rather than vectors as in a classical or binary biplot.   Using the methods from Computational Geometry, the set of prediction regions is converted to a set of points   in such a way that the prediction for each individual is established by its closest   ""category point"". Then interpretation is based on distances rather than on projections.   In this package we implement the geometry of such a representation and construct computational algorithms   for the estimation of parameters and the calculation of prediction regions."
"lordif",0,1,14,"2015-09-18 08:58","Analysis of Differential Item Functioning (DIF) for        dichotomous and polytomous items using an iterative hybrid of        ordinal logistic regression and item response theory (IRT)."
"CopyDetect",0,1,4,"2016-04-27 19:28","Contains several IRT and non-IRT based response similarity indices proposed in the literature for multiple-choice examinations such as the Omega index, Wollack (1997) <doi:10.1177/01466216970214002>; Generalized Binomial Test, van der Linden & Sotaridona (2006) <doi:10.3102/10769986031003283>; K index, K1 and K2 indices, Sotaridona & Meijer (2002) <doi:10.1111/j.1745-3984.2002.tb01138.x>;  and S1 and S2 indices, Sotaridona & Meijer (2003) <doi:10.1111/j.1745-3984.2003.tb01096.x>."
"inpdfr",1,1,7,"2018-10-24 11:50","A set of functions to analyse and compare texts, using classical   text mining	functions, as well as those from theoretical ecology."
"inpdfr",1,1,7,"2018-10-24 11:50","A set of functions to analyse and compare texts, using classical   text mining	functions, as well as those from theoretical ecology."
"rangeModelMetadata",4,1,3,"2019-05-02 00:20","Range Modeling Metadata Standards (RMMS) address three challenges:   they (i) are designed for convenience to encourage use, (ii) accommodate a wide   variety of applications, and (iii) are extensible to allow the community of range   modelers to steer it as needed. RMMS are based on a data dictionary that specifies   a hierarchical structure to catalog different aspects of the range modeling process.   The dictionary balances a constrained, minimalist vocabulary to improve   standardization with flexibility for users to provide their own values.   Merow et al. (2019) <doi:10.1111/geb.12993> describe the standards in   more detail. Note that users who prefer to use the R package 'ecospat' can obtain it from  <https://github.com/ecospat/ecospat>."
"ENMTools",0,1,2,"2020-07-29 15:12","Description: Tools for constructing niche models and analyzing patterns of niche evolution.  Acts as an interface for many popular modeling algorithms, and allows users to conduct Monte Carlo tests to address basic questions in evolutionary ecology and biogeography.  Warren, D.L., R.E. Glor, and M. Turelli (2008) <doi:10.1111/j.1558-5646.2008.00482.x> Glor, R.E., and D.L. Warren (2011) <doi:10.1111/j.1558-5646.2010.01177.x>  Warren, D.L., R.E. Glor, and M. Turelli (2010) <doi:10.1111/j.1600-0587.2009.06142.x> Cardillo, M., and D.L. Warren (2016) <doi:10.1111/geb.12455> D.L. Warren, L.J. Beaumont, R. Dinnage, and J.B. Baumgartner (2019) <doi:10.1111/ecog.03900>.  "
"biomod2",0,3,13,"2016-03-01 18:41","Functions for species distribution modeling, calibration and evaluation,   ensemble of models, ensemble forecasting and visualization. The package permits to run  consistently up to 10 single models on a presence/absences (resp presences/pseudo-absences)  dataset and to combine them in ensemble models and ensemble projections. Some bench of other   evaluation and visualisation tools are also available within the package."
"MinBAR",1,1,2,"2019-07-10 14:30","A versatile tool that aims at (1) defining what is the minimum background extent necessary to fit good partial species distribution models and/or (2) determining if the background area used to fit a partial species distribution model is reliable enough to extract ecologically relevant conclusions from it. See Rotllan-Puig, X. & Traveset, A. (2019) <doi:10.1101/571182>."
"red",0,1,15,"2018-05-08 22:17","Includes algorithms to facilitate the assessment of extinction    risk of species according to the IUCN (International Union for Conservation of    Nature, see <http://www.iucn.org> for more information) red list criteria."
"red",0,1,15,"2018-05-08 22:17","Includes algorithms to facilitate the assessment of extinction    risk of species according to the IUCN (International Union for Conservation of    Nature, see <http://www.iucn.org> for more information) red list criteria."
"morphomap",0,1,3,"2020-01-12 16:40","Extract cross sections from long bone meshes at specified intervals along the diaphysis. Calculate two and three-dimensional morphometric maps, cross-sectional geometric parameters, and semilandmarks on the periosteal and endosteal contours of each cross section. "
"quiddich",0,1,1,"2019-03-25","Provides tools for an automated identification of diagnostic molecular characters, i.e. such columns in a given nucleotide or amino acid alignment that allow to distinguish taxa from each other. These characters can then be used to complement the formal descriptions of the taxa, which are often based on morphological and anatomical features. Especially for morphologically cryptic species, this will be helpful. QUIDDICH distinguishes between four different types of diagnostic characters. For more information, see ""Kuehn, A.L., Haase, M. 2019. QUIDDICH: QUick IDentification of DIagnostic CHaracters."""
"polysat",2,1,18,"2018-06-07 21:50","A collection of tools to handle microsatellite data of any ploidy (and samples of mixed ploidy) where allele copy number is not known in partially heterozygous genotypes.  It can import and export data in ABI 'GeneMapper', 'Structure', 'ATetra', 'Tetrasat'/'Tetra', 'GenoDive', 'SPAGeDi', 'POPDIST', 'STRand', and binary presence/absence formats.  It can calculate pairwise distances between individuals using a stepwise mutation model or infinite alleles model, with or without taking ploidies and allele frequencies into account.  These distances can be used for the calculation of clonal diversity statistics or used for further analysis in R.  Allelic diversity statistics and Polymorphic Information Content are also available.  polysat can  assist the user in estimating the ploidy of samples, and it can estimate allele  frequencies in populations, calculate pairwise or global differentiation statistics  based on those frequencies, and export allele frequencies to 'SPAGeDi' and 'adegenet'.   Functions are also included for assigning alleles to isoloci in cases where one pair  of microsatellite primers amplifies alleles from two or more independently segregating isoloci.  polysat is described by Clark and Jasieniuk (2011) <doi:10.1111/j.1755-0998.2011.02985.x> and Clark and Schreier (2017) <doi:10.1111/1755-0998.12639>."
"StAMPP",0,1,7,"2017-11-10 12:44","Allows users to calculate pairwise Nei's Genetic Distances (Nei 1972), pairwise Fixation Indexes (Fst) (Weir & Cockerham 1984) and also Genomic Relationship matrixes following Yang et al. (2010) in mixed and single ploidy populations. Bootstrapping across loci is implemented during Fst calculation to generate confidence intervals and p-values around pairwise Fst values. StAMPP utilises SNP genotype data of any ploidy level (with the ability to handle missing data) and is coded to   utilise multithreading where available to allow efficient analysis of large datasets. StAMPP is able to handle genotype data from genlight objects  allowing integration with other packages such adegenet. Please refer to LW Pembleton, NOI Cogan & JW Forster, 2013, Molecular Ecology Resources, 13(5), 946-952. <doi:10.1111/1755-0998.12129> for the appropriate citation and user manual. Thank you in advance."
"rmetasim",5,1,31,"2020-01-29 22:30","An interface between R and the metasim simulation engine.    The simulation environment is documented in: ""Strand, A.(2002) <doi:10.1046/j.1471-8286.2002.00208.x> Metasim 1.0: an individual-based environment for simulating population genetics of     complex population dynamics. Mol. Ecol. Notes.     Please see the vignettes CreatingLandscapes and Simulating to get some ideas on how to use the packages.      See the rmetasim vignette to get an overview and to see important changes to the     code in the most recent version."
"hierfstat",2,4,13,"2015-12-04 15:57","Estimates hierarchical F-statistics from haploid or    diploid genetic data with any numbers of levels in the hierarchy, following the    algorithm of Yang (Evolution(1998), 52:950).    Tests via randomisations the significance    of each F and variance components, using the likelihood-ratio statistics G    (Goudet et al. (1996) <https://www.genetics.org/content/144/4/1933>).    Estimates genetic diversity statistics    for haploid and diploid genetic datasets in various formats, including inbreeding and    coancestry coefficients, and population specific F-statistics following    Weir and Goudet (2017) <https://www.genetics.org/content/206/4/2085>."
"adephylo",1,6,11,"2016-12-13 07:57","Multivariate tools to analyze comparative data, i.e. a phylogeny    and some traits measured for each taxa."
"pegas",1,13,26,"2020-03-10 16:50","Functions for reading, writing, plotting, analysing, and manipulating allelic and haplotypic data, including from VCF files, and for the analysis of population nucleotide sequences and micro-satellites including coalescent analyses, linkage disequilibrium, population structure (Fst, Amova) and equilibrium (HWE), haplotype networks, minimum spanning tree and network, and median-joining networks."
"mmod",1,3,12,"2016-09-16 01:57","Provides functions for measuring    population divergence from genotypic data."
"phyr",3,1,3,"2019-11-10 12:20","A collection of functions to do model-based phylogenetic analysis.     It includes functions to calculate community phylogenetic diversity,    to estimate correlations among functional traits while accounting for     phylogenetic relationships, and to fit phylogenetic generalized linear    mixed models. The Bayesian phylogenetic generalized linear mixed models    are fitted with the 'INLA' package (<http://www.r-inla.org>)."
"fishtree",2,1,5,"2019-05-31 23:10","An interface to the Fish Tree of Life API to download taxonomies,    phylogenies, fossil calibrations, and diversification rate information for    ray-finned fishes."
"DAMOCLES",0,1,4,"2020-01-22 13:10","Simulates and computes (maximum) likelihood of a dynamical model of community assembly that takes into account phylogenetic history."
"phyr",3,1,3,"2019-11-10 12:20","A collection of functions to do model-based phylogenetic analysis.     It includes functions to calculate community phylogenetic diversity,    to estimate correlations among functional traits while accounting for     phylogenetic relationships, and to fit phylogenetic generalized linear    mixed models. The Bayesian phylogenetic generalized linear mixed models    are fitted with the 'INLA' package (<http://www.r-inla.org>)."
"fishtree",2,1,5,"2019-05-31 23:10","An interface to the Fish Tree of Life API to download taxonomies,    phylogenies, fossil calibrations, and diversification rate information for    ray-finned fishes."
"DAMOCLES",0,1,4,"2020-01-22 13:10","Simulates and computes (maximum) likelihood of a dynamical model of community assembly that takes into account phylogenetic history."
"phyr",3,1,3,"2019-11-10 12:20","A collection of functions to do model-based phylogenetic analysis.     It includes functions to calculate community phylogenetic diversity,    to estimate correlations among functional traits while accounting for     phylogenetic relationships, and to fit phylogenetic generalized linear    mixed models. The Bayesian phylogenetic generalized linear mixed models    are fitted with the 'INLA' package (<http://www.r-inla.org>)."
"fishtree",2,1,5,"2019-05-31 23:10","An interface to the Fish Tree of Life API to download taxonomies,    phylogenies, fossil calibrations, and diversification rate information for    ray-finned fishes."
"DAMOCLES",0,1,4,"2020-01-22 13:10","Simulates and computes (maximum) likelihood of a dynamical model of community assembly that takes into account phylogenetic history."
"phyr",3,1,3,"2019-11-10 12:20","A collection of functions to do model-based phylogenetic analysis.     It includes functions to calculate community phylogenetic diversity,    to estimate correlations among functional traits while accounting for     phylogenetic relationships, and to fit phylogenetic generalized linear    mixed models. The Bayesian phylogenetic generalized linear mixed models    are fitted with the 'INLA' package (<http://www.r-inla.org>)."
"fishtree",2,1,5,"2019-05-31 23:10","An interface to the Fish Tree of Life API to download taxonomies,    phylogenies, fossil calibrations, and diversification rate information for    ray-finned fishes."
"DAMOCLES",0,1,4,"2020-01-22 13:10","Simulates and computes (maximum) likelihood of a dynamical model of community assembly that takes into account phylogenetic history."
"phyr",3,1,3,"2019-11-10 12:20","A collection of functions to do model-based phylogenetic analysis.     It includes functions to calculate community phylogenetic diversity,    to estimate correlations among functional traits while accounting for     phylogenetic relationships, and to fit phylogenetic generalized linear    mixed models. The Bayesian phylogenetic generalized linear mixed models    are fitted with the 'INLA' package (<http://www.r-inla.org>)."
"fishtree",2,1,5,"2019-05-31 23:10","An interface to the Fish Tree of Life API to download taxonomies,    phylogenies, fossil calibrations, and diversification rate information for    ray-finned fishes."
"DAMOCLES",0,1,4,"2020-01-22 13:10","Simulates and computes (maximum) likelihood of a dynamical model of community assembly that takes into account phylogenetic history."
"phyr",3,1,3,"2019-11-10 12:20","A collection of functions to do model-based phylogenetic analysis.     It includes functions to calculate community phylogenetic diversity,    to estimate correlations among functional traits while accounting for     phylogenetic relationships, and to fit phylogenetic generalized linear    mixed models. The Bayesian phylogenetic generalized linear mixed models    are fitted with the 'INLA' package (<http://www.r-inla.org>)."
"fishtree",2,1,5,"2019-05-31 23:10","An interface to the Fish Tree of Life API to download taxonomies,    phylogenies, fossil calibrations, and diversification rate information for    ray-finned fishes."
"DAMOCLES",0,1,4,"2020-01-22 13:10","Simulates and computes (maximum) likelihood of a dynamical model of community assembly that takes into account phylogenetic history."
"inpdfr",1,1,7,"2018-10-24 11:50","A set of functions to analyse and compare texts, using classical   text mining	functions, as well as those from theoretical ecology."
"inpdfr",1,1,7,"2018-10-24 11:50","A set of functions to analyse and compare texts, using classical   text mining	functions, as well as those from theoretical ecology."
"inpdfr",1,1,7,"2018-10-24 11:50","A set of functions to analyse and compare texts, using classical   text mining	functions, as well as those from theoretical ecology."
"inpdfr",1,1,7,"2018-10-24 11:50","A set of functions to analyse and compare texts, using classical   text mining	functions, as well as those from theoretical ecology."
"inpdfr",1,1,7,"2018-10-24 11:50","A set of functions to analyse and compare texts, using classical   text mining	functions, as well as those from theoretical ecology."
"inpdfr",1,1,7,"2018-10-24 11:50","A set of functions to analyse and compare texts, using classical   text mining	functions, as well as those from theoretical ecology."
"ClusterR",1,12,22,"2019-11-29 20:50","Gaussian mixture models, k-means, mini-batch-kmeans, k-medoids and affinity propagation clustering with the option to plot, validate, predict (new data) and estimate the optimal number of clusters. The package takes advantage of 'RcppArmadillo' to speed up the computationally intensive parts of the functions. For more information, see (i) ""Clustering in an Object-Oriented Environment"" by Anja Struyf, Mia Hubert, Peter Rousseeuw (1997), Journal of Statistical Software, <doi:10.18637/jss.v001.i04>; (ii) ""Web-scale k-means clustering"" by D. Sculley (2010), ACM Digital Library, <doi:10.1145/1772690.1772862>; (iii) ""Armadillo: a template-based C++ library for linear algebra"" by Sanderson et al (2016), The Journal of Open Source Software, <doi:10.21105/joss.00026>; (iv) ""Clustering by Passing Messages Between Data Points"" by Brendan J. Frey and Delbert Dueck, Science 16 Feb 2007: Vol. 315, Issue 5814, pp. 972-976, <doi:10.1126/science.1136800>."
"STEPCAM",2,1,5,"2015-08-07 17:24","Collection of model estimation, and model plotting functions              related to the STEPCAM family of community assembly models.              STEPCAM is a STEPwise Community Assembly Model that infers              the relative contribution of Dispersal Assembly, Habitat Filtering              and Limiting Similarity from a dataset consisting of the              combination of trait and abundance data. See also <http://onlinelibrary.wiley.com/wol1/doi/10.1890/14-0454.1/abstract> for more information."
"Select",1,1,5,"2018-04-02 21:36","The objective of these functions is to derive a species assemblage    that satisfies a functional trait profile. Restoring resilient ecosystems    requires a flexible framework for selecting assemblages that are based on the    functional traits of species. However, current trait-based models have been    limited to algorithms that can only select species by optimising specific trait    values, and could not elegantly accommodate the common desire among restoration    ecologists to produce functionally diverse assemblages. We have solved this    problem by applying a non-linear optimisation algorithm that optimises Rao Q,    a closed-form functional trait diversity index that incorporates species    abundances, subject to other linear constraints. This framework generalises    previous models that only optimised the entropy of the community, and can    optimise both functional diversity and entropy simultaneously. This package    can also be used to generate experimental assemblages to test the effects    of community-level traits on community dynamics and ecosystem function. The    method is based on theory discussed in Laughlin (2014, Ecology Letters)    <doi.org/10.1111/ele.12288>."
"hillR",0,1,2,"2018-11-19 20:20","Calculate taxonomic, functional and phylogenetic diversity measures     through Hill Numbers proposed by Chao, Chiu and Jost (2014)     <doi:10.1146/annurev-ecolsys-120213-091540>."
"ClusterR",1,12,22,"2019-11-29 20:50","Gaussian mixture models, k-means, mini-batch-kmeans, k-medoids and affinity propagation clustering with the option to plot, validate, predict (new data) and estimate the optimal number of clusters. The package takes advantage of 'RcppArmadillo' to speed up the computationally intensive parts of the functions. For more information, see (i) ""Clustering in an Object-Oriented Environment"" by Anja Struyf, Mia Hubert, Peter Rousseeuw (1997), Journal of Statistical Software, <doi:10.18637/jss.v001.i04>; (ii) ""Web-scale k-means clustering"" by D. Sculley (2010), ACM Digital Library, <doi:10.1145/1772690.1772862>; (iii) ""Armadillo: a template-based C++ library for linear algebra"" by Sanderson et al (2016), The Journal of Open Source Software, <doi:10.21105/joss.00026>; (iv) ""Clustering by Passing Messages Between Data Points"" by Brendan J. Frey and Delbert Dueck, Science 16 Feb 2007: Vol. 315, Issue 5814, pp. 972-976, <doi:10.1126/science.1136800>."
"STEPCAM",2,1,5,"2015-08-07 17:24","Collection of model estimation, and model plotting functions              related to the STEPCAM family of community assembly models.              STEPCAM is a STEPwise Community Assembly Model that infers              the relative contribution of Dispersal Assembly, Habitat Filtering              and Limiting Similarity from a dataset consisting of the              combination of trait and abundance data. See also <http://onlinelibrary.wiley.com/wol1/doi/10.1890/14-0454.1/abstract> for more information."
"Select",1,1,5,"2018-04-02 21:36","The objective of these functions is to derive a species assemblage    that satisfies a functional trait profile. Restoring resilient ecosystems    requires a flexible framework for selecting assemblages that are based on the    functional traits of species. However, current trait-based models have been    limited to algorithms that can only select species by optimising specific trait    values, and could not elegantly accommodate the common desire among restoration    ecologists to produce functionally diverse assemblages. We have solved this    problem by applying a non-linear optimisation algorithm that optimises Rao Q,    a closed-form functional trait diversity index that incorporates species    abundances, subject to other linear constraints. This framework generalises    previous models that only optimised the entropy of the community, and can    optimise both functional diversity and entropy simultaneously. This package    can also be used to generate experimental assemblages to test the effects    of community-level traits on community dynamics and ecosystem function. The    method is based on theory discussed in Laughlin (2014, Ecology Letters)    <doi.org/10.1111/ele.12288>."
"hillR",0,1,2,"2018-11-19 20:20","Calculate taxonomic, functional and phylogenetic diversity measures     through Hill Numbers proposed by Chao, Chiu and Jost (2014)     <doi:10.1146/annurev-ecolsys-120213-091540>."
"ClusterR",1,12,22,"2019-11-29 20:50","Gaussian mixture models, k-means, mini-batch-kmeans, k-medoids and affinity propagation clustering with the option to plot, validate, predict (new data) and estimate the optimal number of clusters. The package takes advantage of 'RcppArmadillo' to speed up the computationally intensive parts of the functions. For more information, see (i) ""Clustering in an Object-Oriented Environment"" by Anja Struyf, Mia Hubert, Peter Rousseeuw (1997), Journal of Statistical Software, <doi:10.18637/jss.v001.i04>; (ii) ""Web-scale k-means clustering"" by D. Sculley (2010), ACM Digital Library, <doi:10.1145/1772690.1772862>; (iii) ""Armadillo: a template-based C++ library for linear algebra"" by Sanderson et al (2016), The Journal of Open Source Software, <doi:10.21105/joss.00026>; (iv) ""Clustering by Passing Messages Between Data Points"" by Brendan J. Frey and Delbert Dueck, Science 16 Feb 2007: Vol. 315, Issue 5814, pp. 972-976, <doi:10.1126/science.1136800>."
"STEPCAM",2,1,5,"2015-08-07 17:24","Collection of model estimation, and model plotting functions              related to the STEPCAM family of community assembly models.              STEPCAM is a STEPwise Community Assembly Model that infers              the relative contribution of Dispersal Assembly, Habitat Filtering              and Limiting Similarity from a dataset consisting of the              combination of trait and abundance data. See also <http://onlinelibrary.wiley.com/wol1/doi/10.1890/14-0454.1/abstract> for more information."
"Select",1,1,5,"2018-04-02 21:36","The objective of these functions is to derive a species assemblage    that satisfies a functional trait profile. Restoring resilient ecosystems    requires a flexible framework for selecting assemblages that are based on the    functional traits of species. However, current trait-based models have been    limited to algorithms that can only select species by optimising specific trait    values, and could not elegantly accommodate the common desire among restoration    ecologists to produce functionally diverse assemblages. We have solved this    problem by applying a non-linear optimisation algorithm that optimises Rao Q,    a closed-form functional trait diversity index that incorporates species    abundances, subject to other linear constraints. This framework generalises    previous models that only optimised the entropy of the community, and can    optimise both functional diversity and entropy simultaneously. This package    can also be used to generate experimental assemblages to test the effects    of community-level traits on community dynamics and ecosystem function. The    method is based on theory discussed in Laughlin (2014, Ecology Letters)    <doi.org/10.1111/ele.12288>."
"hillR",0,1,2,"2018-11-19 20:20","Calculate taxonomic, functional and phylogenetic diversity measures     through Hill Numbers proposed by Chao, Chiu and Jost (2014)     <doi:10.1146/annurev-ecolsys-120213-091540>."
"HiveR",1,1,17,"2020-05-07 17:40","Creates and plots 2D and 3D hive plots. Hive plots are a unique method of displaying networks of many types in which node properties are mapped to axes using meaningful properties rather than being arbitrarily positioned.  The hive plot concept was invented by Martin Krzywinski at the Genome Science Center (www.hiveplot.net/).  Keywords: networks, food webs, linnet, systems biology, bioinformatics."
"plsRglm",2,2,19,"2018-06-11 23:12","Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."
"HiveR",1,1,17,"2020-05-07 17:40","Creates and plots 2D and 3D hive plots. Hive plots are a unique method of displaying networks of many types in which node properties are mapped to axes using meaningful properties rather than being arbitrarily positioned.  The hive plot concept was invented by Martin Krzywinski at the Genome Science Center (www.hiveplot.net/).  Keywords: networks, food webs, linnet, systems biology, bioinformatics."
"plsRglm",2,2,19,"2018-06-11 23:12","Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."
"HiveR",1,1,17,"2020-05-07 17:40","Creates and plots 2D and 3D hive plots. Hive plots are a unique method of displaying networks of many types in which node properties are mapped to axes using meaningful properties rather than being arbitrarily positioned.  The hive plot concept was invented by Martin Krzywinski at the Genome Science Center (www.hiveplot.net/).  Keywords: networks, food webs, linnet, systems biology, bioinformatics."
"plsRglm",2,2,19,"2018-06-11 23:12","Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."
"HiveR",1,1,17,"2020-05-07 17:40","Creates and plots 2D and 3D hive plots. Hive plots are a unique method of displaying networks of many types in which node properties are mapped to axes using meaningful properties rather than being arbitrarily positioned.  The hive plot concept was invented by Martin Krzywinski at the Genome Science Center (www.hiveplot.net/).  Keywords: networks, food webs, linnet, systems biology, bioinformatics."
"plsRglm",2,2,19,"2018-06-11 23:12","Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."
"HiveR",1,1,17,"2020-05-07 17:40","Creates and plots 2D and 3D hive plots. Hive plots are a unique method of displaying networks of many types in which node properties are mapped to axes using meaningful properties rather than being arbitrarily positioned.  The hive plot concept was invented by Martin Krzywinski at the Genome Science Center (www.hiveplot.net/).  Keywords: networks, food webs, linnet, systems biology, bioinformatics."
"plsRglm",2,2,19,"2018-06-11 23:12","Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."
"HiveR",1,1,17,"2020-05-07 17:40","Creates and plots 2D and 3D hive plots. Hive plots are a unique method of displaying networks of many types in which node properties are mapped to axes using meaningful properties rather than being arbitrarily positioned.  The hive plot concept was invented by Martin Krzywinski at the Genome Science Center (www.hiveplot.net/).  Keywords: networks, food webs, linnet, systems biology, bioinformatics."
"plsRglm",2,2,19,"2018-06-11 23:12","Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."
"HiveR",1,1,17,"2020-05-07 17:40","Creates and plots 2D and 3D hive plots. Hive plots are a unique method of displaying networks of many types in which node properties are mapped to axes using meaningful properties rather than being arbitrarily positioned.  The hive plot concept was invented by Martin Krzywinski at the Genome Science Center (www.hiveplot.net/).  Keywords: networks, food webs, linnet, systems biology, bioinformatics."
"plsRglm",2,2,19,"2018-06-11 23:12","Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."
"HiveR",1,1,17,"2020-05-07 17:40","Creates and plots 2D and 3D hive plots. Hive plots are a unique method of displaying networks of many types in which node properties are mapped to axes using meaningful properties rather than being arbitrarily positioned.  The hive plot concept was invented by Martin Krzywinski at the Genome Science Center (www.hiveplot.net/).  Keywords: networks, food webs, linnet, systems biology, bioinformatics."
"plsRglm",2,2,19,"2018-06-11 23:12","Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."
"florestal",0,1,2,"2020-07-06 13:50","The functions return sampling parameters for forest inventories with tables and graphics. Methods used in the   package refers to Pellico e Brena (1997) <https://bit.ly/2BDbHJI>. "
"neotoma",0,1,14,"2018-09-26 23:50","Access paleoecological datasets from the Neotoma Paleoecological    Database using the published API (<http://api.neotomadb.org/>).  The functions    in this package access various pre-built API functions and attempt to return    the results from Neotoma in a usable format for researchers and the public."
"neotoma",0,1,14,"2018-09-26 23:50","Access paleoecological datasets from the Neotoma Paleoecological    Database using the published API (<http://api.neotomadb.org/>).  The functions    in this package access various pre-built API functions and attempt to return    the results from Neotoma in a usable format for researchers and the public."
"neotoma",0,1,14,"2018-09-26 23:50","Access paleoecological datasets from the Neotoma Paleoecological    Database using the published API (<http://api.neotomadb.org/>).  The functions    in this package access various pre-built API functions and attempt to return    the results from Neotoma in a usable format for researchers and the public."
"psyverse",1,1,1,"2020-03-26","The constructs used to study the human psychology have    many definitions and corresponding instructions for eliciting    and coding qualitative data pertaining to constructs' content and    for measuring the constructs. This plethora of definitions and    instructions necessitates unequivocal reference to specific    definitions and instructions in empirical and secondary research.    This package implements a human- and machine-readable standard for    specifying construct definitions and instructions for measurement    and qualitative research based on 'YAML'. This standard facilitates    systematic unequivocal reference to specific construct definitions    and corresponding instructions in a decentralized manner (i.e.    without requiring central curation; Peters (2020)    <doi:10.31234/osf.io/xebhn>)."
"wrProteo",1,1,4,"2020-07-15 17:50","Data analysis of proteomics experiments by mass spectrometry is supported by this collection of functions mostly dedicated to the analysis of (bottom-up) quantitative (XIC) data.     Fasta-formatted proteomes (eg from UniProt Consortium <doi:10.1093/nar/gky1049>) can be read with automatic parsing and multiple annotation types (like species origin, abbreviated gene names, etc) extracted.     Quantitative proteomics (Schubert et al 2017 <doi:10.1038/nprot.2017.040>) measurements frequently contain multiple NA values, due to physical absence of given peptides in some samples, limitations in sensitivity or other reasons.     The functions provided here help to inspect graphically the data to investigate the nature of NA-values via their respective replicate measurements and to help/confirm the choice of NA-replacement by low random values.     Dedicated filtering and statistical testing using the framework of package 'limma' <doi:10.18129/B9.bioc.limma> can be run, enhanced by multiple rounds of NA-replacements to provide robustness towards rare stochastic events.     Multi-species samples, as frequently used in benchmark-tests (eg Navarro et al 2016 <doi:10.1038/nbt.3685>, Ramus et al 2016 <doi:10.1016/j.jprot.2015.11.011>), can be run with special options separating the data into sub-groups during normalization and testing.     Subsequently, ROC curves (Hand and Till 2001 <doi:10.1023/A:1010920819831>) can be constructed to compare multiple analysis approaches."
"wrGraph",1,2,3,"2020-07-09 11:30","Additional options for making graphics in the context of analyzing high-throughput data are available here.  This includes automatic segmenting of the current device (eg window) to accommodate multiple new plots,   automatic checking for optimal location of legends in plots, small histograms to insert as legends,   histograms re-transforming axis labels to linear when plotting log2-transformed data,  a violin-plot <doi:10.1080/00031305.1998.10480559> function for a wide variety of input-formats,   principal components analysis (PCA) <doi:10.1080/14786440109462720> with bag-plots <doi:10.1080/00031305.1999.10474494> to highlight and compare the center areas for groups of samples,   generic MA-plots (differential- versus average-value plots) <doi:10.1093/nar/30.4.e15>,   staggered count plots and generation of mouse-over interactive html pages."
"wrProteo",1,1,4,"2020-07-15 17:50","Data analysis of proteomics experiments by mass spectrometry is supported by this collection of functions mostly dedicated to the analysis of (bottom-up) quantitative (XIC) data.     Fasta-formatted proteomes (eg from UniProt Consortium <doi:10.1093/nar/gky1049>) can be read with automatic parsing and multiple annotation types (like species origin, abbreviated gene names, etc) extracted.     Quantitative proteomics (Schubert et al 2017 <doi:10.1038/nprot.2017.040>) measurements frequently contain multiple NA values, due to physical absence of given peptides in some samples, limitations in sensitivity or other reasons.     The functions provided here help to inspect graphically the data to investigate the nature of NA-values via their respective replicate measurements and to help/confirm the choice of NA-replacement by low random values.     Dedicated filtering and statistical testing using the framework of package 'limma' <doi:10.18129/B9.bioc.limma> can be run, enhanced by multiple rounds of NA-replacements to provide robustness towards rare stochastic events.     Multi-species samples, as frequently used in benchmark-tests (eg Navarro et al 2016 <doi:10.1038/nbt.3685>, Ramus et al 2016 <doi:10.1016/j.jprot.2015.11.011>), can be run with special options separating the data into sub-groups during normalization and testing.     Subsequently, ROC curves (Hand and Till 2001 <doi:10.1023/A:1010920819831>) can be constructed to compare multiple analysis approaches."
"wrGraph",1,2,3,"2020-07-09 11:30","Additional options for making graphics in the context of analyzing high-throughput data are available here.  This includes automatic segmenting of the current device (eg window) to accommodate multiple new plots,   automatic checking for optimal location of legends in plots, small histograms to insert as legends,   histograms re-transforming axis labels to linear when plotting log2-transformed data,  a violin-plot <doi:10.1080/00031305.1998.10480559> function for a wide variety of input-formats,   principal components analysis (PCA) <doi:10.1080/14786440109462720> with bag-plots <doi:10.1080/00031305.1999.10474494> to highlight and compare the center areas for groups of samples,   generic MA-plots (differential- versus average-value plots) <doi:10.1093/nar/30.4.e15>,   staggered count plots and generation of mouse-over interactive html pages."
"see",0,8,10,"2020-07-27 09:40","Provides plotting utilities supporting easystats-packages (<https://github.com/easystats/easystats>)     and some extra themes, geoms, and scales for 'ggplot2'. Color scales are     based on <https://www.materialui.co/colors>."
"janitor",2,25,11,"2020-04-08 21:10","The main janitor functions can: perfectly format data.frame column    names; provide quick counts of variable combinations (i.e., frequency    tables and crosstabs); and isolate duplicate records. Other janitor functions    nicely format the tabulation results. These tabulate-and-report functions    approximate popular features of SPSS and Microsoft Excel. This package    follows the principles of the ""tidyverse"" and works well with the pipe function    %>%. janitor was built with beginning-to-intermediate R users in mind and is    optimized for user-friendliness. Advanced R users can already do everything    covered here, but with janitor they can do it faster and save their thinking for    the fun stuff."
"cheatR",1,1,1,"2020-05-06","A set of functions to compare texts for similarity, and plot a graph of similarities among the compared texts. These functions were originally developed for detection of overlap in course hand-in."
"wordgraph",0,1,1,"2020-07-01","Functions that help less experienced R users to make graph analysis for free associated words, or more generally for repeated nominal data for which a undirected graph analysis is meaningful. By corresponding to each word its centrality, it is possible to apply standard quantitative analysis methods in order to associate word selection with other   variables.    The functions are implemented with the aid of the 'tibble', 'tidygraph', 'ggraph' and 'ggplot2' packages. Supported centrality functions are centrality_alpha(), centrality_authority(), centrality_betweenness(), centrality_closeness(), centrality_pagerank(), centrality_eigen(). A data set is included."
"trundler",0,1,6,"2020-07-20 11:10","A wrapper around the 'Trundler' API, which gives    access to historical retail product and pricing data, and can be found    at <https://api.trundler.dev/>."
"scholar",1,1,7,"2018-05-23 10:04","Provides functions to extract citation data from Google    Scholar.  Convenience functions are also provided for comparing    multiple scholars and predicting future h-index values."
"usethis",0,83,10,"2020-04-29 07:50","Automate package and project setup tasks that are otherwise    performed manually. This includes setting up unit testing, test     coverage, continuous integration, Git, 'GitHub', licenses, 'Rcpp', 'RStudio'     projects, and more."
"shinyMonacoEditor",0,1,2,"2020-09-09 12:20","A 'Shiny' app including the 'Monaco' editor. The 'Monaco' editor is the code editor which powers 'VS Code'. It is particularly well developed for 'JavaScript'. In addition to the 'Monaco' editor features, the app provides prettifiers and minifiers for multiple languages, 'SCSS' and 'TypeScript' compilers, code checking for 'C' and 'C++' (requires 'cppcheck')."
"shinydashboardPlus",7,12,6,"2019-04-08 10:20","Extend 'shinydashboard' with 'AdminLTE2' components.              'AdminLTE2' is a free 'Bootstrap 3' dashboard template available             at <https://adminlte.io>. Customize boxes, add timelines and a lot more. "
"reprex",1,2,6,"2018-09-16 06:30","Convenience wrapper that uses the 'rmarkdown' package to render  small snippets of code to target formats that include both code and output.  The goal is to encourage the sharing of small, reproducible, and runnable  examples on code-oriented websites, such as <https://stackoverflow.com> and  <https://github.com>, or in email. The user's clipboard is the default source  of input code and the default target for rendered output. 'reprex' also  extracts clean, runnable R code from various common formats, such as  copy/paste from an R session. "
"precommit",4,1,1,"2020-06-13","Useful git hooks for R building on top of the    multi-language framework 'pre-commit' for hook management. This    package provides git hooks for common tasks like formatting files with    'styler' or spell checking as well as wrapper functions to access the    'pre-commit' executable."
"nph",2,1,2,"2019-08-23 12:50","Piecewise constant hazard functions are used to flexibly model survival distributions with non-proportional hazards and 	to simulate data from the specified distributions. Also, a function to calculate weighted log-rank tests for the 	comparison of two hazard functions is included. Finally, a function to calculate a test using the maximum of a set of 	test statistics from weighted log-rank tests is provided. This test utilizes the asymptotic multivariate normal joint distribution	of the separate test statistics. The correlation is estimated from the data."
"knitr",9,6623,46,"2020-06-23 08:20","Provides a general-purpose tool for dynamic report generation in R    using Literate Programming techniques."
"getTBinR",3,1,12,"2019-09-03 15:50","Quickly and easily import analysis ready    Tuberculosis (TB) burden data, from the World Health Organization    (WHO), into R. The aim of getTBinR is to allow researchers, and other    interested individuals, to quickly and easily gain access to a    detailed TB data set and to start using it to derive key insights. It    provides a consistent set of tools that can be used to rapidly    evaluate hypotheses on a widely used data set before they are explored    further using more complex methods or more detailed data. These tools    include: generic plotting and mapping functions; a data dictionary    search tool; an interactive shiny dashboard; and an automated, country    level, TB report. For newer R users, this package reduces the barrier    to entry by handling data import, munging, and visualisation. All    plotting and mapping functions are built with ggplot2 so can be    readily extended."
"datastructures",1,1,8,"2018-12-19 14:00","Implementation of advanced data structures such as hashmaps,    heaps, or queues. Advanced data structures are essential    in many computer science and statistics problems, for example graph    algorithms or string analysis. The package uses 'Boost' and 'STL' data    types and extends these to R with 'Rcpp' modules."
"crunch",13,2,32,"2020-07-22 16:20","The Crunch.io service <https://crunch.io/> provides a cloud-based    data store and analytic engine, as well as an intuitive web interface.    Using this package, analysts can interact with and manipulate Crunch    datasets from within R. Importantly, this allows technical researchers to    collaborate naturally with team members, managers, and clients who prefer a    point-and-click interface."
"autothresholdr",3,2,16,"2019-07-08 14:10","Algorithms for automatically finding appropriate    thresholds for numerical data, with special functions for thresholding    images. Provides the 'ImageJ' 'Auto Threshold' plugin functionality to    R users. See <http://imagej.net/Auto_Threshold> and Landini et al.    (2017) <doi:10.1111/jmi.12474>."
"shinyobjects",1,1,3,"2020-06-26 22:30","Troubleshooting reactive data in 'shiny' can be difficult. These functions will convert reactive data frames into functions and load all assigned objects into your local environment. If you create a dummy input object, as the function will suggest, you will be able to test your server and ui functions interactively."
"questionr",1,6,16,"2020-05-26 13:30","Set of functions to make the processing and analysis of    surveys easier : interactive shiny apps and addins for data recoding,    contingency tables, dataset metadata handling, and several convenience    functions."
"languageserver",0,1,19,"2020-05-25 08:30","An implementation of the Language Server Protocol    for R. The Language Server protocol is used by an editor client to    integrate features like auto completion. See    <https://microsoft.github.io/language-server-protocol/> for details."
"exampletestr",3,1,17,"2020-04-08 18:40","Take the examples written in your documentation of    functions and use them to create shells (skeletons which must be    manually completed by the user) of test files to be tested with the    'testthat' package. Sort of like python 'doctests' for R."
"gms",0,1,1,"2020-07-01","A collection of tools to create, use and maintain modularized model code written in the modeling     language 'GAMS' (<https://www.gams.com/>). Out-of-the-box 'GAMS' does not come with support for modularized    model code. This package provides the tools necessary to convert a standard 'GAMS' model to a modularized one    by introducing a modularized code structure together with a naming convention which emulates local    environments. In addition, this package provides tools to monitor the compliance of the model code with    modular coding guidelines."
"gms",0,1,1,"2020-07-01","A collection of tools to create, use and maintain modularized model code written in the modeling     language 'GAMS' (<https://www.gams.com/>). Out-of-the-box 'GAMS' does not come with support for modularized    model code. This package provides the tools necessary to convert a standard 'GAMS' model to a modularized one    by introducing a modularized code structure together with a naming convention which emulates local    environments. In addition, this package provides tools to monitor the compliance of the model code with    modular coding guidelines."
"MetaDBparse",0,1,1,"2020-09-09","Provides parsing functionality for over 30 metabolomics databases, with most available without having to create an account on given websites.     Once parsed, calculates given adducts and isotope patterns and inserts into one big database which can be used to annotate unknown m/z values.    Furthermore, formulas can be predicted for a given m/z, and these can be matched to ChemSpider or PubChem for further annotation.    Current databases available: HMDB, ChEBI, LMDB, RMDB, BMDB, MCDB, ECMDB, Wikidata, mVOC, VMH, T3DB, Exposome Explorer, FooDB, MetaCyc (requires account), DrugBank (requires account), ReSPECT, MaConDa, Blood Exposome DB, KEGG, SMPDB, LIPID MAPS, MetaboLights, DimeDB, Phenol Explorer, MassBank, YMDB, PAMDB, NANPDB and STOFF.     Featured in the 'MetaboShiny' package (Wolthuis, J. (2019) <doi:10.1101/734236>)."
"MetaDBparse",0,1,1,"2020-09-09","Provides parsing functionality for over 30 metabolomics databases, with most available without having to create an account on given websites.     Once parsed, calculates given adducts and isotope patterns and inserts into one big database which can be used to annotate unknown m/z values.    Furthermore, formulas can be predicted for a given m/z, and these can be matched to ChemSpider or PubChem for further annotation.    Current databases available: HMDB, ChEBI, LMDB, RMDB, BMDB, MCDB, ECMDB, Wikidata, mVOC, VMH, T3DB, Exposome Explorer, FooDB, MetaCyc (requires account), DrugBank (requires account), ReSPECT, MaConDa, Blood Exposome DB, KEGG, SMPDB, LIPID MAPS, MetaboLights, DimeDB, Phenol Explorer, MassBank, YMDB, PAMDB, NANPDB and STOFF.     Featured in the 'MetaboShiny' package (Wolthuis, J. (2019) <doi:10.1101/734236>)."
"MetaDBparse",0,1,1,"2020-09-09","Provides parsing functionality for over 30 metabolomics databases, with most available without having to create an account on given websites.     Once parsed, calculates given adducts and isotope patterns and inserts into one big database which can be used to annotate unknown m/z values.    Furthermore, formulas can be predicted for a given m/z, and these can be matched to ChemSpider or PubChem for further annotation.    Current databases available: HMDB, ChEBI, LMDB, RMDB, BMDB, MCDB, ECMDB, Wikidata, mVOC, VMH, T3DB, Exposome Explorer, FooDB, MetaCyc (requires account), DrugBank (requires account), ReSPECT, MaConDa, Blood Exposome DB, KEGG, SMPDB, LIPID MAPS, MetaboLights, DimeDB, Phenol Explorer, MassBank, YMDB, PAMDB, NANPDB and STOFF.     Featured in the 'MetaboShiny' package (Wolthuis, J. (2019) <doi:10.1101/734236>)."
"codebook",4,1,11,"2020-01-09 17:20","Easily automate the following tasks to describe data frames:		Summarise the distributions, and labelled missings of variables graphically		and using descriptive statistics.		For surveys, compute and summarise reliabilities (internal consistencies, 		retest, multilevel) for psychological scales.		Combine this information with metadata (such as item labels and labelled 		values) that is derived from R attributes.		To do so, the package relies on 'rmarkdown' partials, so you can generate 		HTML, PDF, and Word documents. 		Codebooks are also available as tables (CSV, Excel, etc.) and in JSON-LD, so		that search engines can find your data and index the metadata.		The metadata are also available at your fingertips via RStudio Addins."
"CIplot",0,1,1,"2017-08-14","Plot confidence interval from the objects of statistical tests such as  t.test(), var.test(), cor.test(), prop.test() and fisher.test() ('htest' class),  Tukey test [TukeyHSD()], Dunnett test [glht() in 'multcomp' package],  logistic regression [glm()], and Tukey or Games-Howell test [posthocTGH() in  'userfriendlyscience' package].  Users are able to set the styles of lines and points.  This package contains the function to calculate odds ratios and their confidence  intervals from the result of logistic regression."
"codebook",4,1,11,"2020-01-09 17:20","Easily automate the following tasks to describe data frames:		Summarise the distributions, and labelled missings of variables graphically		and using descriptive statistics.		For surveys, compute and summarise reliabilities (internal consistencies, 		retest, multilevel) for psychological scales.		Combine this information with metadata (such as item labels and labelled 		values) that is derived from R attributes.		To do so, the package relies on 'rmarkdown' partials, so you can generate 		HTML, PDF, and Word documents. 		Codebooks are also available as tables (CSV, Excel, etc.) and in JSON-LD, so		that search engines can find your data and index the metadata.		The metadata are also available at your fingertips via RStudio Addins."
"CIplot",0,1,1,"2017-08-14","Plot confidence interval from the objects of statistical tests such as  t.test(), var.test(), cor.test(), prop.test() and fisher.test() ('htest' class),  Tukey test [TukeyHSD()], Dunnett test [glht() in 'multcomp' package],  logistic regression [glm()], and Tukey or Games-Howell test [posthocTGH() in  'userfriendlyscience' package].  Users are able to set the styles of lines and points.  This package contains the function to calculate odds ratios and their confidence  intervals from the result of logistic regression."
"xVA",0,1,3,"2016-01-20 09:24","Calculates a number of valuation adjustments including CVA, DVA,    FBA, FCA, MVA and KVA. A two-way margin agreement has been implemented. For    the KVA calculation three regulatory frameworks are supported: CEM, (simplified) SA-CCR, OEM	and IMM. The probability of default is implied through the credit spreads curve.    Currently, only IRSwaps are supported. For more information, you can check    one of the books regarding xVA: <http://www.cvacentral.com/books/credit-value-adjustment>."
"xVA",0,1,3,"2016-01-20 09:24","Calculates a number of valuation adjustments including CVA, DVA,    FBA, FCA, MVA and KVA. A two-way margin agreement has been implemented. For    the KVA calculation three regulatory frameworks are supported: CEM, (simplified) SA-CCR, OEM	and IMM. The probability of default is implied through the credit spreads curve.    Currently, only IRSwaps are supported. For more information, you can check    one of the books regarding xVA: <http://www.cvacentral.com/books/credit-value-adjustment>."
"xVA",0,1,3,"2016-01-20 09:24","Calculates a number of valuation adjustments including CVA, DVA,    FBA, FCA, MVA and KVA. A two-way margin agreement has been implemented. For    the KVA calculation three regulatory frameworks are supported: CEM, (simplified) SA-CCR, OEM	and IMM. The probability of default is implied through the credit spreads curve.    Currently, only IRSwaps are supported. For more information, you can check    one of the books regarding xVA: <http://www.cvacentral.com/books/credit-value-adjustment>."
"xVA",0,1,3,"2016-01-20 09:24","Calculates a number of valuation adjustments including CVA, DVA,    FBA, FCA, MVA and KVA. A two-way margin agreement has been implemented. For    the KVA calculation three regulatory frameworks are supported: CEM, (simplified) SA-CCR, OEM	and IMM. The probability of default is implied through the credit spreads curve.    Currently, only IRSwaps are supported. For more information, you can check    one of the books regarding xVA: <http://www.cvacentral.com/books/credit-value-adjustment>."
"radiant.multivariate",0,1,7,"2019-03-05 16:20","The Radiant Multivariate menu includes interfaces for perceptual    mapping, factor analysis, cluster analysis, and conjoint analysis. The    application extends the functionality in radiant.data."
"radiant",1,1,10,"2019-03-10 22:40","A platform-independent browser-based interface for business    analytics in R, based on the shiny package. The application combines the    functionality of radiant.data, radiant.design, radiant.basics,    radiant.model, and radiant.multivariate."
"radiant.multivariate",0,1,7,"2019-03-05 16:20","The Radiant Multivariate menu includes interfaces for perceptual    mapping, factor analysis, cluster analysis, and conjoint analysis. The    application extends the functionality in radiant.data."
"radiant",1,1,10,"2019-03-10 22:40","A platform-independent browser-based interface for business    analytics in R, based on the shiny package. The application combines the    functionality of radiant.data, radiant.design, radiant.basics,    radiant.model, and radiant.multivariate."
"rosr",0,1,7,"2019-07-12 10:52","Creates reproducible academic projects with integrated academic elements, including datasets, references, codes, images, manuscripts, dissertations, slides and so on. These elements are well connected so that they can be easily synchronized and updated. "
"rmd",0,1,3,"2019-02-01 19:40","The 'rmd' package manages multiple R markdown packages. These R markdown packages include currently 'rmarkdown', 'knitr', 'bookdown', 'bookdownplus', 'blogdown', 'rticles',  'tinytex', 'xaringan', 'citr',  and 'mindr'. They can be installed and loaded in a single step with the 'rmd' package. The conflicts between these packages are evaluated as well."
"rosr",0,1,7,"2019-07-12 10:52","Creates reproducible academic projects with integrated academic elements, including datasets, references, codes, images, manuscripts, dissertations, slides and so on. These elements are well connected so that they can be easily synchronized and updated. "
"rmd",0,1,3,"2019-02-01 19:40","The 'rmd' package manages multiple R markdown packages. These R markdown packages include currently 'rmarkdown', 'knitr', 'bookdown', 'bookdownplus', 'blogdown', 'rticles',  'tinytex', 'xaringan', 'citr',  and 'mindr'. They can be installed and loaded in a single step with the 'rmd' package. The conflicts between these packages are evaluated as well."
"rosr",0,1,7,"2019-07-12 10:52","Creates reproducible academic projects with integrated academic elements, including datasets, references, codes, images, manuscripts, dissertations, slides and so on. These elements are well connected so that they can be easily synchronized and updated. "
"rmd",0,1,3,"2019-02-01 19:40","The 'rmd' package manages multiple R markdown packages. These R markdown packages include currently 'rmarkdown', 'knitr', 'bookdown', 'bookdownplus', 'blogdown', 'rticles',  'tinytex', 'xaringan', 'citr',  and 'mindr'. They can be installed and loaded in a single step with the 'rmd' package. The conflicts between these packages are evaluated as well."
"rosr",0,1,7,"2019-07-12 10:52","Creates reproducible academic projects with integrated academic elements, including datasets, references, codes, images, manuscripts, dissertations, slides and so on. These elements are well connected so that they can be easily synchronized and updated. "
"rmd",0,1,3,"2019-02-01 19:40","The 'rmd' package manages multiple R markdown packages. These R markdown packages include currently 'rmarkdown', 'knitr', 'bookdown', 'bookdownplus', 'blogdown', 'rticles',  'tinytex', 'xaringan', 'citr',  and 'mindr'. They can be installed and loaded in a single step with the 'rmd' package. The conflicts between these packages are evaluated as well."
"tablerDash",0,1,1,"2019-03-08","'R' interface to the 'Tabler' HTML template. See more here <https://tabler.io>.     'tablerDash' is a light 'Bootstrap 4' dashboard template. There are different      layouts available such as a one page dashboard or a multi page template,     where the navigation menu is contained in the navigation bar. A fancy example     is available at <https://dgranjon.shinyapps.io/shinyMons/>."
"kerastuneR",5,1,3,"2020-05-14 10:10","'Keras Tuner' <https://keras-team.github.io/keras-tuner/> is a hypertuning framework made for humans.              It aims at making the life of AI practitioners, hypertuner              algorithm creators and model designers as simple as possible by              providing them with a clean and easy to use API for hypertuning.              'Keras Tuner' makes moving from a base model to a hypertuned one quick and              easy by only requiring you to change a few lines of code."
"tablerDash",0,1,1,"2019-03-08","'R' interface to the 'Tabler' HTML template. See more here <https://tabler.io>.     'tablerDash' is a light 'Bootstrap 4' dashboard template. There are different      layouts available such as a one page dashboard or a multi page template,     where the navigation menu is contained in the navigation bar. A fancy example     is available at <https://dgranjon.shinyapps.io/shinyMons/>."
"kerastuneR",5,1,3,"2020-05-14 10:10","'Keras Tuner' <https://keras-team.github.io/keras-tuner/> is a hypertuning framework made for humans.              It aims at making the life of AI practitioners, hypertuner              algorithm creators and model designers as simple as possible by              providing them with a clean and easy to use API for hypertuning.              'Keras Tuner' makes moving from a base model to a hypertuned one quick and              easy by only requiring you to change a few lines of code."
"tablerDash",0,1,1,"2019-03-08","'R' interface to the 'Tabler' HTML template. See more here <https://tabler.io>.     'tablerDash' is a light 'Bootstrap 4' dashboard template. There are different      layouts available such as a one page dashboard or a multi page template,     where the navigation menu is contained in the navigation bar. A fancy example     is available at <https://dgranjon.shinyapps.io/shinyMons/>."
"kerastuneR",5,1,3,"2020-05-14 10:10","'Keras Tuner' <https://keras-team.github.io/keras-tuner/> is a hypertuning framework made for humans.              It aims at making the life of AI practitioners, hypertuner              algorithm creators and model designers as simple as possible by              providing them with a clean and easy to use API for hypertuning.              'Keras Tuner' makes moving from a base model to a hypertuned one quick and              easy by only requiring you to change a few lines of code."
"tablerDash",0,1,1,"2019-03-08","'R' interface to the 'Tabler' HTML template. See more here <https://tabler.io>.     'tablerDash' is a light 'Bootstrap 4' dashboard template. There are different      layouts available such as a one page dashboard or a multi page template,     where the navigation menu is contained in the navigation bar. A fancy example     is available at <https://dgranjon.shinyapps.io/shinyMons/>."
"kerastuneR",5,1,3,"2020-05-14 10:10","'Keras Tuner' <https://keras-team.github.io/keras-tuner/> is a hypertuning framework made for humans.              It aims at making the life of AI practitioners, hypertuner              algorithm creators and model designers as simple as possible by              providing them with a clean and easy to use API for hypertuning.              'Keras Tuner' makes moving from a base model to a hypertuned one quick and              easy by only requiring you to change a few lines of code."
"tablerDash",0,1,1,"2019-03-08","'R' interface to the 'Tabler' HTML template. See more here <https://tabler.io>.     'tablerDash' is a light 'Bootstrap 4' dashboard template. There are different      layouts available such as a one page dashboard or a multi page template,     where the navigation menu is contained in the navigation bar. A fancy example     is available at <https://dgranjon.shinyapps.io/shinyMons/>."
"kerastuneR",5,1,3,"2020-05-14 10:10","'Keras Tuner' <https://keras-team.github.io/keras-tuner/> is a hypertuning framework made for humans.              It aims at making the life of AI practitioners, hypertuner              algorithm creators and model designers as simple as possible by              providing them with a clean and easy to use API for hypertuning.              'Keras Tuner' makes moving from a base model to a hypertuned one quick and              easy by only requiring you to change a few lines of code."
"tablerDash",0,1,1,"2019-03-08","'R' interface to the 'Tabler' HTML template. See more here <https://tabler.io>.     'tablerDash' is a light 'Bootstrap 4' dashboard template. There are different      layouts available such as a one page dashboard or a multi page template,     where the navigation menu is contained in the navigation bar. A fancy example     is available at <https://dgranjon.shinyapps.io/shinyMons/>."
"kerastuneR",5,1,3,"2020-05-14 10:10","'Keras Tuner' <https://keras-team.github.io/keras-tuner/> is a hypertuning framework made for humans.              It aims at making the life of AI practitioners, hypertuner              algorithm creators and model designers as simple as possible by              providing them with a clean and easy to use API for hypertuning.              'Keras Tuner' makes moving from a base model to a hypertuned one quick and              easy by only requiring you to change a few lines of code."
"modchart",0,1,1,"2020-07-13","This is a 'shiny' module that encapsulates various charting options available in 'htmlwidgets', and provides options for each type of chart, a 'crosstalk' like interface for aggregate reports between 'DT' and other chart types."
"covid19.analytics",1,1,5,"2020-05-03 19:00","Load and analyze updated time series worldwide data of reported cases for the Novel CoronaVirus Disease (CoViD-19) from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) data repository <https://github.com/CSSEGISandData/COVID-19>. The datasets are available in two main modalities, as a time series sequences and aggregated for the last day with greater spatial resolution. Several analysis, visualization and modelling functions are available in the package that will allow the user to compute and visualize total number of cases, total number of changes and growth rate globally or for an specific geographical location, while at the same time generating models using these trends; generate interactive visualizations and generate Susceptible-Infected-Recovered (SIR) model for the disease spread."
"modchart",0,1,1,"2020-07-13","This is a 'shiny' module that encapsulates various charting options available in 'htmlwidgets', and provides options for each type of chart, a 'crosstalk' like interface for aggregate reports between 'DT' and other chart types."
"covid19.analytics",1,1,5,"2020-05-03 19:00","Load and analyze updated time series worldwide data of reported cases for the Novel CoronaVirus Disease (CoViD-19) from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) data repository <https://github.com/CSSEGISandData/COVID-19>. The datasets are available in two main modalities, as a time series sequences and aggregated for the last day with greater spatial resolution. Several analysis, visualization and modelling functions are available in the package that will allow the user to compute and visualize total number of cases, total number of changes and growth rate globally or for an specific geographical location, while at the same time generating models using these trends; generate interactive visualizations and generate Susceptible-Infected-Recovered (SIR) model for the disease spread."
"modchart",0,1,1,"2020-07-13","This is a 'shiny' module that encapsulates various charting options available in 'htmlwidgets', and provides options for each type of chart, a 'crosstalk' like interface for aggregate reports between 'DT' and other chart types."
"covid19.analytics",1,1,5,"2020-05-03 19:00","Load and analyze updated time series worldwide data of reported cases for the Novel CoronaVirus Disease (CoViD-19) from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) data repository <https://github.com/CSSEGISandData/COVID-19>. The datasets are available in two main modalities, as a time series sequences and aggregated for the last day with greater spatial resolution. Several analysis, visualization and modelling functions are available in the package that will allow the user to compute and visualize total number of cases, total number of changes and growth rate globally or for an specific geographical location, while at the same time generating models using these trends; generate interactive visualizations and generate Susceptible-Infected-Recovered (SIR) model for the disease spread."
"sf",7,289,43,"2020-07-14 15:10","Support for simple features, a standardized way to    encode spatial vector data. Binds to 'GDAL' for reading and writing    data, to 'GEOS' for geometrical operations, and to 'PROJ' for    projection conversions and datum transformations. Optionally uses the 's2'    package for spherical geometry operations on geographic coordinates."
"sp",3,640,123,"2020-02-29 00:20","Classes and methods for spatial  data; the classes document where the spatial location information  resides, for 2D or 3D data. Utility functions are provided, e.g. for  plotting data as maps, spatial selection, as well as methods for  retrieving coordinates, for subsetting, print, summary, etc."
"rgbif",4,12,38,"2020-07-23 11:20","A programmatic interface to the Web Service methods    provided by the Global Biodiversity Information Facility ('GBIF';    <https://www.gbif.org/developer/summary>). 'GBIF' is a database    of species occurrence records from sources all over the globe.    'rgbif' includes functions for searching for taxonomic names,    retrieving information on data providers, getting species occurrence    records, getting counts of occurrence records, and using the 'GBIF'    tile map service to make 'rasters' summarizing huge amounts of data."
"gstat",4,58,118,"2020-04-04 16:20","Variogram modelling; simple, ordinary and universal point or block (co)kriging; spatio-temporal kriging; sequential Gaussian or indicator (co)simulation; variogram and variogram map plotting utility functions; supports sf and stars."
"geomorph",4,7,35,"2020-06-03 19:30","Read, manipulate, and digitize landmark data, generate shape    variables via Procrustes analysis for points, curves and surfaces, perform    shape analyses, and provide graphical depictions of shapes and patterns of    shape variation."
"rviewgraph",0,38,3,"2020-05-05 00:00","This is an 'R' interface to Alun Thomas's 'ViewGraph' 'Java' graph    viewing program.    It takes a graph specified as an incidence matrix, list of    edges, or in 'igraph' format and runs a graphical user interface that shows    an animation of a force directed algorithm positioning the vertices in two    dimensions. It works well for graphs of various structure of up to a few    thousand vertices. It's not fazed by graphs that comprise several    components. The coordinates can be read as an 'igraph' style layout     matrix at any time. The user can mess with the layout using a mouse,     preferably one with 3 buttons, and some keyed commands."
"data.tree",2,38,17,"2019-11-09 11:10","Create tree structures from hierarchical data, and traverse the    tree in various orders. Aggregate, cumulate, print, plot, convert to and from    data.frame and more. Useful for decision trees, machine learning, finance,    conversion from and to JSON, and many other applications."
"d3r",0,4,22,"2019-08-21 12:51","Provides a suite of functions to help ease the use of 'd3.js' in R.              These helpers include 'htmltools::htmlDependency' functions, hierarchy              builders, and conversion tools for 'partykit', 'igraph,' 'table',              and 'data.frame' R objects into the 'JSON' that 'd3.js' expects."
"xgboost",4,70,23,"2020-06-14 16:40","Extreme Gradient Boosting, which is an efficient implementation    of the gradient boosting framework from Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>.    This package is its R interface. The package includes efficient linear    model solver and tree learning algorithms. The package can automatically    do parallel computation on a single machine which could be more than 10    times faster than existing gradient boosting packages. It supports    various objective functions, including regression, classification and ranking.    The package is made to be extensible, so that users are also allowed to define    their own objectives easily."
"wikisourcer",1,4,5,"2019-04-04 07:50","Download public domain works from Wikisource <https://wikisource.org/>, a free library from the Wikimedia Foundation project."
"widyr",2,4,4,"2019-09-09 23:20","Encapsulates the pattern of untidying data into a wide matrix,  performing some processing, then turning it back into a tidy form. This  is useful for several operations such as co-occurrence counts,  correlations, or clustering that are mathematically convenient on wide matrices."
"vocaldia",0,66,4,"2017-08-10 17:12","Create adjacency matrices of vocalisation graphs from  dataframes containing sequences of speech and silence intervals,  transforming these matrices into Markov diagrams, and generating  datasets for classification of these diagrams by 'flattening' them  and adding global properties (functionals) etc.  Vocalisation  diagrams date back to early work in psychiatry (Jaffe and Feldstein,  1970) and social psychology (Dabbs and Ruback, 1987) but have only  recently been employed as a data representation method for machine  learning tasks including meeting segmentation (Luz, 2012)  <doi:10.1145/2328967.2328970> and classification (Luz,  2013) <doi:10.1145/2522848.2533788>."
"vkR",0,66,2,"2016-12-02 10:46","Provides an interface to the VK API <https://vk.com/dev/methods>.      VK <https://vk.com/> is the largest European online social networking      service, based in Russia."
"visNetwork",1,66,20,"2019-08-28 15:40","Provides an R interface to the 'vis.js' JavaScript charting    library. It allows an interactive visualization of networks."
"TunePareto",0,2,7,"2014-03-30 21:50","Generic methods for parameter tuning of classification algorithms using multiple scoring functions."
"treespace",4,1,8,"2018-05-14 17:48","Tools for the exploration of distributions of phylogenetic trees.    This package includes a 'shiny' interface which can be started from R using    treespaceServer().     For further details see Jombart et al. (2017) <doi:10.1111/1755-0998.12676>."
"TNC",0,1,1,"2017-09-06","Node centrality measures for temporal networks. Available measures are temporal degree centrality, temporal closeness centrality and temporal betweenness centrality defined by Kim and Anderson (2012) <doi:10.1103/PhysRevE.85.026107>. Applying the REN algorithm by Hanke and Foraita (2017) <doi:10.1186/s12859-017-1677-x> when calculating the centrality measures keeps the computational running time linear in the number of graph snapshots. Further, all methods can run in parallel up to the number of nodes in the network."
"tidyjson",2,1,5,"2020-05-28 15:40","Turn complex 'JSON' data into tidy data frames."
"themetagenomics",2,1,4,"2020-04-14 11:00","A means to explore the structure of 16S rRNA surveys using a Structural   Topic Model coupled with functional prediction. The user provides an abundance   table, sample metadata, and taxonomy information, and themetagenomics infers   associations between topics and sample features, as well as topics and predicted   functional content. Functional prediction can be accomplished via Tax4Fun (for   Silva references) and PICRUSt (for GreenGeenes references). See   <doi:10.1371/journal.pone.0219235>."
"textplot",1,1,4,"2020-05-23 18:50","Visualise complex relations in texts. This is done by providing functionalities for displaying     text co-occurrence networks, text correlation networks, dependency relationships as well as text clustering.     Feel free to join the effort of providing interesting text visualisations."
"TextMiningGUI",0,1,1,"2020-08-11","Graphic interface for text analysis, implement a few methods such as biplots, correspondence analysis, co-occurrence, clustering, topic models, correlations and sentiments."
"textmineR",6,1,19,"2019-03-22 13:30","An aid for text mining in R, with a syntax that    should be familiar to experienced R users. Provides a wrapper for several     topic models that take similarly-formatted input and give similarly-formatted    output. Has additional functionality for analyzing and diagnostics for    topic models."
"TDAmapper",0,8,1,"2015-05-31","Topological Data Analysis using Mapper (discrete Morse theory).    Generate a 1-dimensional simplicial complex from a filter     function defined on the data:  1. Define a filter function (lens) on the     data.  2. Perform clustering within within each level set and generate     one node (vertex) for each cluster.  3. For each pair of clusters in     adjacent level sets with a nonempty intersection, generate one edge     between vertices.  The function mapper1D uses a filter function with    codomain R, while the the function mapper2D uses a filter function with    codomain R^2."
"stm",1,8,16,"2019-12-17 13:50","The Structural Topic Model (STM) allows researchers   to estimate topic models with document-level covariates.   The package also includes tools for model selection, visualization,  and estimation of topic-covariate regressions. Methods developed in  Roberts et. al. (2014) <doi:10.1111/ajps.12103> and   Roberts et. al. (2016) <doi:10.1080/01621459.2016.1141684>. Vignette  is Roberts et. al. (2019) <doi:10.18637/jss.v091.i02>."
"stampr",0,6,2,"2017-01-12 15:13","Perform spatial temporal analysis of moving polygons; a    longstanding analysis problem in Geographic Information Systems. Facilitates    directional analysis, shape analysis, and some other simple functionality for    examining spatial-temporal patterns of moving polygons."
"stabs",2,6,5,"2017-01-31 06:51","Resampling procedures to assess the stability of selected variables    with additional finite sample error control for high-dimensional variable    selection procedures such as Lasso or boosting. Both, standard stability    selection (Meinshausen & Buhlmann, 2010, <doi:10.1111/j.1467-9868.2010.00740.x>)     and complementary pairs stability selection with improved error bounds     (Shah & Samworth, 2013, <doi:10.1111/j.1467-9868.2011.01034.x>) are    implemented. The package can be combined with arbitrary user specified    variable selection approaches."
"stabm",0,92,4,"2020-02-26 20:40","An implementation of many measures for the assessment of the stability    of feature selection. Both simple measures and measures which take into account    the similarities between features are available, see Bommert et al. (2017) <doi:10.1155/2017/7907163>."
"spectralGraphTopology",1,92,4,"2019-09-22 23:50","In the era of big data and hyperconnectivity, learning    high-dimensional structures such as graphs from data has become a prominent    task in machine learning and has found applications in many fields such as    finance, health care, and networks. 'spectralGraphTopology' is an open source,    documented, and well-tested R package for learning graphs from data. It    provides implementations of state of the art algorithms such as Combinatorial    Graph Laplacian Learning (CGL), Spectral Graph Learning (SGL), Graph Estimation    based on Majorization-Minimization (GLE-MM), and Graph Estimation based on    Alternating Direction Method of Multipliers (GLE-ADMM). In addition, graph    learning has been widely employed for clustering, where specific algorithms    are available in the literature. To this end, we provide an implementation of    the Constrained Laplacian Rank (CLR) algorithm."
"spdynmod",1,92,7,"2015-07-16 23:14","A spatio-dynamic modelling package that focuses on three    characteristic wetland plant communities in a semiarid Mediterranean    wetland in response to hydrological pressures from the catchment. The    package includes the data on watershed hydrological pressure and the    initial raster maps of plant communities but also allows for random initial    distribution of plant communities. For more detailed info see: Martinez-Lopez et al. (2015) <doi:10.1016/j.ecolmodel.2014.11.024>."
"spdep",5,92,129,"2019-09-18 15:30","A collection of functions to create spatial weights matrix  objects from polygon 'contiguities', from point patterns by distance and  tessellations, for summarizing these objects, and for permitting their  use in spatial data analysis, including regional aggregation by minimum  spanning tree; a collection of tests for spatial 'autocorrelation',  including global 'Morans I' and 'Gearys C' proposed by 'Cliff' and 'Ord'  (1973, ISBN: 0850860369) and (1981, ISBN: 0850860814), 'Hubert/Mantel'  general cross product statistic, Empirical Bayes estimates and  'Assunção/Reis' (1999) <doi:10.1002/(SICI)1097-0258(19990830)18:16%3C2147::AID-SIM179%3E3.0.CO;2-I> Index, 'Getis/Ord' G ('Getis' and 'Ord' 1992)  <doi:10.1111/j.1538-4632.1992.tb00261.x> and multicoloured  join count statistics, 'APLE' ('Li 'et al.' )  <doi:10.1111/j.1538-4632.2007.00708.x>, local 'Moran's I'  ('Anselin' 1995) <doi:10.1111/j.1538-4632.1995.tb00338.x> and  'Getis/Ord' G ('Ord' and 'Getis' 1995)  <doi:10.1111/j.1538-4632.1995.tb00912.x>,  'saddlepoint' approximations ('Tiefelsdorf' 2002)  <doi:10.1111/j.1538-4632.2002.tb01084.x> and exact tests  for global and local 'Moran's I' ('Bivand et al.' 2009)  <doi:10.1016/j.csda.2008.07.021> and 'LOSH' local indicators  of spatial heteroscedasticity ('Ord' and 'Getis')  <doi:10.1007/s00168-011-0492-y>. The implementation of most of  the measures is described in 'Bivand' and 'Wong' (2018)  <doi:10.1007/s11749-018-0599-x>.  'spdep' >= 1.1-1 corresponds to 'spatialreg' >= 1.1-1, in which the model  fitting functions are deprecated and pass through to 'spatialreg', but  will mask those in 'spatialreg'. From versions 1.2-1, the functions will  be made defunct in 'spdep'.  For now 'spatialreg' only has functions from 'spdep', where they are shown  as deprecated. 'spatialreg' only loads the namespace of 'spdep'; if you  attach 'spdep', the same functions in the other package will be masked.  Some feed through adequately, others do not."
"sparsebnUtils",0,4,7,"2018-03-16 22:41","A set of tools for representing and estimating sparse Bayesian networks from continuous and discrete data, as described in Aragam, Gu, and Zhou (2017) <arXiv:1703.04025>."
"sparsebn",1,1,8,"2020-09-10 15:00","Fast methods for learning sparse Bayesian networks from high-dimensional data using sparse regularization, as described in Aragam, Gu, and Zhou (2017) <arXiv:1703.04025>. Designed to handle mixed experimental and observational data with thousands of variables with either continuous or discrete observations."
"sirt",0,15,49,"2020-02-15 19:00","    Supplementary functions for item response models aiming    to complement existing R packages. The functionality includes among others    multidimensional compensatory and noncompensatory IRT models    (Reckase, 2009, <doi:10.1007/978-0-387-89976-3>),     MCMC for hierarchical IRT models and testlet models    (Fox, 2010, <doi:10.1007/978-1-4419-0742-4>),     NOHARM (McDonald, 1982, <doi:10.1177/014662168200600402>),     Rasch copula model (Braeken, 2011, <doi:10.1007/s11336-010-9190-4>;    Schroeders, Robitzsch & Schipolowski, 2014, <doi:10.1111/jedm.12054>),    faceted and hierarchical rater models (DeCarlo, Kim & Johnson, 2011,    <doi:10.1111/j.1745-3984.2011.00143.x>),    ordinal IRT model (ISOP; Scheiblechner, 1995, <doi:10.1007/BF02301417>),     DETECT statistic (Stout, Habing, Douglas & Kim, 1996,     <doi:10.1177/014662169602000403>), local structural equation modeling     (LSEM; Hildebrandt, Luedtke, Robitzsch, Sommer & Wilhelm, 2016,    <doi:10.1080/00273171.2016.1142856>)."
"sensitivity",0,11,40,"2020-07-18 18:40","A collection of functions for factor screening, global sensitivity analysis and robustness analysis. Most of the functions have to be applied on model with scalar output, but several functions support multi-dimensional outputs."
"selac",0,4,2,"2019-03-01 00:00","Sets up and executes a SelAC model (Selection on Amino acids and codons) for testing the presence of selection in amino acid or codon among a set of genes on a fixed phylogeny. Beaulieu et al (2019) <doi:10.1093/molbev/msy222>."
"sbm",1,4,2,"2020-07-07 10:30","A collection of tools and function to adjust a variety of stochastic blockmodels (SBM).   Support at the moment Simple and Bipartite SBM (undirected and directed) for Bernoulli, Poisson and   Gaussian emission laws of the edges as described in Léger, 2016 <arxiv:1602.07587>."
"Ryacas0",5,4,2,"2019-10-09 15:40","A legacy version of 'Ryacas', an interface to the 'yacas' computer algebra system (<http://www.yacas.org/>)."
"Ryacas",7,4,15,"2020-01-07 10:50","Interface to the 'yacas' computer algebra system (<http://www.yacas.org/>)."
"rvinecopulib",0,4,17,"2020-05-07 10:40","Provides an interface to 'vinecopulib', a C++ library for vine  copula modeling. The 'rvinecopulib' package implements the core features of the popular 'VineCopula' package, in particular inference algorithms for both vine  copula and bivariate copula models. Advantages over 'VineCopula' are a sleeker  and more modern API, improved performances, especially in high dimensions,  nonparametric and multi-parameter families, and the ability to model discrete  variables. The 'rvinecopulib' package includes 'vinecopulib' as header-only  C++ library (currently version 0.5.2). Thus users do not need to install  'vinecopulib' itself in order to use 'rvinecopulib'. Since their initial  releases, 'vinecopulib' is licensed under the MIT License, and 'rvinecopulib'  is licensed under the GNU GPL version 3."
"rtweet",4,8,11,"2019-12-19 08:02","An implementation of calls designed to collect and organize Twitter data via Twitter's REST and stream Application Program Interfaces (API), which can be found at the following URL: <https://developer.twitter.com/en/docs>. This package has been peer-reviewed by rOpenSci (v. 0.6.9)."
"RScelestial",1,4,2,"2019-11-28 15:10","	Scelestial infers a lineage tree from single-cell DNA mutation matrix. 	It generates a tree with approximately maximum parsimony through 	a Steiner tree approximation algorithm. "
"rquery",9,4,31,"2020-02-18 17:50","A piped query generator based on Edgar F. Codd's relational    algebra, and on production experience using 'SQL' and 'dplyr' at big data    scale.  The design represents an attempt to make 'SQL' more teachable by    denoting composition by a sequential pipeline notation instead of nested    queries or functions.   The implementation delivers reliable high     performance data processing on large data systems such as 'Spark',    databases, and 'data.table'. Package features include: data processing trees    or pipelines as observable objects (able to report both columns    produced and columns used), optimized 'SQL' generation as an explicit    user visible table modeling step, plus explicit query reasoning and checking."
"rodham",1,1,4,"2017-07-14 07:42","Fetch and process Hillary Rodham Clinton's ""personal"" emails."
"rnetcarto",1,1,2,"2015-11-11 11:54","It provides functions to compute the modularity and modularity-related roles in networks. It is a wrapper around the rgraph library (Guimera & Amaral, 2005, doi:10.1038/nature03288). "
"rIsing",0,1,1,"2016-11-25","Fits an Ising model to a binary dataset using L1 regularized    logistic regression and extended BIC. Also includes a fast lasso logistic    regression function for high-dimensional problems. Uses the 'libLBFGS'    optimization library by Naoaki Okazaki."
"repo",1,1,8,"2019-12-19 17:40","A data manager meant to avoid manual storage/retrieval of    data to/from the file system. It builds one (or more) centralized    repository where R objects are stored with rich annotations,    including corresponding code chunks, and easily searched and    retrieved. See Napolitano (2017) <doi:10.1037/a0028240> for further    information."
"redist",1,28,8,"2016-12-21 23:29","Enables researchers to sample redistricting plans from a pre-specified    target distribution using Sequential Monte Carlo and Markov Chain Monte Carlo    algorithms.  The package allows for the implementation of various constraints in    the redistricting process such as geographic compactness and population parity    requirements. Tools for analysis such as computation of various summary statistics    and plotting functionality are also included. The package implements methods    described in Fifield, Higgins, Imai and Tarr (2020) <doi:10.1080/10618600.2020.1739532>,    Fifield, Imai, Kawahara, and Kenny (2020) <doi:10.1080/2330443X.2020.1791773>,    and McCartan and Imai (2020) <arXiv: 2008.06131>."
"recipes",7,28,14,"2020-05-01 09:30","An extensible framework to create and preprocess     design matrices. Recipes consist of one or more data manipulation     and analysis ""steps"". Statistical parameters for the steps can     be estimated from an initial data set and then applied to     other data sets. The resulting design matrices can then be used     as inputs into statistical or machine learning models. "
"readsdr",1,440,1,"2020-06-12","The goal of 'readsdr' is to bridge the design capabilities from    specialised System Dynamics software with the powerful numerical tools     offered by 'R' libraries. The package accomplishes this goal by parsing     'XMILE' files ('Vensim' and 'Stella') models into 'R' objects to construct     networks (graph theory); 'ODE' functions for 'Stan'; and inputs to simulate    via 'deSolve' as described in Duggan (2016) <doi:10.1007/978-3-319-34043-2>."
"rbmn",0,440,2,"2013-08-01 15:29","Creation, manipulation, simulation of linear Gaussian Bayesian             networks from text files and more..."
"raster",0,440,99,"2020-06-27 16:20","Reading, writing, manipulating, analyzing and modeling of spatial data. The package implements basic and high-level functions for raster data and for vector data operations such as intersections. See the manual and tutorials on <https://rspatial.org/> to get started."
"quanteda",1,28,47,"2020-07-27 07:10","A fast, flexible, and comprehensive framework for     quantitative text analysis in R.  Provides functionality for corpus management,    creating and manipulating tokens and ngrams, exploring keywords in context,     forming and manipulating sparse matrices    of documents by features and feature co-occurrences, analyzing keywords, computing feature similarities and    distances, applying content dictionaries, applying supervised and unsupervised machine learning,     visually representing text and text analyses, and more. "
"polmineR",2,4,18,"2020-07-22 12:50","Package for corpus analysis using the Corpus Workbench     ('CWB', <http://cwb.sourceforge.net/>) as an efficient back end for indexing    and querying large corpora. The package offers functionality to flexibly create    subcorpora and to carry out basic statistical operations (count, co-occurrences    etc.). The original full text of documents can be reconstructed and inspected at    any time. Beyond that, the package is intended to serve as an interface to     packages implementing advanced statistical procedures. Respective data structures    (document-term matrices, term-co-occurrence matrices etc.) can be created based     on the indexed corpora."
"pedtools",1,4,6,"2020-06-18 20:50","A lightweight, but comprehensive collection of tools for creating,     manipulating and visualising pedigrees and genetic marker data. Pedigrees     can be read from text files or created on the fly with built-in functions.     A range of utilities enable modifications like adding or removing individuals,    breaking loops, and merging pedigrees. Pedigree plots are produced by wrapping    the plotting functionality of the 'kinship2' package."
"pbixr",1,4,1,"2020-05-31","Access data and metadata from 'Microsoft' 'Power BI' ('.pbix', <https://powerbi.microsoft.com>) documents with R. The 'pbixr' package enables one to extract 'Power Query M' formulas (<https://docs.microsoft.com/en-us/power-query/>) and 'Data Analysis Expressions' ('DAX', <https://docs.microsoft.com/en-us/dax/>) queries and their properties, report layout and style, and data and data models."
"paramlink",0,4,14,"2017-08-14 17:08","A suite of tools for analysing pedigrees with marker data, including parametric linkage analysis, forensic computations, relatedness analysis and marker simulations. The core of the package is an implementation of the Elston-Stewart algorithm for pedigree likelihoods, extended to allow mutations as well as complex inbreeding. Features for linkage analysis include singlepoint LOD scores, power analysis, and multipoint analysis (the latter through a wrapper to the 'MERLIN' software). Forensic applications include exclusion probabilities, genotype distributions and conditional simulations. Data from the 'Familias' software can be imported and analysed in 'paramlink'. Finally, 'paramlink' offers many utility functions for creating, manipulating and plotting pedigrees with or without marker data (the actual plotting is done by the 'kinship2' package)."
"PairViz",3,1,7,"2018-08-09 11:20","Improving graphics by ameliorating order effects, using Eulerian tours and Hamiltonian decompositions of  graphs. "
"outbreaker2",4,1,4,"2019-06-13 17:40","Bayesian reconstruction of disease outbreaks using epidemiological    and genetic information. Jombart T, Cori A, Didelot X, Cauchemez S, Fraser    C and Ferguson N. 2014. <doi:10.1371/journal.pcbi.1003457>. Campbell, F,    Cori A, Ferguson N, Jombart T. 2019. <doi:10.1371/journal.pcbi.1006930>."
"osmar",0,1,9,"2013-09-15 11:15","This package provides infrastructure to access    OpenStreetMap data from different sources, to work with the data    in common R manner, and to convert data into available    infrastructure provided by existing R packages (e.g., into sp and    igraph objects)."
"oaqc",1,1,1,"2017-11-14","Implements the efficient algorithm by Ortmann and Brandes (2017)  <doi:10.1007/s41109-017-0027-2> to compute the orbit-aware frequency distribution of induced and non-induced quads, i.e. subgraphs of size four. Given an edge matrix, data frame, or a graph object (e.g., 'igraph'), the orbit-aware counts are computed respective each of the edges and nodes."
"nosoi",5,3,4,"2020-04-03 16:10","The aim of 'nosoi' (pronounced no.si) is to provide a flexible agent-based stochastic transmission chain/epidemic simulator (Lequime et al. bioRxiv 2020.03.03.973107). It is named after the daimones of plague, sickness and disease that escaped Pandora's jar in the Greek mythology. 'nosoi' is able to take into account the influence of multiple variable on the transmission process (e.g. dual-host systems (such as arboviruses), within-host viral dynamics, transportation, population structure), alone or taken together, to create complex but relatively intuitive epidemiological simulations."
"NLMR",1,3,10,"2018-11-16 13:10","Provides neutral landscape models (<doi:10.1007/BF02275262>,    <http://sci-hub.tw/10.1007/bf02275262>).      Neutral landscape models range from ""hard""     neutral models (completely random distributed), to ""soft"" neutral models     (definable spatial characteristics) and generate landscape patterns that are     independent of ecological processes.    Thus, these patterns can be used as null models in landscape ecology. 'nlmr'     combines a large number of algorithms from other published software for     simulating neutral landscapes. The simulation results are obtained in a    geospatial data format (raster* objects from the 'raster' package) and can,    therefore, be used in any sort of raster data operation that is performed     with standard observation data.                                                                                                  "
"NetworkRiskMeasures",0,3,3,"2017-03-26 23:01","Implements some risk measures for (financial) networks, such as DebtRank, Impact Susceptibility, Impact Diffusion and Impact Fluidity. "
"NetworkInference",2,3,9,"2018-10-24 17:20","This is an R implementation of the netinf algorithm (Gomez Rodriguez, Leskovec, and Krause, 2010)<doi:10.1145/1835804.1835933>. Given a set of events that spread between a set of nodes the algorithm infers the most likely stable diffusion network that is underlying the diffusion process."
"nbTransmission",1,3,1,"2020-07-13","Estimates the relative transmission probabilities between cases in an infectious disease outbreak or cluster using naive Bayes. Included are various functions to use these probabilities to estimate transmission parameters such as the generation/serial interval and reproductive number as well as finding the contribution of covariates to the probabilities and visualizing results. The ideal use is for an infectious disease dataset with metadata on the majority of cases but more informative data such as contact tracing or pathogen whole genome sequencing on only a subset of cases. For a detailed description of the methods see Leavitt et al. (2020) <doi:10.1093/ije/dyaa031>."
"MSG",0,3,6,"2019-09-03 17:50","A companion to the Chinese book “Modern Statistical Graphics”."
"molic",3,3,2,"2020-06-13 16:00","Outlier detection in, possibly high-dimensional, categorical data following	     Mads Lindskou et al. (2019) <doi:10.1111/sjos.12407>."
"mlr3pipelines",0,3,6,"2020-08-18 09:40","Dataflow programming toolkit that enriches 'mlr3' with a diverse  set of pipelining operators ('PipeOps') that can be composed into graphs.  Operations exist for data preprocessing, model fitting, and ensemble  learning. Graphs can themselves be treated as 'mlr3' 'Learners' and can  therefore be resampled, benchmarked, and tuned."
"MBNMAtime",1,4,2,"2019-10-28 14:40","Fits Bayesian time-course models for model-based network meta-analysis (MBNMA) and model-based meta  -analysis (MBMA) that account for repeated measures over time within studies by applying different time-course functions,  following the method of Pedder et al. (2019) <doi:10.1002/jrsm.1351>.   The method allows synthesis of studies with multiple follow-up measurements that can account for time-course for a single or multiple   treatment comparisons. Several general time-course functions are provided; others may be added   by the user. Various characteristics can be flexibly added to the models, such as correlation between time points and shared   class effects. The consistency of direct and indirect evidence in the network can be assessed using unrelated mean effects   models and/or by node-splitting."
"maotai",0,4,8,"2020-05-17 07:00","Matrix is an universal and sometimes primary object/unit in applied mathematics and statistics. We provide a number of algorithms for selected problems in optimization and statistical inference. For general exposition to the topic with focus on statistical context, see the book by Banerjee and Roy (2014, ISBN:9781420095388)."
"madrat",1,2,5,"2019-12-17 00:10","Provides a framework which should improve reproducibility and transparency in data processing. It provides functionality such as automatic meta data creation and management, rudimentary quality management, data caching, work-flow management and data aggregation.    * The title is a wish not a promise. By no means we expect this package to deliver everything what is needed to achieve full reproducibility and transparency, but we believe that it supports efforts in this direction."
"lolog",2,2,3,"2018-10-22 21:30","Estimation of Latent Order Logistic (LOLOG) Models for Networks.    LOLOGs are a flexible and fully general class of statistical graph models.     This package provides functions for performing MOM, GMM and variational     inference. Visual diagnostics and goodness of fit metrics are provided.     See Fellows (2018) <arXiv:1804.04583> for a detailed description of the methods."
"loe",0,2,2,"2014-01-31 12:04","Local Ordinal embedding (LOE) is one of graph embedding methods for unweighted graphs."
"Libra",0,13,6,"2016-02-17 14:15","Efficient procedures for fitting the regularization path    for linear, binomial, multinomial, Ising and Potts models with lasso,    group lasso or column lasso(only for multinomial) penalty.    The package uses Linearized Bregman Algorithm to solve the    regularization path through iterations. Bregman Inverse Scale Space Differential    Inclusion solver is also provided for linear model with lasso penalty."
"lfe",4,13,56,"2018-06-28 21:47","Transforms away factors with many levels prior to doing an OLS.  Useful for estimating linear models with multiple group fixed effects, and for  estimating linear models which uses factors with many levels as pure control variables.  Includes support for instrumental variables, conditional F statistics for weak instruments,  robust and multi-way clustered standard errors, as well as limited mobility bias correction."
"lava",3,10,34,"2018-01-13 01:39","A general implementation of Structural Equation Models	with latent variables (MLE, 2SLS, and composite likelihood	estimators) with both continuous, censored, and ordinal	outcomes (Holst and Budtz-Joergensen (2013) <doi:10.1007/s00180-012-0344-y>).	Mixture latent variable models and non-linear latent variable models	(Holst and Budtz-Joergensen (2019) <doi:10.1093/biostatistics/kxy082>).	The package also provides methods for graph exploration (d-separation,	back-door criterion), simulation of general non-linear latent variable	models, and estimation of influence functions for a broad range of	statistical models."
"knitrBootstrap",5,7,8,"2017-07-19 20:00","A framework to create Bootstrap <http://getbootstrap.com/> HTML reports from 'knitr'    'rmarkdown'."
"isa2",0,6,11,"2015-04-09 01:05","The ISA is a biclustering algorithm that finds modules   in an input matrix. A module or bicluster is a block of the  reordered input matrix."
"hydra",0,2,1,"2019-04-04","Calculate an optimal embedding of a set of data points into low-dimensional hyperbolic space. This uses the strain-minimizing hyperbolic embedding of Keller-Ressel and Nargang (2019), see <arXiv:1903.08977>. "
"hero",0,2,2,"2018-10-30 17:30","An implementation of the sandwich smoother proposed in Fast Bivariate Penalized Splines by Xiao et al. (2012) <doi:10.1111/rssb.12007>.    A hero is a specific type of sandwich.  Dictionary.com (2018) <https://www.dictionary.com> describes a hero as: a large sandwich, usually consisting of a small loaf of bread or long roll cut in half lengthwise and containing a variety of ingredients, as meat, cheese, lettuce, and tomatoes. "
"HelpersMG",0,2,23,"2020-06-15 23:30","Contains miscellaneous functions useful for managing 'NetCDF' files (see <http://en.wikipedia.org/wiki/NetCDF>), get tide levels on any point of the globe, get moon phase and time for sun rise and fall, analyse and reconstruct periodic time series of temperature with irregular sinusoidal pattern, show scales and wind rose in plot with change of color of text, Metropolis-Hastings algorithm for Bayesian MCMC analysis, plot graphs or boxplot with error bars, search files in disk by there names or their content, read the contents of all files from a folder at one time."
"gsbm",3,1,2,"2020-02-28 11:50","Given an adjacency matrix drawn from a Generalized Stochastic Block Model with missing observations, this package robustly estimates the probabilities of connection between nodes and detects outliers nodes, as describes in Gaucher, Klopp and Robin (2019) <arXiv:1911.13122>."
"graphon",0,1,8,"2019-11-25 21:20","Provides a not-so-comprehensive list of methods for estimating graphon,    a symmetric measurable function, from a single or multiple of observed networks.     For a detailed introduction on graphon and popular estimation techniques,     see the paper by Orbanz, P. and Roy, D.M.(2014) <doi:10.1109/TPAMI.2014.2334607>.    It also contains several auxiliary functions for generating sample networks    using various network models and graphons."
"ghypernet",3,102,1,"2020-05-20","Provides functions for model fitting and selection of generalised hypergeometric ensembles of random graphs (gHypEG).    To learn how to use it, check the vignettes for a quick tutorial.    Please reference its use as Casiraghi, G., Nanumyan, V. (2019) <doi:10.5281/zenodo.2555300>    together with those relevant references from the one listed below.    The package is based on the research developed at the Chair of Systems Design, ETH Zurich.    Casiraghi, G., Nanumyan, V., Scholtes, I., Schweitzer, F. (2016) <arXiv:1607.02441>.    Casiraghi, G., Nanumyan, V., Scholtes, I., Schweitzer, F. (2017) <doi:10.1007/978-3-319-67256-4_11>.    Casiraghi, G., (2017) <arxiv:1702.02048>    Casiraghi, G., Nanumyan, V. (2018) <arXiv:1810.06495>.    Brandenberger, L., Casiraghi, G., Nanumyan, V., Schweitzer, F. (2019) <doi:10.1145/3341161.3342926>    Casiraghi, G. (2019) <doi:10.1007/s41109-019-0241-1>."
"GGally",0,102,30,"2020-03-25 19:20","    The R package 'ggplot2' is a plotting system based on the grammar of graphics.    'GGally' extends 'ggplot2' by adding several functions    to reduce the complexity of combining geometric objects with transformed data.    Some of these functions include a pairwise plot matrix, a two group pairwise plot    matrix, a parallel coordinates plot, a survival plot, and several functions to    plot networks."
"genscore",1,21,2,"2020-04-03 18:40","Implementation of the Generalized Score Matching estimator in Yu et al. (2019) <http://jmlr.org/papers/v20/18-278.html> for non-negative graphical models (truncated Gaussian, exponential square-root, gamma, a-b models) and univariate truncated Gaussian distributions. Also includes the original estimator for untruncated Gaussian graphical models from Lin et al. (2016) <doi:10.1214/16-EJS1126>, with the addition of a diagonal multiplier."
"factoextra",0,21,5,"2019-12-05 08:50","Provides some easy-to-use functions to extract and visualize the    output of multivariate data analyses, including 'PCA' (Principal Component    Analysis), 'CA' (Correspondence Analysis), 'MCA' (Multiple Correspondence    Analysis), 'FAMD' (Factor Analysis of Mixed Data), 'MFA' (Multiple Factor Analysis) and 'HMFA' (Hierarchical Multiple    Factor Analysis) functions from different R packages. It contains also functions    for simplifying some clustering analysis steps and provides 'ggplot2' - based    elegant data visualization."
"ecospat",1,4,8,"2018-06-27 18:21","Collection of R functions and data sets for the support of spatial ecology analyses with a focus on pre, core and post modelling analyses of species distribution, niche quantification and community assembly. Written by current and former members and collaborators of the ecospat group of Antoine Guisan, Department of Ecology and Evolution (DEE) and Institute of Earth Surface Dynamics (IDYST), University of Lausanne, Switzerland. Read Di Cola et al. (2016) <doi:10.1111/ecog.02671> for details."
"dyndimred",0,1,3,"2019-08-07 11:40","  Provides a common interface for applying dimensionality reduction methods,  such as Principal Component Analysis ('PCA'), Independent Component Analysis ('ICA'), diffusion maps,   Locally-Linear Embedding ('LLE'), t-distributed Stochastic Neighbor Embedding ('t-SNE'),   and Uniform Manifold Approximation and Projection ('UMAP').   Has built-in support for sparse matrices."
"dst",4,2,5,"2019-08-20 06:50","Using the Theory of Belief Functions for evidence calculus. Basic probability assignments, or mass functions, can be defined on the subsets of a set of possible values and combined. A mass function can be extended to a larger frame. Marginalization, i.e. reduction to a smaller frame can also be done. These features can be combined to analyze small belief networks and take into account situations where information cannot be satisfactorily described by probability distributions."
"DramaAnalysis",0,2,3,"2020-01-13 10:20","Analysis of preprocessed dramatic texts, with respect to literary research.   The package provides functions to analyze and visualize information about characters,  stage directions, the dramatic structure and the text itself.  The dramatic texts are expected to be in CSV format, which can be installed from within   the package, sample texts are provided. The package and the reasoning behind it are described in   Reiter et al. (2017) <doi:10.18420/in2017_119>."
"dodgr",4,2,13,"2020-04-17 11:00","Distances on dual-weighted directed graphs using priority-queue    shortest paths (Padgham (2019) <doi:10.32866/6945>). Weighted directed    graphs have weights from A to B which may differ from those from B to A.    Dual-weighted directed graphs have two sets of such weights. A canonical    example is a street network to be used for routing in which routes are    calculated by weighting distances according to the type of way and mode of    transport, yet lengths of routes must be calculated from direct distances."
"dna",1,1,4,"2014-03-22 23:32","Package for conducting differential network analysis from        microarray data."
"diverse",0,1,5,"2016-12-09 15:15","Computes the most common diversity measures used in social and other sciences, and includes new measures from interdisciplinary research."
"dimRed",1,3,6,"2018-11-13 18:50","A collection of dimensionality reduction    techniques from R packages and a common    interface for calling the methods."
"DGCA",1,29,2,"2016-11-17 18:33","Performs differential correlation analysis on input    matrices, with multiple conditions specified by a design matrix. Contains    functions to filter, process, save, visualize, and interpret differential    correlations of identifier-pairs across the entire identifier space, or with    respect to a particular set of identifiers (e.g., one). Also contains several    functions to perform differential correlation analysis on clusters (i.e., modules)    or genes. Finally, it contains functions to generate empirical p-values for the    hypothesis tests and adjust them for multiple comparisons. Although the package    was built with gene expression data in mind, it is applicable to other types of    genomics data as well, in addition to being potentially applicable to data from    other fields entirely. It is described more fully in the manuscript    introducing it, freely available at <doi:10.1186/s12918-016-0349-1>."
"dbscan",2,29,16,"2019-08-05 19:50","A fast reimplementation of several density-based algorithms of    the DBSCAN family for spatial data. Includes the DBSCAN (density-based spatial    clustering of applications with noise) and OPTICS (ordering points to identify    the clustering structure) clustering algorithms HDBSCAN (hierarchical DBSCAN) and the LOF (local outlier    factor) algorithm. The implementations use the kd-tree data structure (from    library ANN) for faster k-nearest neighbor search. An R interface to fast kNN    and fixed-radius NN search is also provided.     See Hahsler M, Piekenbrock M and Doran D (2019) <doi:10.18637/jss.v091.i01>."
"datapack",1,1,6,"2017-08-29 13:55","Provides a flexible container to transport and manipulate complex    sets of data. These data may consist of multiple data files and associated    meta data and ancillary files. Individual data objects have associated system    level meta data, and data files are linked together using the OAI-ORE standard    resource map which describes the relationships between the files. The OAI-    ORE standard is described at <https://www.openarchives.org/ore>. Data packages    can be serialized and transported as structured files that have been created    following the BagIt specification. The BagIt specification is described at    <https://tools.ietf.org/html/draft-kunze-bagit-08>."
"dartR",1,1,9,"2019-02-07 15:13","Functions are provided that facilitate the import and analysis of    SNP (single nucleotide polymorphism) and silicodart (presence/absence) data. The main focus is on data generated    by DarT (Diversity Arrays Technology). However, once SNP or related fragment    presence/absence data from any source is imported into a genlight object many    of the functions can be used. Functions are available for input and output of    SNP and silicodart data, for reporting on and filtering on various criteria    (e.g. CallRate, Heterozygosity, Reproducibility, maximum allele frequency).    Advanced filtering is based on Linkage Disequilibrium and HWE (Hardy-Weinberg equilibrium). Other functions    are available for visualization after PCoA (Principle Coordinate Analysis), or to facilitate transfer of data    between genlight/genind objects and newhybrids, related, phylip, structure, faststructure packages."
"cppRouting",1,1,3,"2019-09-30 11:20","Calculation of distances, shortest paths and isochrones on weighted graphs using several variants of Dijkstra algorithm.    Proposed algorithms are unidirectional Dijkstra (Dijkstra, E. W. (1959) <doi:10.1007/BF01386390>),    bidirectional Dijkstra (Goldberg, Andrew & Fonseca F. Werneck, Renato (2005) <https://pdfs.semanticscholar.org/0761/18dfbe1d5a220f6ac59b4de4ad07b50283ac.pdf>),    A* search (P. E. Hart, N. J. Nilsson et B. Raphael (1968) <doi:10.1109/TSSC.1968.300136>),    new bidirectional A* (Pijls & Post (2009) <http://repub.eur.nl/pub/16100/ei2009-10.pdf>),    Contraction hierarchies (R. Geisberger, P. Sanders, D. Schultes and D. Delling (2008) <doi:10.1007/978-3-540-68552-4_24>),    PHAST (D. Delling, A.Goldberg, A. Nowatzyk, R. Werneck (2011) <doi:10.1016/j.jpdc.2012.02.007>)."
"CovCombR",1,1,1,"2020-01-18","Combine partial covariance matrices using a Wishart-EM algorithm.     Methods are described in the November 2019 article by Akdemir et al.     <https://www.biorxiv.org/content/10.1101/857425v1>.    It can be used to combine partially overlapping covariance matrices     from independent trials, partially overlapping multi-view relationship    data from genomic experiments, partially overlapping Gaussian graphs    described by their covariance structures.     High dimensional covariance estimation,     multi-view data integration.    high dimensional covariance graph estimation."
"Corbi",1,1,9,"2019-11-22 09:50","Provides a bundle of basic and fundamental bioinformatics tools,    such as network querying and alignment, subnetwork extraction and search,    network biomarker identification."
"ConnMatTools",0,1,4,"2016-11-02 17:52","Collects several different methods for analyzing and    working with connectivity data in R.  Though primarily oriented towards    marine larval dispersal, many of the methods are general and useful for    terrestrial systems as well."
"CNVScope",4,1,13,"2020-04-29 10:20","Provides the ability to create interaction maps, discover CNV map domains (edges), gene annotate interactions, and create interactive visualizations of these CNV interaction maps."
"ccdrAlgorithm",0,1,5,"2017-09-11 10:23","Implementation of the CCDr (Concave penalized Coordinate Descent with reparametrization) structure learning algorithm as described in Aragam and Zhou (2015) <http://www.jmlr.org/papers/v16/aragam15a.html>. This is a fast, score-based method for learning Bayesian networks that uses sparse regularization and block-cyclic coordinate descent."
"bsub",3,1,2,"2020-07-30 13:00","It submits R code/R scripts/shell commands to 'LSF cluster'   (<https://en.wikipedia.org/wiki/Platform_LSF>, the 'bsub' system) without   leaving R. There is also an interactive 'shiny' app for monitoring the job status."
"BoomSpikeSlab",0,1,14,"2020-03-31 18:50","Spike and slab regression with a variety of residual error  distributions corresponding to Gaussian, Student T, probit, logit, SVM, and a  few others.  Spike and slab regression is Bayesian regression with prior  distributions containing a point mass at zero.  The posterior updates the  amount of mass on this point, leading to a posterior distribution that is  actually sparse, in the sense that if you sample from it many coefficients are  actually zeros.  Sampling from this posterior distribution is an elegant way  to handle Bayesian variable selection and model averaging.  See  <doi:10.1504/IJMMNO.2014.059942> for an explanation of the Gaussian case."
"bnlearn",0,37,51,"2020-09-15 10:50","Bayesian network structure learning, parameter learning and inference.  This package implements constraint-based (PC, GS, IAMB, Inter-IAMB, Fast-IAMB, MMPC,  Hiton-PC, HPC), pairwise (ARACNE and Chow-Liu), score-based (Hill-Climbing and Tabu  Search) and hybrid (MMHC, RSMAX2, H2PC) structure learning algorithms for discrete,  Gaussian and conditional Gaussian networks, along with many score functions and  conditional independence tests.  The Naive Bayes and the Tree-Augmented Naive Bayes (TAN) classifiers are also implemented.  Some utility functions (model comparison and manipulation, random data generation, arc  orientation testing, simple and advanced plots) are included, as well as support for  parameter estimation (maximum likelihood and Bayesian) and inference, conditional  probability queries, cross-validation, bootstrap and model averaging.  Development snapshots with the latest bugfixes are available from <https://www.bnlearn.com/>."
"blackbox",0,2,10,"2019-08-20 19:00","Performs prediction of a response function from simulated response values, allowing black-box optimization of functions estimated with some error. Includes a simple user interface for such applications, as well as more specialized functions designed to be called by the Migraine software (Rousset and Leblois, 2012 <doi:10.1093/molbev/MSR262>; Leblois et al., 2014 <doi:10.1093/molbev/msu212>; and see URL). The latter functions are used for prediction of likelihood surfaces and implied likelihood ratio confidence intervals, and for exploration of predictor space of the surface. Prediction of the response is based on ordinary Kriging (with residual error) of the input. Estimation of smoothing parameters is performed by generalized cross-validation."
"BiodiversityR",0,2,34,"2019-10-03 08:50","Graphical User Interface (via the R-Commander) and utility functions (often based on the vegan package) for statistical analysis of biodiversity and ecological communities, including species accumulation curves, diversity indices, Renyi profiles, GLMs for analysis of species abundance and presence-absence, distance matrices, Mantel tests, and cluster, constrained and unconstrained ordination analysis. A book on biodiversity and community ecology analysis is available for free download from the website. In 2012, methods for (ensemble) suitability modelling and mapping were expanded in the package."
"bio3d",1,6,14,"2019-11-26 18:40","Utilities to process, organize and explore protein structure,    sequence and dynamics data. Features include the ability to read and write    structure, sequence and dynamic trajectory data, perform sequence and structure    database searches, data summaries, atom selection, alignment, superposition,    rigid core identification, clustering, torsion analysis, distance matrix    analysis, structure and sequence conservation analysis, normal mode analysis,    principal component analysis of heterogeneous structure data, and correlation    network analysis from normal mode and molecular dynamics data. In addition,    various utility functions are provided to enable the statistical and graphical    power of the R environment to work with biological sequence and structural data.    Please refer to the URLs below for more information."
"beadplexr",3,3,4,"2020-02-05 18:00","Reproducible and automated analysis of multiplex bead assays such    as CBA (Morgan et al. 2004; <doi:10.1016/j.clim.2003.11.017>), LEGENDplex    (Yu et al. 2015; <doi:10.1084/jem.20142318>), and MACSPlex (Miltenyi    Biotec 2014; Application note: Data acquisition and analysis without the    MACSQuant analyzer;    <https://www.miltenyibiotec.com/upload/assets/IM0021608.PDF>). The    package provides functions for streamlined reading of fcs files, and    identification of bead clusters and analyte expression. The package eases    the calculation of standard curves and the subsequent calculation of the    analyte concentration."
"bcp",0,3,28,"2018-06-08 23:37","Provides an implementation of the Barry and Hartigan (1993) product    partition model for the normal errors change point problem using Markov Chain    Monte Carlo. It also extends the methodology to regression models on a connected    graph (Wang and Emerson, 2015); this allows estimation of change point models    with multivariate responses. Parallel MCMC, previously available in bcp v.3.0.0,    is currently not implemented."
"ape",1,302,96,"2020-06-03 14:20","Functions for reading, writing, plotting, and manipulating phylogenetic trees, analyses of comparative data in a phylogenetic framework, ancestral character analyses, analyses of diversification and macroevolution, computing distances from DNA sequences, reading and writing nucleotide sequences as well as importing from BioConductor, and several tools such as Mantel's test, generalized skyline plots, graphical exploration of phylogenetic data (alex, trex, kronoviz), estimation of absolute evolutionary rates and clock-like trees using mean path lengths and penalized likelihood, dating trees with non-contemporaneous sequences, translating DNA into AA sequences, and assessing sequence alignments. Phylogeny estimation can be done with the NJ, BIONJ, ME, MVR, SDM, and triangle methods, and several methods handling incomplete distance matrices (NJ*, BIONJ*, MVR*, and the corresponding triangle method). Some functions call external applications (PhyML, Clustal, T-Coffee, Muscle) whose results are returned into R."
"anocva",0,1,2,"2016-12-28 17:01","Provides ANOCVA (ANalysis Of Cluster VAriability), a non-parametric statistical test    to compare clustering structures with applications in functional magnetic resonance imaging    data (fMRI). The ANOCVA allows us to compare the clustering structure of multiple groups    simultaneously and also to identify features that contribute to the differential clustering."
"anipaths",1,1,5,"2019-02-26 21:40","Animation of observed trajectories using spline-based interpolation (see for example, Buderman, F. E., Hooten, M. B., Ivan, J. S. and Shenk, T. M. (2016), <doi:10.1111/2041-210X.12465> ""A functional model for characterizing long-distance movement behaviour"". Methods Ecol Evol). Intended to be used exploratory data analysis, and perhaps for preparation of presentations."
"agop",0,1,5,"2019-03-08 11:42","Tools supporting multi-criteria and group decision making,    including variable number of criteria, by means of    aggregation operators, spread measures,    fuzzy logic connectives, fusion functions,    and preordered sets. Possible applications include,    but are not limited to, quality management, scientometrics,    software engineering, etc."
"XMRF",0,1,1,"2015-06-25","Fit Markov Networks to a wide range of high-throughput genomics data."
"xLLiM",0,2,3,"2017-05-16 16:46","Provides a tool for non linear mapping (non linear regression) using a mixture of regression model and an inverse regression strategy. The methods include the GLLiM model (see Deleforge et al (2015) <doi:10.1007/s11222-014-9461-5>) based on Gaussian mixtures and a robust version of GLLiM, named SLLiM (see Perthame et al (2016) <https://hal.archives-ouvertes.fr/hal-01347455>) based on a mixture of Generalized Student distributions. The methods also include BLLiM (see Devijver et al (2017) <https://arxiv.org/abs/1701.07899>) which is an extension of GLLiM with a sparse block diagonal structure for large covariance matrices (particularly interesting for transcriptomic data)."
"wTO",1,2,12,"2018-05-30 18:05","Computes the Weighted Topological Overlap with positive and negative signs (wTO) networks given a data frame containing the mRNA count/ expression/ abundance per sample, and a vector containing the interested nodes of interaction (a subset of the elements of the full data frame). It also computes the cut-off threshold or p-value based on the individuals bootstrap or the values reshuffle per individual. It also allows the construction of a consensus network, based on multiple wTO networks. The package includes a visualization tool for the networks.  More about the methodology can be found at <arXiv:1711.04702>."
"wiseR",0,1,1,"2018-11-29","A Shiny application for learning Bayesian Decision Networks from data. This package can be used for probabilistic reasoning (in the observational setting), causal inference (in the presence of interventions) and learning policy decisions (in Decision Network setting). Functionalities include end-to-end implementations for data-preprocessing, structure-learning, exact inference, approximate inference, extending the learned structure to Decision Networks and policy optimization using statistically rigorous methods such as bootstraps, resampling, ensemble-averaging and cross-validation. In addition to Bayesian Decision Networks, it also features correlation networks, community-detection, graph visualizations, graph exports and web-deployment of the learned models as Shiny dashboards.   "
"whitechapelR",0,1,2,"2018-07-31 11:40","Provides a set of functions to make tracking the hidden movements   of the 'Jack' player easier. By tracking every possible path Jack might have   traveled from the point of the initial murder including special movement such   as through alleyways and via carriages, the police can more accurately narrow   the field of their search. Additionally, by tracking all possible hideouts from   round to round, rounds 3 and 4 should have a vastly reduced field of search."
"WebGestaltR",0,1,12,"2020-01-16 22:00","The web version WebGestalt <http://www.webgestalt.org> supports 12 organisms, 354 gene identifiers and 321,251 function categories. Users can upload the data and functional categories with their own gene identifiers. In addition to the Over-Representation Analysis, WebGestalt also supports Gene Set Enrichment Analysis and Network Topology Analysis. The user-friendly output report allows interactive and efficient exploration of enrichment results. The WebGestaltR package not only supports all above functions but also can be integrated into other pipeline or simultaneously analyze multiple gene lists."
"VSE",1,1,1,"2016-03-21","Calculates the enrichment of associated variant set (AVS) for an array of genomic regions. The AVS is the collection of disjoint LD blocks computed from a list of disease associated SNPs and their linked (LD) SNPs. VSE generates a null distribution of matched random variant sets (MRVSs) from 1000 Genome Project Phase III data that are identical to AVS, LD block by block. It then computes the enrichment of AVS intersecting with user provided genomic features (e.g., histone marks or transcription factor binding sites) compared with the null distribution."
"vosonSML",1,1,6,"2020-04-25 09:30","A suite of tools for collecting and constructing networks from social media data.    Provides easy-to-use functions for collecting data across popular platforms (Twitter, YouTube    and Reddit) and generating different types of networks for analysis."
"VOSONDash",0,9,4,"2020-05-20 12:50","A 'Shiny' application for the interactive visualisation and    analysis of networks that also provides a web interface for collecting    social media data using 'vosonSML'."
"VertexSimilarity",0,9,1,"2016-01-24","Creates Vertex Similarity matrix of an undirected graph based  on the method stated by E. A. Leicht, Petter Holme, AND M. E. J. Newman in  their paper <doi:10.1103/PhysRevE.73.026120>."
"vennLasso",1,9,7,"2017-07-15 23:06","Provides variable selection and estimation routines for models    with main effects stratified on multiple binary factors. The 'vennLasso' package    is an implementation of the method introduced in Huling, et al. (2017) <doi:10.1111/biom.12769>."
"treemap",1,9,14,"2015-10-14 19:25","A treemap is a space-filling visualization of hierarchical    structures. This package offers great flexibility to draw treemaps."
"treefit",2,1,1,"2020-03-10","Perform two types of analysis: 1) checking the    goodness-of-fit of tree models to your single-cell gene expression    data; and 2) deciding which tree best fits your data."
"TPEA",0,1,5,"2017-01-03 07:33","We described a novel Topology-based pathway enrichment analysis, which integrated the global position of the nodes and the topological property of the pathways in  Kyoto Encyclopedia of Genes and Genomes Database.             We also provide some functions to obtain the latest information about pathways to finish pathway enrichment analysis using this method. "
"topoDistance",1,25,1,"2019-08-02","A toolkit for calculating topographic distances and identifying and plotting topographic paths. Topographic distances can be calculated along shortest topographic paths (Wang (2009) <doi:10.1111/j.1365-294X.2009.04338.x>), weighted topographic paths (Zhan et al. (1993) <doi:10.1007/3-540-57207-4_29>), and topographic least cost paths (Wang and Summers (2010) <doi:10.1111/j.1365-294X.2009.04465.x>). Functions can map topographic paths on colored or hill shade maps and plot topographic cross sections (elevation profiles) for the paths."
"TOHM",0,25,1,"2019-08-28","Approximations of global p-values when testing hypothesis in presence of non-identifiable nuisance parameters. The method relies on the Euler characteristic heuristic and the expected Euler characteristic is efficiently computed by  in Algeri and van Dyk (2018) <arXiv:1803.03858>. "
"Tlasso",1,25,1,"2016-09-19","An optimal alternating optimization algorithm for estimation of precision matrices of sparse tensor graphical models, and an efficient inference procedure for support recovery of the precision matrices."
"tilemaps",1,25,2,"2020-06-19 12:50","Implements an algorithm for generating maps, known as tile maps,    in which each region is represented by a single tile of the same shape and    size. The algorithm was first proposed in ""Generating Tile Maps"" by Graham    McNeill and Scott Hale (2017) <doi:10.1111/cgf.13200>. Functions allow    users to generate, plot, and compare square or hexagon tile maps."
"tidySEM",4,25,1,"2020-06-25","A tidy workflow for generating, estimating, reporting,    and plotting structural equation models using 'lavaan' or 'Mplus'.     Throughout this workflow, elements of syntax, results, and graphs are     represented as 'tidy' data, making them easy to customize."
"tidygraph",0,25,5,"2019-02-18 23:30","A graph, while not ""tidy"" in itself, can be thought of as two tidy    data frames describing node and edge data respectively. 'tidygraph'    provides an approach to manipulate these two virtual data frames using the    API defined in the 'dplyr' package, as well as provides tidy interfaces to     a lot of common graph algorithms."
"textrank",1,4,3,"2017-12-18 10:17","The 'textrank' algorithm is an extension of the 'Pagerank' algorithm for text. The algorithm allows to summarize text by calculating how sentences are related to one another. This is done by looking at overlapping terminology used in sentences in order to set up links between sentences. The resulting sentence network is next plugged into the 'Pagerank' algorithm which identifies the most important sentences in your text and ranks them.     In a similar way 'textrank' can also be used to extract keywords. A word network is constructed by looking if words are following one another. On top of that network the 'Pagerank' algorithm is applied to extract relevant words after which relevant words which are following one another are combined to get keywords.      More information can be found in the paper from Mihalcea, Rada & Tarau, Paul (2004) <http://www.aclweb.org/anthology/W04-3252>."
"TDA",1,4,13,"2017-12-09 10:25","Tools for the statistical analysis of persistent homology and for    density clustering. For that, this package provides an R interface for the    efficient algorithms of the C++ libraries 'GUDHI' <http://gudhi.gforge.inria.fr/>, 'Dionysus' <http://www.mrzv.org/software/dionysus/>, and 'PHAT' <https://bitbucket.org/phat-code/phat/>. This package also implements the methods in Fasy et al. (2014) <doi:10.1214/14-AOS1252> and Chazal et al. (2014) <doi:10.1145/2582112.2582128>  for analyzing the statistical significance of persistent homology features."
"TAShiny",0,2,1,"2018-11-27","Interactive shiny application for working with textmining and text analytics. Various visualizations  are provided."
"SystemicR",0,2,1,"2020-05-08","The past decade has demonstrated an increased need to better understand risks leading to systemic crises. This framework offers scholars, practitioners and policymakers a useful toolbox to explore such risks in financial systems. Specifically, this framework provides popular econometric and network measures to monitor systemic risk and to measure the consequences of regulatory decisions. These systemic risk measures are based on the frameworks of Adrian and Brunnermeier (2016) <doi:10.1257/aer.20120555> and Billio, Getmansky, Lo and Pelizzon (2012) <doi:10.1016/j.jfineco.2011.12.010>."
"SubtypeDrug",1,2,3,"2020-06-26 06:50","A systematic biology tool was developed to prioritize cancer subtype-specific drugs by integrating genetic perturbation, drug action, biological pathway, and cancer subtype.     The capabilities of this tool include inferring patient-specific subpathway activity profiles in the context of gene expression profiles with subtype labels, calculating differentially     expressed subpathways based on cultured human cells treated with drugs in the 'cMap' (connectivity map) database, prioritizing cancer subtype specific drugs according to drug-disease     reverse association score based on subpathway, and visualization of results (Castelo (2013) <doi:10.1186/1471-2105-14-7>; Han et al (2019) <doi:10.1093/bioinformatics/btz894>; Lamb and Justin (2006) <doi:10.1126/science.1132939>)."
"STraTUS",0,2,3,"2019-03-18 17:23","For a single, known pathogen phylogeny, provides functions for enumeration of the set of compatible epidemic transmission trees, and for uniform sampling from that set. Optional arguments allow for incomplete sampling with a known number of missing individuals, multiple sampling, and known infection time limits. Always assumed are a complete transmission bottleneck and no superinfection or reinfection. See Hall and Colijn (2019) <doi:10.1093/molbev/msz058> for methodology."
"stplanr",6,2,36,"2020-08-28 13:30","Tools for transport planning with an emphasis on spatial transport    data and non-motorized modes. Enables common transport planning tasks including:    downloading and cleaning transport datasets; creating geographic ""desire lines""    from origin-destination (OD) data; route assignment, locally and via    interfaces to routing services such as <https://cyclestreets.net/>;    calculation of route segment attributes such as bearing and aggregate flow;    and 'travel watershed' analysis.    See Lovelace and Ellison (2018) <doi:10.32614/RJ-2018-053> and vignettes    for details."
"stminsights",1,3,4,"2018-08-30 15:10","This app enables interactive validation, interpretation and visualization of structural topic models from the 'stm' package by Roberts and others (2014) <doi:10.1111/ajps.12103>. It also includes helper functions for model diagnostics and extracting data from effect estimates."
"stemmatology",1,3,2,"2018-05-27 18:34","Explore and analyse the genealogy of textual or musical traditions, from their variants, with various stemmatological methods, mainly the disagreement-based algorithms suggested by Camps and Cafiero (2015) <doi:10.1484/M.LECTIO-EB.5.102565>."
"statGraph",0,3,3,"2019-01-10 19:00","Contains statistical methods to analyze graphs, such as        graph parameter estimation, model selection based on the GIC        (Graph Information Criterion), statistical tests to        discriminate two or more populations of graphs (ANOGVA -        Analysis of Graph Variability), correlation between graphs, and        clustering of graphs. References: Takahashi et al. (2012)        <doi:10.1371/journal.pone.0049949>, Futija et al. (2017)        <doi:10.3389/fnins.2017.00066>, Fujita et al. (2017)        <doi:10.1016/j.csda.2016.11.016>, Tang et al. (2017)        <doi:10.3150/15-BEJ789>, Tang et al. (2017)        <doi:10.1080/10618600.2016.1193505>, Ghoshdastidar et al.        (2017) <arXiv:1705.06168>, Ghoshdastidar et al. (2017)        <arXiv:1707.00833>, Cerqueira et al. (2017)        <doi:10.1109/TNSE.2017.2674026>, Fraiman and Fraiman (2018)        <doi:10.1038/s41598-018-23152-5>, Fujita et al. (2019)        <doi:10.1093/comnet/cnz028>."
"SSN",0,3,16,"2013-04-06 06:46","Spatial statistical modeling and prediction for data on stream networks, including models based on in-stream distance (Ver Hoef, J.M. and Peterson, E.E., 2010. <doi:10.1198/jasa.2009.ap08248>.) Models are created using moving average constructions. Spatial linear models, including explanatory variables, can be fit with (restricted) maximum likelihood.  Mapping and other graphical functions are included. "
"spreadr",1,1,1,"2018-11-19","The notion of spreading activation is a prevalent metaphor in the cognitive sciences.     This package provides the tools for cognitive scientists and psychologists to conduct computer simulations     that implement spreading activation in a network representation. The algorithmic method implemented in 'spreadr'     subroutines follows the approach described in Vitevitch, Ercal, and Adagarla (2011, Frontiers),     who viewed activation as a fixed cognitive resource that could spread among nodes that were connected to     each other via edges or connections (i.e., a network). See Vitevitch, M. S., Ercal, G., & Adagarla, B. (2011).     Simulating retrieval from a highly clustered network: Implications for spoken word recognition.     Frontiers in Psychology, 2, 369. <doi:10.3389/fpsyg.2011.00369>. "
"specr",5,3,1,"2020-03-26","Provides utilities for conducting specification curve analyses (Simonsohn, Simmons & Nelson (2015, <doi:10.2139/ssrn.2694998>) or multiverse analyses (Steegen, Tuerlinckx, Gelman & Vanpaemel, 2016, <doi:10.1177/1745691616658637>) including functions to setup, run, evaluate, and plot all specifications."
"spatsoc",4,3,8,"2020-03-28 06:30","Detects spatial and temporal groups in GPS relocations     (Robitaille et al. (2020) <doi:10.1111/2041-210X.13215>).     It can be used to convert GPS relocations to     gambit-of-the-group format to build proximity-based social networks     In addition, the randomizations function provides data-stream     randomization methods suitable for GPS data. "
"SpatialGraph",0,3,2,"2018-11-02 19:20","Provision of the S4 SpatialGraph class built on top of objects provided by 'igraph' and 'sp' packages, and associated utilities. See the documentation of the SpatialGraph-class within this package for further description. An example of how from a few points one can arrive to a SpatialGraph is provided in the function sl2sg().  "
"spathial",1,3,2,"2020-03-25 16:30","A generic tool for manifold analysis. It allows to infer a relevant transition or evolutionary path which can highlights the features involved in a specific process. 'spathial' can be useful in all the scenarios where the temporal (or pseudo-temporal) evolution is the main problem (e.g. tumor progression). The algorithm for finding the principal path is described in: Ferrarotti et al., (2019) <doi:10.1109/TNNLS.2018.2884792>."""
"SpaDES.core",4,3,14,"2020-08-28 10:00","Provides the core framework for a discrete event system (DES) to     implement a complete data-to-decisions, reproducible workflow.    The core DES components facilitate modularity, and easily enable the user    to include additional functionality by running user-built modules.    Includes conditional scheduling, restart after interruption, packaging of    reusable modules, tools for developing arbitrary automated workflows,    automated interweaving of modules of different temporal resolution,    and tools for visualizing and understanding the DES project."
"SourceSet",1,1,3,"2018-04-24 10:01","The algorithm pursues the identification of the set of variables driving the differences in two different experimental conditions (i.e., the primary genes) within a graphical model context. It uses the idea of simultaneously looking for the differences between two multivariate normal distributions in all marginal and conditional distributions associated with a decomposable graph, which represents the pathway under exam. The implementation accommodates genomics specific issues (low sample size and multiple testing issues) and provides a number of functions offering numerical and visual summaries to help the user interpret the obtained results. In order to use the (optional) 'Cytoscape' functionalities, the suggested 'r2cytoscape' package must be installed from the 'GitHub' repository ('devtools::install_github('cytoscape/r2cytoscape')'). "
"solitude",0,1,7,"2020-07-07 11:20","Isolation forest is anomaly detection method introduced by the paper Isolation based Anomaly Detection (Liu, Ting and Zhou <doi:10.1145/2133360.2133363>)."
"SNscan",0,4,1,"2016-01-19","Scan statistics applied in social network data can be used to test the cluster characteristics among a social network."
"snowboot",0,4,6,"2019-02-24 18:00","Functions for analysis of network objects, which are imported or simulated by the package. The non-parametric methods of analysis center on snowball and bootstrap sampling for estimating functions of network degree distribution. For other parameters of interest, see, e.g., 'bootnet' package."
"snahelper",0,4,4,"2020-03-04 01:40","'RStudio' addin which provides a GUI to visualize and analyse networks.     After finishing a session, the code to produce the plot is inserted in the current script.    Alternatively, the function SNAhelperGadget() can be used directly from the console.    Additional addins include the Netreader() for reading network files, Netbuilder() to create    small networks via point and click, and the Componentlayouter() to layout networks with many components manually."
"smotefamily",0,4,5,"2018-12-18 12:20","A collection of various oversampling techniques developed from SMOTE is provided. SMOTE is a oversampling technique which synthesizes a new minority instance between a pair of one minority instance and one of its K nearest neighbor. (see <https://www.jair.org/media/953/live-953-2037-jair.pdf> for more information) Other techniques adopt this concept with other criteria in order to generate balanced dataset for class imbalance problem."
"SmCCNet",0,1,1,"2019-03-04","    A canonical correlation based framework for constructing phenotype-specific multi-omics networks by integrating multiple omics data types and a quantitative phenotype of interest. "
"smartR",2,1,2,"2018-12-01 00:40","A tool for assessing bio-economic feedback in   different management scenarios (D'Andrea et al., 2020   <doi:10.1111/2041-210X.13394>). 'smartR' (Spatial Management and Assessment   of demersal Resources for Trawl fisheries) combines information from different   tasks gathered within the European Data Collection Framework for the fishery  sector. The 'smartR' package implements the SMART model (Russo et al., 2014   <doi:10.1371/journal.pone.0086222>), through the object-oriented   programming paradigm, and within this package it is possible to achieve the   complete set of analyses required by the SMART approach: from the editing and   formatting of the raw data; the construction and maintenance of coherent   datasets; the numerical and visual inspection of the generated metadata; to   the final simulation of management scenarios and the forecast of their   effects. The interaction between the user and the application could take   place through invocation of methods via the command line or could be entirely   committed to the graphical user interfaces (GUI)."
"SLICER",1,1,1,"2017-08-22","Provides an implementation of SLICER, an algorithm for inferring cellular trajectories from single cell RNA sequencing data. See Welch, JD, Hartemink AJ, Prins JF (2016) <doi:10.1186/s13059-016-0975-3>."
"skynet",1,1,7,"2020-06-02 16:20","A flexible tool that allows generating bespoke    air transport statistics for urban studies based on publicly available    data from the Bureau of Transport Statistics (BTS) in the United States    <https://www.transtats.bts.gov/databases.asp?Mode_ID=1&Mode_Desc=Aviation&Subject_ID2=0>."
"sismonr",1,1,5,"2019-12-12 10:10","A tool for the simulation of gene expression profiles for in silico regulatory networks. The package generates gene regulatory networks, which include protein-coding and noncoding genes linked via different types of regulation: regulation of transcription, translation, RNA or protein decay, and post-translational modifications. The effect of genetic mutations on the system behaviour is accounted for via the simulation of genetically different in silico individuals. The ploidy of the system is not restricted to the usual haploid or diploid situations, but is defined by the user. A choice of stochastic simulation algorithms allow us to simulate the expression profiles (RNA and if applicable protein abundance) of the genes in the in silico system for the different in silico individuals. A tutorial explaining how to use the package is available at <https://oliviaab.github.io/sismonr/>. Manuscript in preparation; see also Angelin-Bonnet O., Biggs P.J. and Vignes M. (2018) <doi:10.1109/BIBM.2018.8621131>. Note that sismonr relies on Julia code called internally by the functions. No knowledge of Julia is required in order to use sismonr, but Julia must be installed on the computer (instructions can be found in the tutorial, the GitHub page or the vignette of the package)."
"SIRE",0,1,2,"2018-07-30 15:20","Provides two main functionalities.    1 - Given a system of simultaneous equation,    it decomposes  the matrix of coefficients weighting the endogenous variables     into three submatrices: one includes the subset of coefficients that have a causal nature    in the model, two include the subset of coefficients that have a interdependent nature    in the model, either at systematic level or induced by the correlation between error terms.    2 - Given a decomposed model,    it tests for the significance of the interdependent relationships acting in the system,     via Maximum likelihood and Wald test, which can be built starting from the function output.    For theoretical reference see Faliva (1992) <doi:10.1007/BF02589085> and     Faliva and Zoia (1994) <doi:10.1007/BF02589041>."
"SimSurvey",1,1,1,"2020-09-09","Simulate age-structured populations that vary in space and time and     explore the efficacy of a range of built-in or user-defined sampling     protocols to reproduce the population parameters of the known population.     (See Regular et al. (2020) <doi.org/10.1371/journal.pone.0232822> for more    details)."
"simPATHy",1,1,4,"2019-10-18 23:20","Simulate data from a Gaussian graphical model or a Gaussian Bayesian network in two conditions. Given a covariance matrix of a reference condition simulate plausible disregulations."
"signnet",6,1,5,"2020-06-30 17:40","Methods for the analysis of signed networks. This includes several measures for structural balance as introduced by Cartwright and Harary (1956) <doi:10.1037/h0046049>, blockmodeling algorithms from Doreian (2008) <doi:10.1016/j.socnet.2008.03.005>, various centrality indices, and projections of signed two-mode networks introduced by Schoch (2020) <doi:10.1080/0022250X.2019.1711376>."
"sigmaNet",1,1,2,"2018-01-24 11:05","Create interactive graph visualizations using 'Sigma.js' <http://sigmajs.org/>.  This package is meant to be used in conjunction    with 'igraph', replacing the (somewhat underwhelming) plotting features of the package.  The idea is to quickly render    graphs, regardless of their size, in a way that allows for easy, iterative modification of aesthetics.  Because     'Sigma.js' is a 'javascript' library, the visualizations are inherently interactive and are well suited for integration    with 'Shiny' apps.  While there are several 'htmlwidgets' focused on network visualization, they tend to underperform on     medium to large sized graphs.  'Sigma.js' was designed for larger network visualizations and this package aims to     make those strengths available to 'R' users."
"sigmajs",0,1,4,"2019-04-09 13:10","Interface to 'sigma.js' graph visualization library including animations, plugins and shiny proxies."
"sidier",0,1,9,"2017-06-08 18:25","Evolutionary reconstruction based on substitutions and insertion-deletion (indels) analyses in a distance-based framework."
"SID",0,1,1,"2015-03-07","The code computes the structural intervention distance (SID) between a true directed acyclic graph (DAG) and an estimated DAG. Definition and details about the implementation can be found in J. Peters and P. Bühlmann: ""Structural intervention distance (SID) for evaluating causal graphs"", Neural Computation 27, pages 771-799, 2015."
"shock",0,1,1,"2015-12-24","Block-diagonal covariance selection for high dimensional Gaussian    graphical models. The selection procedure is based on the slope heuristics."
"shazam",5,2,17,"2020-07-19 18:30","Provides a computational framework for analyzing mutations in    immunoglobulin (Ig) sequences. Includes methods for Bayesian estimation of    antigen-driven selection pressure, mutational load quantification, building of    somatic hypermutation (SHM) models, and model-dependent distance calculations.    Also includes empirically derived models of SHM for both mice and humans.    Citations:     Gupta and Vander Heiden, et al (2015) <doi:10.1093/bioinformatics/btv359>,     Yaari, et al (2012) <doi:10.1093/nar/gks457>,     Yaari, et al (2013) <doi:10.3389/fimmu.2013.00358>,     Cui, et al (2016) <doi:10.4049/jimmunol.1502263>."
"sharpshootR",0,1,8,"2019-11-21 00:40","Miscellaneous soil data management, summary, visualization, and conversion utilities to support soil survey."
"Seurat",0,31,21,"2020-09-07 12:20","A toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. 'Seurat' aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. See Satija R, Farrell J, Gennert D, et al (2015) <doi:10.1038/nbt.3192>, Macosko E, Basu A, Satija R, et al (2015) <doi:10.1016/j.cell.2015.05.002>, and Stuart T, Butler A, et al (2019) <doi:10.1016/j.cell.2019.05.031> for more details."
"SetRank",1,1,3,"2015-12-09 21:44","Implements an algorithm to conduct advanced    gene set enrichment analysis on the results of genomics experiments."
"SeqNet",0,1,2,"2019-06-24 13:10","Methods to generate random gene-gene association networks and simulate RNA-seq data from them. Includes functions to generate random networks of any size and perturb them to obtain differential networks. Network objects are built from individual, overlapping modules that represent pathways. The resulting network has various topological properties that are characteristic of gene regulatory networks. RNA-seq data can be generated such that the association among gene expression profiles reflect the underlying network. A reference RNA-seq dataset can be provided to model realistic marginal distributions. Plotting functions are available to visualize a network, compare two networks, and compare the expression of two genes across multiple networks."
"seqHMM",4,11,15,"2019-06-17 15:50","Designed for fitting hidden (latent) Markov models and mixture    hidden Markov models for social sequence data and other categorical time series.    Also some more restricted versions of these type of models are available: Markov    models, mixture Markov models, and latent class models. The package supports    models for one or multiple subjects with one or multiple parallel sequences    (channels). External covariates can be added to explain cluster membership in    mixture models. The package provides functions for evaluating and comparing    models, as well as functions for visualizing of multichannel sequence data and    hidden Markov models. Models are estimated using maximum likelihood via the EM    algorithm and/or direct numerical maximization with analytical gradients. All    main algorithms are written in C++ with support for parallel computation.     Documentation is available via several vignettes in this page, and the      paper by Helske and Helske (2019, <doi:10.18637/jss.v088.i03>)."
"SeqDetect",1,11,3,"2020-02-21 18:20","Sequence detector in this package contains a specific automaton model that can be used to learn and detect data and process sequences.    Automaton model in this package is capable of learning and tracing sequences. Automaton model can be found in Krleža, Vrdoljak, Brcic (2019) <doi:10.1109/ACCESS.2019.2955245>.    This research has been partly supported under Competitiveness and Cohesion Operational Programme from the European Regional and Development Fund, as part of the Integrated Anti-Fraud System project no. KK.01.2.1.01.0041. This research has also been partly supported by the European Regional Development Fund under the grant KK.01.1.1.01.0009."
"semPlot",0,11,9,"2017-03-27 15:56","Path diagrams and visual analysis of various SEM packages' output."
"SemNeT",0,1,7,"2020-06-09 16:10","Implements several functions for the analysis of semantic networks including different network estimation algorithms, partial node bootstrapping (Kenett, Anaki, & Faust, 2014 <doi:10.3389/fnhum.2014.00407>), random walk simulation (Kenett & Austerweil, 2016 <http://alab.psych.wisc.edu/papers/files/Kenett16CreativityRW.pdf>), and a function to compute global network measures. Significance tests and plotting features are also implemented. "
"SEMID",0,1,6,"2017-10-15 01:02","Provides routines to check identifiability or non-identifiability    of linear structural equation models as described in Drton, Foygel, and    Sullivant (2011) <doi:10.1214/10-AOS859>, Foygel, Draisma, and Drton (2012)     <doi:10.1214/12-AOS1012>, and other works. The routines are based on the graphical     representation of structural equation models by a path diagram/mixed graph."
"SelectBoost",3,1,2,"2019-05-27 11:20","An implementation of the selectboost algorithm (Bertrand et al. 2020, <arXiv:1810.01670>), which is a general algorithm that improves the precision of any existing variable selection method. This algorithm is based on highly intensive simulations and takes into account the correlation structure of the data. It can either produce a confidence index for variable selection or it can be used in an experimental design planning perspective."
"secuTrialR",1,4,1,"2020-04-24","Seamless and standardized interaction with data    exported from the clinical data management system (CDMS)    'secuTrial'<https://www.secutrial.com>.    The primary data export the package works with is a standard non-rectangular export."
"scRNAtools",0,4,1,"2018-07-05","We integrated the common analysis methods utilized in single cell RNA sequencing data, which included cluster method, principal components analysis (PCA), the filter of differentially expressed genes, pathway enrichment analysis and correlated analysis methods."
"ScorePlus",0,4,1,"2019-06-14","Implementation of community detection algorithm SCORE in the paper J. Jin (2015) <arXiv:1211.5803>, and SCORE+ in J. Jin, Z. Ke and S. Luo (2018) <arXiv:1811.05927>. Membership estimation algorithm called Mixed-SCORE in J. Jin, Z. Ke and S. Luo (2017) <arXiv:1708.07852>."
"sccore",0,4,1,"2020-09-30","Core utilities for single-cell RNA-seq data analysis. Contained within are utility functions for working with differential expression (DE) matrices and count matrices, a collection of functions for manipulating and plotting data via 'ggplot2', and functions to work with cell graphs and cell embeddings. Graph-based methods include embedding kNN cell graphs into a UMAP <doi:10.21105/joss.00861>, collapsing vertices of each cluster in the graph, and propagating graph labels."
"scalpel",0,4,3,"2018-09-28 20:00","Identifies the locations of neurons, and estimates their calcium concentrations over time using the SCALPEL method proposed in Petersen, Ashley; Simon, Noah; Witten, Daniela. SCALPEL: Extracting neurons from calcium imaging data. Ann. Appl. Stat. 12 (2018), no. 4, 2430–2456. doi:10.1214/18-AOAS1159. <https://projecteuclid.org/euclid.aoas/1542078051>."
"sBIC",6,4,1,"2016-10-01","Computes the sBIC for various singular model collections including:    binomial mixtures, factor analysis models, Gaussian mixtures,    latent forests, latent class analyses, and reduced rank regressions."
"safedata",2,4,3,"2020-07-07 17:00","The SAFE Project (<https://www.safeproject.net/>) is a large 	scale ecological experiment in Malaysian Borneo that explores the impact 	of habitat fragmentation and conversion on ecosystem function and services. 	Data collected at the SAFE Project is made available under a common format 	through the Zenodo data repository and this package makes it easy to 	discover and load that data into R."
"rsyntax",0,4,1,"2020-06-02","Various functions for querying and reshaping dependency trees,     as for instance created with the 'spacyr' or 'udpipe' packages.    This enables the automatic extraction of useful semantic relations from texts,    such as quotes (who said what) and clauses (who did what). Method proposed in     Van Atteveldt et al. (2017) <doi:10.1017/pan.2016.12>."
"rsppfp",3,4,2,"2018-11-20 16:50","An implementation of functionalities to transform directed graphs that are bound to a set of  known forbidden paths. There are several transformations, following the rules provided by Villeneuve   and Desaulniers (2005) <doi:10.1016/j.ejor.2004.01.032>, and Hsu et al. (2009) <doi:10.1007/978-3-642-03095-6_60>.   The resulting graph is generated in a data-frame format. See rsppfp website for more information,   documentation an examples."
"RPS",0,4,2,"2018-07-18 15:50","Based on RPS tools, a rather complete resistant shape    analysis of 2D and 3D datasets based on landmarks can be performed. In addition,     landmark-based resistant shape analysis of individual asymmetry in 2D for    matching or object symmetric structures is also possible."
"rPref",2,4,11,"2016-12-18 11:23","Routines to select and visualize the maxima for a given strict    partial order. This especially includes the computation of the Pareto    frontier, also known as (Top-k) Skyline operator (see Börzsönyi, et al.     (2001) <doi:10.1109/ICDE.2001.914855>), and some generalizations     known as database preferences (see Kießling (2002)     <doi:10.1016/B978-155860869-6/50035-4>)."
"RPANDA",0,2,10,"2020-03-06 20:10","Implements macroevolutionary analyses on phylogenetic trees. See    Morlon et al. (2010) <doi:10.1371/journal.pbio.1000493>, Morlon et al. (2011)    <doi:10.1073/pnas.1102543108>, Condamine et al. (2013) <doi:10.1111/ele.12062>,     Morlon et al. (2014) <doi:10.1111/ele.12251>, Manceau et al. (2015) <doi:10.1111/ele.12415>,    Lewitus & Morlon (2016) <doi:10.1093/sysbio/syv116>, Drury et al. (2016) <doi:10.1093/sysbio/syw020>,    Manceau et al. (2016) <doi:10.1093/sysbio/syw115>, Morlon et al. (2016) <doi:10.1111/2041-210X.12526>, Clavel & Morlon (2017) <doi:10.1073/pnas.1606868114>,     Drury et al. (2017) <doi:10.1093/sysbio/syx079>, Lewitus & Morlon (2017) <doi:10.1093/sysbio/syx095>,     Drury et al. (2018) <doi:10.1371/journal.pbio.2003563>, Clavel et al. (2019) <doi:10.1093/sysbio/syy045>, Maliet et al. (2019) <doi:10.1038/s41559-019-0908-0>,    Billaud et al. (2019) <doi:10.1093/sysbio/syz057>, Lewitus et al. (2019) <doi:10.1093/sysbio/syz061>,    Aristide & Morlon (2019) <doi:10.1111/ele.13385>, and Maliet et al. (2020) <doi:10.1111/ele.13592>."
"roprov",0,2,3,"2018-08-16 17:10","A suite of classes and methods which provide low-level support for modeling provenance between in-memory R objects. This is an infrastructure package and is not intended to be used directly by end-users."
"roots",1,1,1,"2017-07-11","A set of tools to reconstruct ordered ontogenic trajectories from    single cell RNAseq data."
"robustSingleCell",1,1,2,"2019-04-01 09:30","Robust single cell clustering and comparison of population compositions across tissues and   experimental models via similarity analysis from Magen 2019 <doi:10.1101/543199>. "
"robustrao",0,1,6,"2020-01-22 14:40","A collection of functions to compute the Rao-Stirling diversity index	(Porter and Rafols, 2009) <doi:10.1007/s11192-008-2197-2> and its extension to	acknowledge missing data (i.e.,	uncategorized references) by calculating its	interval of uncertainty using	mathematical optimization as proposed in Calatrava	et al. (2016) <doi:10.1007/s11192-016-1842-4>.	The Rao-Stirling diversity index is a well-established bibliometric indicator	to measure the interdisciplinarity of scientific publications. Apart from the	obligatory dataset of publications with their respective references and	a	taxonomy of disciplines that categorizes references as well as a measure of	similarity between the disciplines, the Rao-Stirling diversity index requires	a complete categorization of all references of a publication into disciplines.	Thus, it fails for a incomplete categorization; in this case, the robust	extension has to be used, which encodes the uncertainty caused by missing	bibliographic data as an uncertainty interval.	Classification / ACM - 2012: Information systems ~ Similarity measures,	Theory of computation ~ Quadratic	programming, Applied computing ~ Digital	libraries and archives."
"Robocoap",0,1,1,"2017-07-06","Generation of dynamic coappearance matrices for elements             within a text along with utilities to aid in the generation	     of Gephi dynamic networks."
"Rnmr1D",1,1,4,"2019-03-27 16:40","Perform the complete processing of a set of proton nuclear magnetic resonance spectra from the free induction decay (raw data) and based on a processing sequence (macro-command file). An additional file specifies all the spectra to be considered by associating their sample code as well as the levels of experimental factors to which they belong. More detail can be found in Jacob et al. (2017) <doi:10.1007/s11306-017-1178-y>."
"RNAseqNet",1,1,5,"2020-03-05 15:30","Infer log-linear Poisson Graphical Model with an auxiliary data    set. Hot-deck multiple imputation method is used to improve the reliability    of the inference with an auxiliary dataset. Standard log-linear Poisson     graphical model can also be used for the inference and the Stability     Approach for Regularization Selection (StARS) is implemented to drive the     selection of the regularization parameter. The method is fully described in    <doi:10.1093/bioinformatics/btx819>."
"rmonad",4,1,6,"2018-03-10 22:55","    A monadic solution to pipeline analysis. All operations – and the errors,    warnings and messages they emit – are merged into a directed graph. Infix    binary operators mediate when values are stored, how exceptions are    handled, and where pipelines branch and merge. The resulting structure may    be queried for debugging or report generation. 'rmonad' complements, rather    than competes with, non-monadic pipeline packages like 'magrittr' or    'pipeR'. This work is funded by the NSF (award number 1546858)."
"rmcfs",1,1,19,"2019-05-12 07:00","MCFS-ID (Monte Carlo Feature Selection and Interdependency Discovery) is a Monte Carlo method-based tool for feature selection. It also allows for the discovery of interdependencies between the relevant features. MCFS-ID is particularly suitable for the analysis of high-dimensional, 'small n large p' transactional and biological data. M. Draminski, J. Koronacki (2018) <doi:10.18637/jss.v085.i12>."
"rgexf",1,1,7,"2015-03-24 18:27","Create, read and write 'GEXF' (Graph Exchange 'XML' Format) graph    files (used in 'Gephi' and others). Using the 'XML' package, it allows the user to    easily build/read graph files including attributes, 'GEXF' visual attributes (such    as color, size, and position), network dynamics (for both edges and nodes) and    edge weighting. Users can build/handle graphs element-by-element or massively    through data-frames, visualize the graph on a web browser through 'gexf-js' (a    'javascript' library) and interact with the 'igraph' package."
"rflexscan",0,2,4,"2020-06-23 14:30","Functions for the detection of spatial clusters using the flexible    spatial scan statistic developed by Tango and Takahashi (2005) <doi:10.1186/1476-072X-4-11>.    This package implements a wrapper for the C routine used in the FleXScan 3.1.2     <https://sites.google.com/site/flexscansoftware/home> developed by Takahashi,    Yokoyama, and Tango."
"RevEcoR",1,2,3,"2015-06-08 08:35","An implementation of the reverse ecology framework. Reverse ecology    refers to the use of genomics to study ecology with no a priori assumptions    about the organism(s) under consideration, linking organisms to their    environment. It allows researchers to reconstruct the metabolic networks and    study the ecology of poorly characterized microbial species from their    genomic information, and has substantial potentials for microbial community    ecological analysis."
"restlos",0,2,5,"2012-08-24 11:10","The restlos package provides algorithms for robust estimation of location (mean and mode) and scatter based on minimum spanning trees (pMST), self-organizing maps (Flood Algorithm), Delaunay triangulations (RDELA), and nested minimum volume convex sets (MVCH). The functions are also suitable for outlier detection."
"rescue",0,2,1,"2020-07-18","Given a log-transformed expression matrix and list of informative genes:     subsample informative genes, cluster samples using shared nearest neighbors clustering,     estimate missing expression values with the distribution mean of means extrapolated     from these cell clusterings, and return an imputed expression matrix. See Tracy, S.,     Yuan, G.C. and Dries, R. (2019) <doi:10.1186/s12859-019-2977-0> for more details."
"RedditExtractoR",0,2,7,"2018-06-03 16:54","A collection of tools for extracting structured data from <https://www.reddit.com/>."
"ReDaMoR",1,2,1,"2020-03-31","The aim of this package is to manipulate relational data models in R.   It provides functions to create, modify and export data models in json format.   It also allows importing models created with 'MySQL Workbench' (<https://www.mysql.com/products/workbench/>).   These functions are accessible through a graphical user interface made with 'shiny'.   Constraints such as types, keys, uniqueness and mandatory fields are automatically checked and corrected when editing a model.   Finally, real data can be confronted to a model to check their compatibility."
"RDS",0,2,16,"2019-05-31 09:50","Provides functionality for carrying out estimation    with data collected using Respondent-Driven Sampling. This includes    Heckathorn's RDS-I and RDS-II estimators as well as Gile's Sequential    Sampling estimator. The package is part of the ""RDS Analyst"" suite of    packages for the analysis of respondent-driven sampling data.    See Gile and Handcock (2010) <doi:10.1111/j.1467-9531.2010.01223.x> and    Gile and Handcock (2015) <doi:10.1111/rssa.12091>."
"rcrimeanalysis",1,1,5,"2020-03-04 01:30","An implementation of functions for the analysis of crime incident or records  management system data. The package implements analysis algorithms scaled for city  or regional crime analysis units. The package provides functions for kernel density  estimation for crime heat maps, geocoding using the 'Google Maps' API, identification   of repeat crime incidents, spatio-temporal map comparison across time intervals,   time series analysis (forecasting and decomposition), detection of optimal parameters   for the identification of near repeat incidents, and near repeat analysis with crime   network linkage."
"RcmdrPlugin.RMTCJags",0,1,3,"2014-12-17 23:13","Mixed Treatment Comparison is a methodology to compare directly and/or indirectly health strategies (drugs, treatments, devices). This package provides an 'Rcmdr' plugin to perform Mixed Treatment Comparison for binary outcome using BUGS code from Bristol University (Lu and Ades)."
"RcextTools",0,1,2,"2017-06-06 17:38","Set of analytical procedures based on advanced data analysis in support of Brazil's public sector external control activity."
"rags2ridges",0,1,11,"2017-04-13 23:53","Proper L2-penalized maximum likelihood estimators for precision matrices and supporting functions to employ these estimators in a graphical modeling setting. For details see van Wieringen & Peeters (2016) <doi:10.1016/j.csda.2016.05.012> and associated publications."
"Radviz",2,4,7,"2020-03-29 16:20","An implementation of the radviz projection in R. It enables the visualization of    multidimensional data while maintaining the relation to the original dimensions.    This package provides functions to create and plot radviz projections, and a number of summary    plots that enable comparison and analysis. For reference see Ankerst *et al.* (1996)     (<http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.1811>) for original implementation,     see Di Caro *et al* (2012) (<http://link.springer.com/chapter/10.1007/978-3-642-13672-6_13>)     for the original method for dimensional anchor arrangements, see Demsar *et al.* (2007)     (<doi:10.1016/j.jbi.2007.03.010>) for the original Freeviz implementation."
"RaceID",1,4,13,"2020-05-20 16:30","Application of 'RaceID' allows inference of cell types and prediction of lineage trees by he StemID2 algorithm. Herman, J.S., Sagar, Grün D. (2018) <doi:10.1038/nmeth.4662>."
"R.temis",0,4,3,"2019-01-07 19:20","An integrated solution to perform    a series of text mining tasks such as importing and cleaning a corpus, and    analyses like terms and documents counts, lexical summary, terms    co-occurrences and documents similarity measures, graphs of terms,    correspondence analysis and hierarchical clustering. Corpora can be imported    from spreadsheet-like files, directories of raw text files,    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files."
"quickPlot",1,4,6,"2018-10-01 23:00","A high-level plotting system, built using 'grid' graphics, that is    optimized for speed and modularity. This has great utility for quick    visualizations when testing code, with the key benefit that visualizations    are updated independently of one another."
"QuACN",1,38,11,"2012-10-12 10:30","Quantitative Analysis of Complex Networks. This package offers a set of topological network measures to analyze complex Networks structurally."
"qgraph",0,38,46,"2019-01-24 15:20","Weighted network visualization and analysis, as well as Gaussian graphical model computation. See Epskamp et al. (2012) <doi:10.18637/jss.v048.i04>."
"qdap",0,4,28,"2020-04-20 08:10","Automates many of the tasks associated with quantitative discourse analysis of transcripts containing discourse              including frequency counts of sentence types, words, sentences, turns of talk, syllables and other assorted              analysis tasks. The package provides parsing tools for preparing transcript data. Many functions enable the user              to aggregate data by any number of grouping variables, providing analysis and seamless integration with other R              packages that undertake higher level analysis and visualization of text. This affords the user a more efficient              and targeted analysis. 'qdap' is designed for transcript analysis, however, many functions are applicable to other              areas of Text Mining/ Natural Language Processing."
"psSubpathway",1,5,2,"2019-06-21 09:20","A network-based systems biology tool for flexible identification of phenotype-specific subpathways in the cancer gene expression data   with multiple categories (such as multiple subtype or developmental stages of cancer). Subtype Set Enrichment Analysis (SubSEA) and Dynamic Changed   Subpathway Analysis (DCSA) are developed to flexible identify subtype specific and dynamic changed subpathways respectively. The operation modes   include extraction of subpathways from biological pathways, inference of subpathway activities in the context of gene expression data, identification   of subtype specific subpathways with SubSEA, identification of dynamic changed subpathways associated with the cancer developmental stage with DCSA,   and visualization of the activities of resulting subpathways by using box plots and heat maps. Its capabilities render the tool could find the specific  abnormal subpathways in the cancer dataset with multi-phenotype samples."
"provGraphR",0,5,1,"2020-01-20","Creates and manages a provenance graph corresponding to the     provenance created by the 'rdtLite' package, which    collects provenance from R scripts.  'rdtLite' is available on CRAN.    The provenance format is an extension of the     W3C PROV JSON format (<https://www.w3.org/Submission/2013/SUBM-prov-json-20130424/>).    The extended JSON provenance format is described     in <https://github.com/End-to-end-provenance/ExtendedProvJson>."
"propr",6,5,21,"2019-07-10 07:00","The bioinformatic evaluation of gene co-expression often begins with    correlation-based analyses. However, correlation lacks validity when    applied to relative data, including count data generated by next-generation    sequencing. This package implements several metrics for proportionality, including    phi [Lovell et al (2015) <doi:10.1371/journal.pcbi.1004075>] and    rho [Erb and Notredame (2016) <doi:10.1007/s12064-015-0220-8>]. This package also    implements several metrics for differential proportionality. Unlike correlation,    these measures give the same result for both relative and absolute data."
"ProjectManagement",0,4,7,"2020-05-07 18:50","Management problems of deterministic and stochastic projects. It obtains the duration of a project and the appropriate slack for each activity in a deterministic context. In addition it obtains a schedule of activities' time (Castro, Gómez & Tejada (2007) <doi:10.1016/j.orl.2007.01.003>). It also allows the management of resources. When the project is done, and the actual duration for each activity is known, then it can know how long the project is delayed and make a fair delivery of the delay between each activity (Bergantiños, Valencia-Toledo & Vidal-Puga (2018) <doi:10.1016/j.dam.2017.08.012>). In a stochastic context it can estimate the average duration of the project and plot the density of this duration, as well as, the density of the early and last times of the chosen activities. As in the deterministic case, it can make a distribution of the delay generated by observing the project already carried out."
"prioritizr",6,4,14,"2020-05-15 11:50","Conservation prioritization using integer    programming techniques. To solve large-scale problems, users    should install the 'gurobi' optimizer    (available from <http://www.gurobi.com/>)."
"PriorCD",1,4,1,"2019-02-24","Prioritize candidate cancer drugs for drug repositioning based on the random walk with restart algorithm in a drug-drug functional similarity network. 1) We firstly constructed a drug-drug functional similarity network by integrating pathway activity and drug activity derived from the NCI-60 cancer cell lines. 2) Secondly, we calculated drug repurposing score according to a set of approved therapeutic drugs of interested cancer based on the random walk with restart algorithm in the drug-drug functional similarity network. 3) Finally, the permutation test was used to calculate the statistical significance level for the drug repurposing score."
"poppr",3,4,34,"2020-02-25 11:10","Population genetic analyses for hierarchical analysis of partially    clonal populations built upon the architecture of the 'adegenet' package.     Originally described in Kamvar, Tabima, and Grünwald (2014)     <doi:10.7717/peerj.281> with version 2.0 described in Kamvar, Brooks, and     Grünwald (2015) <doi:10.3389/fgene.2015.00208>."
"poplite",1,2,6,"2019-02-18 21:00","Provides objects and accompanying methods which facilitates populating and querying SQLite databases.    "
"pop",0,2,1,"2016-06-07","Population dynamic models underpin a range of analyses and applications in ecology and epidemiology. The various approaches for analysing population dynamics models (MPMs, IPMs, ODEs, POMPs, PVA) each require the model to be defined in a different way. This makes it difficult to combine different modelling approaches and data types to solve a given problem. 'pop' aims to provide a flexible and easy to use common interface for constructing population dynamic models and enabling to them to be fitted and analysed in lots of different ways."
"pomdp",1,2,5,"2019-12-16 10:30","Provides the infrastructure to define and analyze the solutions of Partially Observable Markov Decision Processes (POMDP) models. The package includes pomdp-solve to solve POMDPs using a variety of exact and approximate value iteration algorithms. Smallwood and Sondik (1973) <doi:10.1287/opre.21.5.1071>."
"polymapR",2,2,11,"2019-12-12 23:00","Creation of linkage maps in polyploid species from marker dosage    scores of an F1 cross from two heterozygous parents. Currently works for outcrossing diploid, autotriploid, autotetraploid and autohexaploid species,     as well as segmental allotetraploids. Methods are described in a manuscript of Bourke et al. (2018) <doi:10.1093/bioinformatics/bty371>. Since version 1.1.0,    both discrete and probabilistic genotypes are acceptable input; for more details on the latter see Liao et al. (submitted, 2020)."
"pmd",1,2,3,"2019-08-22 17:50","Paired mass distance (PMD) analysis proposed in Yu, Olkowicz and Pawliszyn (2018) <doi:10.1016/j.aca.2018.10.062> for gas/liquid chromatography–mass spectrometry (GC/LC-MS) based non-targeted analysis. PMD analysis including GlobalStd algorithm and structure/reaction directed analysis. GlobalStd algorithm could found independent peaks in m/z-retention time profiles based on retention time hierarchical cluster analysis and frequency analysis of paired mass distances within retention time groups. Structure directed analysis could be used to find potential relationship among those independent peaks in different retention time groups based on frequency of paired mass distances. A GUI for PMD analysis is also included as a 'shiny' application."
"PLNmodels",6,2,6,"2020-06-16 17:20","The Poisson-lognormal model and variants can be used for     a variety of multivariate problems when count data are at play, including     principal component analysis for count data (Chiquet, Mariadassou and Robin,     2018 <doi:10.1214/18-AOAS1177>), discriminant analysis and     network inference (Chiquet, Mariadassou and Robin, 2018 <http://proceedings.mlr.press/v97/chiquet19a.html>).     Implements variational algorithms to fit such models accompanied with a set of     functions for visualization and diagnostic. "
"plinkQC",4,2,8,"2019-10-19 07:20","Genotyping arrays enable the direct measurement of an individuals    genotype at thousands of markers. 'plinkQC' facilitates genotype quality    control for genetic association studies as described by Anderson and    colleagues (2010) <doi:10.1038/nprot.2010.116>. It makes 'PLINK' basic    statistics (e.g. missing genotyping rates per individual, allele frequencies    per genetic marker) and relationship functions accessible from 'R' and    generates a per-individual and per-marker quality control report.    Individuals and markers that fail the quality control can subsequently be    removed to generate a new, clean dataset. Removal of individuals based on    relationship status is optimised to retain as many individuals as possible    in the study."
"PlackettLuce",1,2,7,"2019-09-05 12:20","Functions to prepare rankings data and fit the Plackett-Luce model    jointly attributed to Plackett (1975) <doi:10.2307/2346567> and Luce    (1959, ISBN:0486441369). The standard Plackett-Luce model is generalized    to accommodate ties of any order in the ranking. Partial rankings, in which    only a subset of items are ranked in each ranking, are also accommodated in    the implementation. Disconnected/weakly connected networks implied by the    rankings may be handled by adding pseudo-rankings with a hypothetical item.    Optionally, a multivariate normal prior may be set on the log-worth    parameters and ranker reliabilities may be incorporated as proposed by    Raman and Joachims (2014) <doi:10.1145/2623330.2623654>. Maximum a    posteriori estimation is used when priors are set. Methods are provided to    estimate standard errors or quasi-standard errors for inference as well as    to fit Plackett-Luce trees. See the package website or vignette for further    details."
"pkgnet",0,4,8,"2019-04-05 23:10","Tools from the domain of graph theory can be used to quantify the complexity             and vulnerability to failure of a software package. That is the guiding philosophy             of this package. 'pkgnet' provides tools to analyze the dependencies between functions             in an R package and between its imported packages.  See the pkgnet website for vignettes              and other supplementary information."
"pkggraph",1,44,3,"2017-09-01 23:51","Interactively explore various dependencies of a package(s) (on the Comprehensive R Archive Network Like repositories) and perform analysis using tidy philosophy. Most of the functions return a 'tibble' object (enhancement of 'dataframe') which can be used for further analysis. The package offers functions to produce 'network' and 'igraph' dependency graphs. The 'plot' method produces a static plot based on 'ggnetwork' and 'plotd3' function produces an interactive D3 plot based on 'networkD3'."
"piecewiseSEM",1,44,12,"2018-07-24 23:40","Implements piecewise structural equation modeling from a single    list of structural equations, with new methods for non-linear, latent, and    composite variables, standardized coefficients, query-based prediction and    indirect effects. See <http://jslefche.github.io/piecewiseSEM/> for more."
"phyloTop",0,44,7,"2017-10-10 18:49","Tools for calculating and viewing topological properties of phylogenetic trees."
"phylosignal",2,44,4,"2018-01-10 17:05","A collection of tools to explore the phylogenetic signal in univariate and multivariate data. The package provides functions to plot traits data against a phylogenetic tree, different measures and tests for the phylogenetic signal, methods to describe where the signal is located and a phylogenetic clustering method."
"phyloseqGraphTest",1,44,3,"2018-06-27 06:36","Provides functions for graph-based multiple-sample    testing and visualization of microbiome data, in particular data    stored in 'phyloseq' objects. The tests are based on those    described in Friedman and Rafsky (1979)    <http://www.jstor.org/stable/2958919>, and the tests are described    in more detail in Callahan et al. (2016)    <doi:10.12688/f1000research.8986.1>."
"phyloregion",2,44,2,"2020-03-30 18:20","Computational infrastructure for biogeography, community ecology,     and biodiversity conservation (Daru et al. 2020) <doi:10.1101/2020.02.12.945691>.     It is based on the methods described in Daru et al. (2020) <doi:10.1038/s41467-020-15921-6>.     The original conceptual work is described in Daru et al. (2017) <doi:10.1016/j.tree.2017.08.013>     on patterns and processes of biogeographical regionalization. Additionally, the package     contains fast and efficient functions to compute more standard conservation measures     such as phylogenetic diversity, phylogenetic endemism, evolutionary distinctiveness     and global endangerment, as well as compositional turnover (e.g., beta diversity). "
"phylopath",2,44,12,"2019-08-28 18:10","A comprehensive and easy to use R implementation of confirmatory    phylogenetic path analysis as described by Von Hardenberg and Gonzalez-Voyer    (2012) <doi:10.1111/j.1558-5646.2012.01790.x>."
"phangorn",5,44,49,"2019-03-23 23:30","Package contains methods for estimation of phylogenetic trees and    networks using Maximum Likelihood, Maximum Parsimony, distance methods and    Hadamard conjugation. Allows to compare trees, models selection and offers    visualizations for trees and split networks. "
"PGRdup",1,14,11,"2015-04-14 09:25","Provides functions to aid the identification of probable/possible    duplicates in Plant Genetic Resources (PGR) collections using    'passport databases' comprising of information records of each constituent    sample. These include methods for cleaning the data, creation of a    searchable Key Word in Context (KWIC) index of keywords associated with    sample records and the identification of nearly identical records with    similar information by fuzzy, phonetic and semantic matching of keywords."
"perturbR",1,14,4,"2018-07-27 19:10","The perturbR() function incrementally perturbs network edges (using the   rewireR function)and compares the resulting community detection solutions     from the rewired networks with the solution found for the original network.     These comparisons aid in understanding the stability of the     original solution. The package requires symmetric, weighted (specifically, count) matrices/networks."
"pencopulaCond",0,14,1,"2017-05-31","Estimating Non-Simplified Vine Copulas Using Penalized Splines."
"pcFactorStan",1,14,9,"2020-04-25 19:40","Provides convenience functions and pre-programmed    Stan models related to the paired comparison factor model. Its purpose    is to make fitting paired comparison data using Stan easy. This    package is described in Pritikin (2020) <doi:10.1016/j.heliyon.2020.e04821>."
"pcalg",2,14,35,"2020-08-16 18:00","Functions for causal structure  learning and causal inference using graphical models. The main algorithms  for causal structure learning are PC (for observational data without hidden  variables), FCI and RFCI (for observational data with hidden variables),  and GIES (for a mix of data from observational studies  (i.e. observational data) and data from experiments  involving interventions (i.e. interventional data) without hidden  variables). For causal inference the IDA algorithm, the Generalized  Backdoor Criterion (GBC), the Generalized Adjustment Criterion (GAC)  and some related functions are implemented. Functions for incorporating  background knowledge are provided."
"Patterns",2,1,3,"2019-08-24 06:20","A modeling tool dedicated to biological network modeling. It allows for single or joint modeling of, for instance, genes and proteins. It starts with the selection of the actors that will be the used in the reverse engineering upcoming step. An actor can be included in that selection based on its differential measurement (for instance gene expression or protein abundance) or on its time course profile. Wrappers for actors clustering functions and cluster analysis are provided. It also allows reverse engineering of biological networks taking into account the observed time course patterns of the actors. Many inference functions are provided and dedicated to get specific features for the inferred network such as sparsity, robust links, high confidence links or stable through resampling links. Some simulation and prediction tools are also available for cascade networks. Example of use with microarray or RNA-Seq data are provided."
"pathfindR",6,1,12,"2020-06-07 02:00","Enrichment analysis enables researchers to uncover mechanisms     underlying a phenotype. However, conventional methods for enrichment     analysis do not take into account protein-protein interaction information,     resulting in incomplete conclusions. pathfindR is a tool for enrichment     analysis utilizing active subnetworks. The main function identifies active     subnetworks in a protein-protein interaction network using a user-provided     list of genes and associated p values. It then performs enrichment analyses     on the identified subnetworks, identifying enriched terms (i.e. pathways or,     more broadly, gene sets) that possibly underlie the phenotype of interest.    pathfindR also offers functionalities to cluster the enriched terms and     identify representative terms in each cluster, to score the enriched terms     per sample and to visualize analysis results. The enrichment, clustering and     other methods implemented in pathfindR are described in detail in     Ulgen E, Ozisik O, Sezerman OU. 2019. pathfindR: An R Package for     Comprehensive Identification of Enriched Pathways in Omics Data Through     Active Subnetworks. Front. Genet. <doi:10.3389/fgene.2019.00858>."
"particles",1,1,2,"2018-02-26 13:11","Simulating particle movement in 2D space has many application. The    'particles' package implements a particle simulator based on the ideas     behind the 'd3-force' 'JavaScript' library. 'particles' implements all     forces defined in 'd3-force' as well as others such as vector fields, traps,     and attractors."
"PAFway",1,1,2,"2019-12-16 12:50","Finds pairs of functional annotations or gene ontology (GO)     terms that are enriched within a directed network (such as a gene    regulatory network).  This works with or without edge weights and    includes visualizations (both as a network where the functions are    nodes and as a heatmap).  PAFway is an acronym for Pairwise Associations of     Functional annotations in biological networks and pathWAYs. "
"PAFit",1,1,24,"2020-02-07 07:50","Statistical methods for estimating preferential attachment and node fitness generative mechanisms in temporal complex networks are provided. Thong Pham et al. (2015) <doi:10.1371/journal.pone.0137796>. Thong Pham et al. (2016) <doi:10.1038/srep32558>. Thong Pham et al. (2020) <doi:10.18637/jss.v092.i03>. "
"PAC",1,8,13,"2019-01-03 09:50","Implements partition-assisted clustering and multiple alignments of networks. It 1) utilizes partition-assisted clustering to find robust and accurate clusters and 2) discovers coherent relationships of clusters across multiple samples. It is particularly useful for analyzing single-cell data set. Please see Li et al. (2017) <doi:10.1371/journal.pcbi.1005875> for detail method description."
"OUwie",1,8,36,"2020-06-25 17:40","Estimates rates for continuous character evolution under Brownian motion and a new set of Ornstein-Uhlenbeck based Hansen models that allow both the strength of the pull and stochastic motion to vary across selective regimes. Beaulieu et al (2012) <doi:10.1111/j.1558-5646.2012.01619.x>."
"ORION",1,8,2,"2020-05-11 14:50","Functions to handle ordinal relations reflected within the feature space. Those function allow to search for ordinal relations in multi-class datasets. One can check whether proposed relations are reflected in a specific feature representation. Furthermore, it provides functions to filter, organize and further analyze those ordinal relations. "
"OpenRepGrid.ic",0,8,2,"2020-07-30 12:30","Shiny UI to identify cliques of related constructs in repertory grid data.     See Burr, King, & Heckmann (2020) <doi:10.1080/14780887.2020.1794088> for a description     of the interpretive clustering (IC) method."
"OpasnetUtils",0,8,4,"2015-06-03 14:30","Contains tools for open assessment and modelling in Opasnet, a wiki-based web site and workspace for societal decision making (see <http://en.opasnet.org/w/Main_Page> for more information). The core principle of the workspace is maximal openness and modularity. Variables are defined on public wiki pages using wiki inputs/tables, databases and R code. This package provides the functionality to download and use these variables. It also contains health impact assessment tools such as spatial methods for exposure modelling."
"OCNet",1,8,5,"2020-06-26 19:20","Generate and analyze Optimal Channel Networks (OCNs): 	oriented spanning trees reproducing all scaling features characteristic 	of real, natural river networks. As such, they can be used in a variety 	of numerical experiments in the fields of hydrology, ecology and 	epidemiology. See Carraro et al. (2020) <doi:10.1002/ece3.6479> 	for a presentation of the package; Rinaldo et al. (2014) 	<doi:10.1073/pnas.1322700111> for a theoretical overview on the OCN 	concept; Furrer and Sain (2010) <doi:10.18637/jss.v036.i10> for the 	construct used."
"nutriNetwork",0,8,1,"2019-04-26","Statistical tool for learning the structure of direct associations among variables for 			 continuous data, discrete data and mixed discrete-continuous data. The package is based			 on the copula graphical model in Behrouzi and Wit (2017) <doi:10.1111/rssc.12287>. "
"nlrx",6,8,5,"2019-12-03 13:20","Setup, run and analyze 'NetLogo' (<https://ccl.northwestern.edu/netlogo/>) model simulations in 'R'.    'nlrx' experiments use a similar structure as 'NetLogos' Behavior Space experiments.     However, 'nlrx' offers more flexibility and additional tools for running and analyzing complex simulation designs and sensitivity analyses.    The user defines all information that is needed in an intuitive framework, using class objects.    Experiments are submitted from 'R' to 'NetLogo' via 'XML' files that are dynamically written, based on specifications defined by the user.    By nesting model calls in future environments, large simulation design with many runs can be executed in parallel.    This also enables simulating 'NetLogo' experiments on remote high performance computing machines.    In order to use this package, 'Java' and 'NetLogo' (>= 5.3.1) need to be available on the executing system."
"nimble",0,8,21,"2019-12-18 22:10","A system for writing hierarchical statistical models largely    compatible with 'BUGS' and 'JAGS', writing nimbleFunctions to operate models and    do basic R-style math, and compiling both models and nimbleFunctions via custom-    generated C++. 'NIMBLE' includes default methods for MCMC, particle filtering,    Monte Carlo Expectation Maximization, and some other tools. The nimbleFunction    system makes it easy to do things like implement new MCMC samplers from R,    customize the assignment of samplers to different parts of a model from R, and    compile the new samplers automatically via C++ alongside the samplers 'NIMBLE'    provides. 'NIMBLE' extends the 'BUGS'/'JAGS' language by making it extensible:    New distributions and functions can be added, including as calls to external    compiled code. Although most people think of MCMC as the main goal of the    'BUGS'/'JAGS' language for writing models, one can use 'NIMBLE' for writing    arbitrary other kinds of model-generic algorithms as well. A full User Manual is    available at <https://r-nimble.org>."
"nhdplusTools",4,3,9,"2020-08-03 17:30","Tools for traversing and working with National Hydrography Dataset Plus (NHDPlus) data. All methods implemented in 'nhdplusTools' are available in the NHDPlus documentation available from the US Environmental Protection Agency <https://www.epa.gov/waterdata/basic-information>."
"NFP",1,3,3,"2016-11-21 12:01","An implementation of the network fingerprint framework that introduced   in paper ""Network fingerprint: a knowledge-based characterization of biomedical   networks"" (Cui, 2015) <doi:10.1038/srep13286>. This method worked by making   systematic comparisons to a set of well-studied ""basic networks"", measuring   both the functional and topological similarity.  A biological could be  characterized as a spectrum-like vector consisting of similarities to basic   networks. It shows great potential in biological network study."
"networktools",0,3,8,"2020-02-13 15:40","Includes assorted tools for network analysis. Bridge centrality; goldbricker; MDS, PCA, & eigenmodel network plotting."
"NetworkToolbox",0,2,15,"2020-01-11 01:50","Implements network analysis and graph theory measures used in neuroscience, cognitive science, and psychology. Methods include various filtering methods and approaches such as threshold, dependency (Kenett, Tumminello, Madi, Gur-Gershgoren, Mantegna, & Ben-Jacob, 2010 <doi:10.1371/journal.pone.0015032>), Information Filtering Networks (Barfuss, Massara, Di Matteo, & Aste, 2016 <doi:10.1103/PhysRevE.94.062306>), and Efficiency-Cost Optimization (Fallani, Latora, & Chavez, 2017 <doi:10.1371/journal.pcbi.1005305>). Brain methods include the recently developed Connectome Predictive Modeling (see references in package). Also implements several network measures including local network characteristics (e.g., centrality), community-level network characteristics (e.g., community centrality), global network characteristics (e.g., clustering coefficient), and various other measures associated with the reliability and reproducibility of network analysis. "
"networkTomography",0,1,2,"2012-12-13 06:49","networkTomography implements the methods developed and evaluated in    Blocker and Airoldi (2011) and Airoldi and Blocker (2012). These include the    authors' own dynamic multilevel model with calibration based upon a Gaussian    state-space model in addition to implementations of the methods of Tebaldi &    West (1998; Poisson-Gamma model with MCMC sampling), Zhang et al. (2002;    tomogravity), Cao et al. (2000; Gaussian model with mean-variance relation),    and Vardi (1996; method of moments). Data from the 1router network of Cao et    al. (2000), the Abilene network of Fang et al. (2007), and the CMU network    of Blocker and Airoldi (2011) are included for testing and reproducibility."
"NetworkSim",0,25,1,"2018-12-18","Functions to evaluate    pair-wise similarity of two networks especially on high-dimensional data."
"networkGen",0,25,2,"2017-07-05 10:43","A network Maze generator that creates different types of network mazes. "
"NetworkDistance",1,25,5,"2019-11-26 23:30","Network is a prevalent form of data structure in many fields. As an object of analysis, many distance or metric measures have been proposed to define the concept of similarity between two networks. We provide a number of distance measures for networks. See Jurman et al (2011) <doi:10.3233/978-1-60750-692-8-227> for an overview on spectral class of inter-graph distance measures."
"networkD3",0,25,14,"2017-02-16 07:55","Creates 'D3' 'JavaScript' network, tree, dendrogram, and Sankey    graphs from 'R'."
"NetworkChange",1,2,7,"2020-02-06 06:20","Network changepoint analysis for undirected network data. The package implements a hidden Markov network change point model (Park and Sohn 2020). Functions for break number detection using the approximate marginal likelihood and WAIC are also provided."
"nets",0,2,3,"2016-03-04 08:25","Sparse VAR estimation based on LASSO."
"netrankr",9,2,3,"2018-09-18 10:20","Implements methods for centrality related analyses of networks.     While the package includes the possibility to build more than 20 indices,     its main focus lies on index-free assessment of centrality via partial     rankings obtained by neighborhood-inclusion or positional dominance. These     partial rankings can be analyzed with different methods, including     probabilistic methods like computing expected node ranks and relative     rank probabilities (how likely is it that a node is more central than another?).    The methodology is described in depth in the vignettes and in    Schoch (2018) <doi:10.1016/j.socnet.2017.12.003>."
"NetOrigin",0,5,2,"2016-07-07 07:39","Performs network-based source estimation. Different approaches are available: effective distance median, recursive backtracking, and centrality-based source estimation. Additionally, we provide public transportation network data as well as methods for data preparation, source estimation performance analysis and visualization."
"NetMix",0,5,2,"2019-07-26 11:10","Variational EM estimation of mixed-membership stochastic blockmodel for networks,             incorporating node-level predictors of mixed-membership vectors, as well as              dyad-level predictors. For networks observed over time, the model defines a hidden             Markov process that allows the effects of node-level predictors to evolve in discrete,             historical periods. In addition, the package offers a variety of utilities for              exploring results of estimation, including tools for conducting posterior              predictive checks of goodness-of-fit and several plotting functions. The package              implements methods described in Olivella, Pratt and Imai (2019) “Dynamic Stochastic             Blockmodel Regression for Social Networks: Application to International Conflicts”,             available at <http://santiagoolivella.info/wp-content/uploads/2018/07/dSBM_Reg.pdf>."
"netjack",2,5,4,"2018-08-19 23:10","Tools for managing large sets of network data and performing whole network analysis.    This package is focused on the network based statistic jackknife method, and     implements a framework that can be extended to other network manipulations and analyses."
"netgwas",0,5,12,"2019-04-12 16:02","A multi-core R package that contains a set of tools based on copula graphical             models for accomplishing the three interrelated goals in genetics and genomics in an			 unified way: (1) linkage map construction, (2) constructing linkage disequilibrium			 networks, and (3) exploring high-dimensional genotype-phenotype network and genotype-			 phenotype-environment interactions networks. 			 The netgwas package can deal with biparental inbreeding and outbreeding species with			 any ploidy level, namely diploid (2 sets of chromosomes), triploid (3 sets of chromosomes),			 tetraploid (4 sets of chromosomes) and so on. We target on high-dimensional data where 			 number of variables p is considerably larger than number of sample sizes (p >> n). 			 The computations is memory-optimized using the sparse matrix output. The package is 			 implemented the recent methodological developments in Behrouzi and Wit (2017) <doi:10.1111/rssc.12287> 			 and Behrouzi and Wit (2017) <doi:10.1093/bioinformatics/bty777>.              NOTICE proper functionality of 'netgwas' requires that the 'RBGL' package is installed from 'bioconductor'; for installation instruction please refer to the 'RBGL' web page given below."
"netgsa",1,5,4,"2016-06-16 18:27","Carry out Network-based Gene Set Analysis by incorporating external information about interactions among genes, as well as novel interactions learned from data."
"netgen",0,5,5,"2016-01-22 10:26","Methods for the generation of a wide range of network geographies,    e.g., grid networks or clustered networks. Useful for the generation of    benchmarking instances for the investigation of, e.g., Vehicle-Routing-Problems    or Travelling Salesperson Problems."
"netdiffuseR",4,5,11,"2020-02-12 11:40","Empirical statistical analysis, visualization and simulation of    diffusion and contagion processes on networks. The package implements algorithms    for calculating network diffusion statistics such as transmission rate, hazard    rates, exposure models, network threshold levels, infectiousness (contagion),    and susceptibility. The package is inspired by work published in Valente,    et al., (2015) <doi:10.1016/j.socscimed.2015.10.001>; Valente (1995) <ISBN:    9781881303213>, Myers (2000) <doi:10.1086/303110>, Iyengar and others (2011)    <doi:10.1287/mksc.1100.0566>, Burt (1987) <doi:10.1086/228667>; among others."
"netdep",1,5,1,"2018-07-10","When network dependence is present, that is when social relations can engender dependence in the outcome of interest, treating such observations as independent results in invalid, anti-conservative statistical inference. We propose a test of independence among observations sampled from a single network <arXiv:1710.03296>."
"netcom",0,5,2,"2017-07-08 05:32","Functions to take two networks stored as matrices and return a node-level injection between them (bijection if the input networks are of the same size). The alignment is made by comparing diffusion kernels originating from each node in one network to those originating from each node in the other network. This creates a cost matrix where rows are nodes from one network and columns are nodes from the other network. Optimal node pairings are then found using the Hungarian algorithm."
"netCoin",2,5,13,"2020-02-04 00:10","Create interactive analytic networks. It joins the data analysis power of R to obtain coincidences, co-occurrences and correlations, and the visualization libraries of 'JavaScript' in one package. The methods are described in Escobar and Martinez-Uribe (2020) <doi:10.18637/jss.v093.i11>."
"netchain",1,5,2,"2019-01-16 19:40","In networks, treatments may spill over from the treated individual to his or her social contacts and outcomes may be contagious over time. Under this setting, causal inference on the collective outcome observed over all network is often of interest. We use chain graph models approximating the projection of the full longitudinal data onto the observed data to identify the causal effect of the intervention on the whole outcome. Justification of such approximation is demonstrated in Ogburn et al. (2018) <arXiv:1812.04990>."
"neo4r",0,5,2,"2019-01-28 00:20","A Modern and Flexible 'Neo4J' Driver, allowing you to query     data on a 'Neo4J' server and handle the results in R. It's modern in     the sense it provides a driver     that can be easily integrated in a data analysis workflow, especially by     providing an API working smoothly with other data analysis and graph     packages. It's flexible in the  way it returns the results, by     trying to stay as close as     possible to the way 'Neo4J' returns data. That way, you have the control     over the way you will compute the results. At the same time, the result     is not too complex, so that the ""heavy lifting"" of data wrangling is not     left to the user. "
"Neighboot",0,5,1,"2020-09-21","A bootstrap method for Respondent-Driven Sampling (RDS) that relies on the underlying structure of the RDS network to estimate uncertainty."
"neatmaps",0,5,7,"2019-03-15 18:11","Simplify the exploratory data analysis process for multiple network             data sets with the help of hierarchical clustering, consensus              clustering and heatmaps. Multiple network data consists of multiple             disjoint networks that have common variables (e.g. ego networks).              This package contains the necessary tools for exploring such data,             from the data pre-processing stage to the creation of dynamic             visualizations."
"neat",1,5,9,"2017-05-25 19:01","Includes functions and examples to compute NEAT, the Network Enrichment Analysis Test described in Signorelli et al. (2016, <doi:10.1186/s12859-016-1203-6>)."
"nat.templatebrains",0,5,6,"2017-04-23 00:24","Extends package 'nat' (NeuroAnatomy Toolbox) by providing objects    and functions for handling template brains."
"nat",2,5,17,"2020-02-07 07:30","NeuroAnatomy Toolbox (nat) enables analysis and visualisation of 3D    biological image data, especially traced neurons. Reads and writes 3D images    in NRRD and 'Amira' AmiraMesh formats and reads surfaces in 'Amira' hxsurf    format. Traced neurons can be imported from and written to SWC and 'Amira'    LineSet and SkeletonGraph formats. These data can then be visualised in 3D    via 'rgl', manipulated including applying calculated registrations, e.g.    using the 'CMTK' registration suite, and analysed. There is also a simple    representation for neurons that have been subjected to 3D skeletonisation    but not formally traced; this allows morphological comparison between    neurons including searches and clustering (via the 'nat.nblast' extension    package)."
"multivariance",0,1,8,"2019-06-18 14:20","Distance multivariance is a measure of dependence which can be used to detect     and quantify dependence of arbitrarily many random vectors. The necessary functions are    implemented in this packages and examples are given. It includes: distance multivariance,     distance multicorrelation, dependence structure detection, tests of independence and    copula versions of distance multivariance based on the Monte Carlo empirical transform.    Detailed references are given in the package description, as starting point for the     theoretic background we refer to:    B. Böttcher, Dependence and Dependence Structures: Estimation and Visualization Using     the Unifying Concept of Distance Multivariance. Open Statistics, Vol. 1, No. 1 (2020),     <doi:10.1515/stat-2020-0001>."
"multinma",11,1,1,"2020-06-30","Network meta-analysis and network meta-regression models for     aggregate data, individual patient data, and mixtures of both individual     and aggregate data using multilevel network meta-regression as described by    Phillippo et al. (2020) <doi:10.1111/rssa.12579>. Models are estimated in a    Bayesian framework using 'Stan'."
"multinets",0,1,4,"2018-07-07 20:00","Analyze multilevel networks as described in Lazega et al (2008)    <doi:10.1016/j.socnet.2008.02.001> and in Lazega and Snijders     (2016, ISBN:978-3-319-24520-1). The package was developed essentially as an     extension to 'igraph'."
"multilaterals",0,1,1,"2017-09-07","Computing transitive (and non-transitive) index numbers (Coelli et al., 2005 <doi:10.1007/b136381>) for cross-sections and panel data. For the calculation of transitive indexes, the EKS (Coelli et al., 2005 <doi:10.1007/b136381>; Rao et al., 2002 <doi:10.1007/978-1-4615-0851-9_4>) and Minimum spanning tree (Hill, 2004 <doi:10.1257/0002828043052178>) methods are implemented. Traditional fixed-base and chained indexes, and their growth rates, can also be derived using the Paasche, Laspeyres, Fisher and Tornqvist formulas. "
"MTA",2,1,4,"2018-05-14 14:49","Build multiscalar territorial analysis based on various contexts."
"mstknnclust",1,1,4,"2020-07-14 18:20","Implements the MST-kNN clustering algorithm which was proposed by Inostroza-Ponta, M. (2008) <https://trove.nla.gov.au/work/28729389?selectedversion=NBD44634158>.  "
"MRS",0,1,8,"2016-07-17 18:53","An implementation of the MRS algorithm for comparison across distributions,  as described in Jacopo Soriano, Li Ma (2016) <doi:10.1111/rssb.12180>.   The model is based on a nonparametric process taking the form of a Markov model   that transitions between a ""null"" and an ""alternative"" state   on a multi-resolution partition tree of the sample space.   MRS effectively detects and characterizes a variety of underlying differences.   These differences can be visualized using several plotting functions."
"MRReg",1,1,2,"2020-05-12 16:30","We provide the framework to analyze multiresolution partitions (e.g. country, provinces, subdistrict) where each individual data point belongs to only one partition in each layer (e.g. i belongs to subdistrict A, province P, and country Q).   We assume that a partition in a higher layer subsumes lower-layer partitions (e.g. a nation is at the 1st layer subsumes all provinces at the 2nd layer). Given N individuals that have a pair of real values (x,y) that generated from  independent variable X and dependent variable Y. Each individual i belongs to one partition per layer. Our goal is to find which partitions at which highest level that all individuals  in the these partitions share the same linear model Y=f(X) where f is a linear function. The framework deploys the Minimum Description Length principle (MDL) to infer solutions. The publication of this package is at Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok, and Suttipong Thajchayapong (2019) <arXiv:1907.05234>."
"MRFcov",3,1,3,"2018-11-26 07:40","Approximate node interaction parameters of Markov Random Fields     graphical networks. Models can incorporate additional covariates, allowing users to estimate    how interactions between nodes in the graph are predicted to change across    covariate gradients. The general methods implemented in this package are described     in Clark et al. (2018) <doi:10.1002/ecy.2221>."
"mppR",1,1,3,"2019-05-31 17:50","Analysis of experimental multi-parent populations to detect             regions of the genome (called quantitative trait loci, QTLs)             influencing phenotypic traits. The population must be composed of crosses             between a set of at least three parents (e.g. factorial design,             'diallel', or nested association mapping). The functions cover data             processing, QTL detection, and results visualization. The implemented             methodology is described by Garin, Wimmer, Mezmouk, Malosetti and             van Eeuwijk (2017) <doi:10.1007/s00122-017-2923-3>."
"motifcluster",1,1,2,"2020-05-18 16:50","    Tools for spectral clustering of weighted directed networks using motif    adjacency matrices. Methods perform well on large and sparse networks, and    random sampling methods for generating weighted directed networks are also    provided. Based on methodology detailed in Underwood, Elliott and Cucuringu    (2020) <arXiv:2004.01293>."
"missSBM",1,1,2,"2019-06-08 10:10","When a network is partially observed (here, NAs in the adjacency matrix rather than 1 or 0   due to missing information between node pairs), it is possible to account for the underlying process  that generates those NAs. 'missSBM' adjusts the popular stochastic block model from network data   sampled under various missing data conditions, as described in Tabouy, Barbillon and Chiquet (2019) <doi:10.1080/01621459.2018.1562934>."
"miniCRAN",3,1,14,"2020-07-13 13:00","Makes it possible to create an internally consistent    repository consisting of selected packages from CRAN-like repositories.    The user specifies a set of desired packages, and 'miniCRAN' recursively    reads the dependency tree for these packages, then downloads only this    subset. The user can then install packages from this repository directly,    rather than from CRAN.  This is useful in production settings, e.g. server    behind a firewall, or remote locations with slow (or zero) Internet access."
"metacoder",1,7,10,"2019-07-18 08:35","A set of tools for parsing, manipulating, and graphing data    classified by a hierarchy (e.g. a taxonomy)."
"MetaboLouise",1,7,1,"2018-11-30","Simulating dynamic (longitudinal, time-resolved) metabolomics data based on an underlying biological network. The network is initiating with certain concentrations and evolves over a simulated time period. Optionally external influxes (concentration drivers) can be added."
"Mercator",2,7,5,"2020-03-06 19:30","Defines the classes used to explore, cluster and  visualize distance matrices, especially those arising from binary data."
"mcvis",1,7,1,"2020-06-03","Visualize the relationship between linear regression variables and causes of multi-collinearity."
"MBNMAdose",1,7,6,"2020-03-02 18:00","Fits Bayesian dose-response model-based network meta-analysis (MBNMA)     that incorporate multiple doses within an agent by modelling different dose-response functions, as    described by Mawdsley et al. (2016) <doi:10.1002/psp4.12091>.     By modelling dose-response relationships this can connect networks of evidence that might    otherwise be disconnected, and can improve precision on treatment estimates. Several common     dose-response functions are provided; others may be added by the user. Various characteristics    and assumptions can be flexibly added to the models, such as shared class effects. The consistency     of direct and indirect evidence in the network can be assessed using unrelated mean effects models     and/or by node-splitting at the treatment level."
"mazeGen",0,7,4,"2017-03-18 01:09","A maze generator that creates the Elithorn Maze (HTML file) and the functions to calculate the associated maze parameters (i.e. Difficulty and Ability). "
"maxmatching",0,7,1,"2017-01-15","Computes the maximum matching for unweighted graph and maximum    matching for (un)weighted bipartite graph efficiently."
"mau",1,7,2,"2017-07-18 10:10","Provides functions for the creation, evaluation and test of decision models based in    Multi Attribute Utility Theory (MAUT). Can process and evaluate local risk aversion utilities    for a set of indexes, compute utilities and weights for the whole decision tree defining the    decision model and simulate weights employing Dirichlet distributions under addition constraints     in weights."
"matie",0,7,3,"2013-06-27 10:24","Uses a ratio of weighted distributions to estimate association between variables in a data set."
"markovchain",3,7,45,"2020-05-21 07:10","Functions and S4 methods to create and manage discrete time Markov    chains more easily. In addition functions to perform statistical (fitting    and drawing random variates) and probabilistic (analysis of their structural    proprieties) analysis are provided."
"malan",2,1,1,"2020-06-25","MAle Lineage ANalysis by simulating     genealogies backwards and imposing short tandem repeats (STR) mutations forwards.     Intended for forensic Y chromosomal STR (Y-STR) haplotype analyses.     Numerous analyses are possible, e.g. number of matches and meiotic distance to matches.     Refer to papers mentioned in citation(""malan"") (DOI's:     <doi:10.1371/journal.pgen.1007028>,     <doi:10.21105/joss.00684> and     <doi:10.1016/j.fsigen.2018.10.004>)."
"lvm4net",1,1,4,"2015-04-11 01:00","Latent variable models for network data using fast inferential    procedures. For more information please visit: <http://igollini.github.io/lvm4net/>."
"LoopDetectR",1,1,1,"2020-07-20","Detect feedback loops (cycles, circuits) between species (nodes) in ordinary differential equation (ODE) models. Feedback loops are paths from a node to itself without visiting any other node twice, and they have important regulatory functions. Loops are reported with their order of participating nodes and their length, and whether the loop is a positive or a negative feedback loop. An upper limit of the number of feedback loops limits runtime (which scales with feedback loop count). Model parametrizations and values of the modelled variables are accounted for. Computation uses the characteristics of the Jacobian matrix as described e.g. in Thomas and Kaufman (2002) <doi:10.1016/s1631-0691(02)01452-x>. Input can be the Jacobian matrix of the ODE model or the ODE function definition; in the latter case, the Jacobian matrix is determined using 'numDeriv'. Graph-based algorithms from 'igraph' are employed for path detection. "
"linkprediction",1,1,1,"2018-10-19","Implementations of most of the existing proximity-based methods of   link prediction in graphs. Among the 20 implemented methods are e.g.:  Adamic L. and Adar E. (2003) <doi:10.1016/S0378-8733(03)00009-1>,  Leicht E., Holme P., Newman M. (2006) <doi:10.1103/PhysRevE.73.026120>,  Zhou T. and Zhang Y (2009) <doi:10.1140/epjb/e2009-00335-8>, and  Fouss F., Pirotte A., Renders J., and Saerens M. (2007) <doi:10.1109/TKDE.2007.46>."
"lexRankr",1,1,7,"2017-12-13 15:02","An R implementation of the LexRank algorithm described by G. Erkan and D. R. Radev (2004) <doi:10.1613/jair.1523>."
"leiden",3,1,5,"2020-01-18 07:20","Implements the 'Python leidenalg' module to be called in R.    Enables clustering using the leiden algorithm for partition a graph into communities.    See the 'Python' repository for more details: <https://github.com/vtraag/leidenalg>    Traag et al (2018) From Louvain to Leiden: guaranteeing well-connected communities. <arXiv:1810.08473>."
"lconnect",0,19,1,"2019-04-05","Provides functions to upload vectorial data and derive landscape    connectivity metrics in habitat or matrix systems. Additionally, includes an     approach to assess individual patch contribution to the overall landscape     connectivity, enabling the prioritization of habitat patches. The computation    of landscape connectivity and patch importance are very useful in Landscape     Ecology research. The metrics available are: number of components, number of     links, size of the largest component, mean size of components, class coincidence    probability, landscape coincidence probability, characteristic path length,     expected cluster size, area-weighted flux and integral index of connectivity.    Pascual-Hortal, L., and Saura, S. (2006) <doi:10.1007/s10980-006-0013-z>    Urban, D., and Keitt, T. (2001) <doi:10.2307/2679983>    Laita, A., Kotiaho, J., Monkkonen, M. (2011) <doi:10.1007/s10980-011-9620-4>."
"LANDD",0,19,2,"2015-10-06 08:55","Using Liquid Association for Network Dynamics Detection."
"kknn",0,19,17,"2015-07-09 18:11","Weighted k-Nearest Neighbors for Classification, Regression and Clustering."
"keyplayer",0,1,2,"2015-10-16 01:07","Computes group centrality scores and identifies the most central group of players in a network."
"KDViz",0,1,2,"2019-01-11 16:30","Knowledge domain visualization using 'mpa' co-words method as the word clustering method and network graphs with 'D3.js' library as visualization tool."
"kangar00",0,1,4,"2019-02-10 04:43","Methods to extract information on pathways, genes and various single-nucleotid polymorphisms (SNPs) from online databases. It provides functions for data preparation and evaluation of genetic influence on a binary outcome using the logistic kernel machine test (LKMT). Three different kernel functions are offered to analyze genotype information in this variance component test: A linear kernel, a size-adjusted kernel and a network-based kernel (Friedrichs et al., 2017, <doi:10.1155/2017/6742763>)."
"Jdmbs",1,13,5,"2018-05-02 00:44","Option is a one of the financial derivatives and its pricing is an important problem in practice. The process of stock prices are represented as Geometric Brownian motion [Black (1973) <doi:10.1086/260062>] or jump diffusion processes [Kou (2002) <doi:10.1287/mnsc.48.8.1086.166>]. In this package, algorithms and visualizations are implemented by Monte Carlo method in order to calculate European option price for three equations by Geometric Brownian motion and jump diffusion processes and furthermore a model that presents jumps among companies affect each other."
"ITNr",0,13,4,"2018-10-11 23:30","Functions to clean and process international trade data into an international trade network (ITN) are provided. It then provides a set a functions to undertake analysis and plots of the ITN (extract the backbone, centrality, blockmodels, clustering). Examining the key players in the ITN and regional trade patterns. "
"IOHanalyzer",0,13,4,"2020-01-10 16:30","The data analysis module for the Iterative Optimization Heuristics    Profiler ('IOHprofiler'). This module provides statistical analysis methods for the     benchmark data generated by optimization heuristics, which can be visualized through a     web-based interface. The benchmark data is usually generated by the     experimentation module, called 'IOHexperimenter'. 'IOHanalyzer' also supports    the widely used 'COCO' (Comparing Continuous Optimisers) data format for benchmarking."
"intergraph",1,13,5,"2015-01-17 07:22","Functions implemented in this package allow to coerce (i.e.	convert) network data between classes provided by other R packages.	Currently supported classes are those defined in packages: network and	igraph."
"IntClust",1,3,2,"2016-03-31 17:38","Several integrative data methods in which information of objects from different data sources can be combined are included in the IntClust package. As a single data source is limited in its point of view, this provides more insight and the opportunity to investigate how the variables are interconnected. Clustering techniques are to be applied to the combined information. For now, only agglomerative hierarchical clustering is implemented. Further, differential gene expression and pathway analysis can be conducted on the clusters. Plotting functions are available to visualize and compare results of the different methods."
"inlmisc",0,3,24,"2020-07-23 11:30","A collection of functions for creating high-level graphics,    performing raster-based analysis, processing MODFLOW-based models,    selecting subsets using a genetic algorithm, creating interactive web maps,    accessing color palettes, etc. Used to support packages and scripts written    by researchers at the United States Geological Survey (USGS)    Idaho National Laboratory (INL) Project Office."
"influential",1,2,6,"2020-06-26 16:50","Contains functions for the classification and ranking of top candidate features, reconstruction of networks from    adjacency matrices and data frames, analysis of the topology of the network     and calculation of centrality measures, and identification of the most    influential nodes. Also, a function is provided for running SIRIR model, which     is the combination of leave-one-out cross validation technique and the conventional SIR model, on a network to unsupervisedly rank the true influence of vertices. Additionally, some functions have been provided for the assessment     of dependence and correlation of two network centrality measures as well as     the conditional probability of deviation from their corresponding means in opposite direction.    Fred Viole and David Nawrocki (2013, ISBN:1490523995).    Csardi G, Nepusz T (2006). ""The igraph software package for complex network research."" InterJournal, Complex Systems, 1695.    Adopted algorithms and sources are referenced in function document."
"influenceR",0,2,1,"2015-09-03","Provides functionality to compute various node centrality measures on networks.    Included are functions to compute betweenness centrality (by utilizing Madduri and Bader's    SNAP library), implementations of Burt's constraint and effective    network size (ENS) metrics, Borgatti's algorithm to identify key players, and Valente's    bridging metric. On Unix systems, the betweenness, Key Players, and    bridging implementations are parallelized with OpenMP, which may run    faster on systems which have OpenMP configured."
"imsig",0,31,2,"2018-07-05 12:40","Estimate the relative abundance of tissue-infiltrating immune subpopulations abundances using gene expression data. "
"immuneSIM",0,31,1,"2019-09-27","Simulate full B-cell and T-cell receptor repertoires using an in silico     recombination process that includes a wide variety of tunable parameters to introduce noise and biases.     Additional post-simulation modification functions allow the user to implant motifs or codon biases as     well as remodeling sequence similarity architecture. The output repertoires contain records of all     relevant repertoire dimensions and can be analyzed using provided repertoire analysis functions.    Preprint is available at bioRxiv (Weber et al., 2019 <doi:10.1101/759795>)."
"immcp",1,31,2,"2020-06-09 15:40","The pathway fingerprint is a method to indicate the profile of significant pathways being influenced by drugs, which may hint drug functions. Through the similarity of pathway fingerprints, the potential relationship between disease and prescription can be found. Ye (2012) <doi:10.1007/s13238-012-2011-z>."
"IMaGES",1,31,2,"2019-01-25 16:50","Functions for the implementation of Independent Multiple-sample  Greedy Equivalence Search (IMaGES), a causal inference algorithm for creating  aggregate graphs and structural equation modeling data for one or more  datasets. This package is useful for time series data with specific regions of  interest. This implementation is inspired by the paper ""Six problems  for causal inference from fMRI"" by Ramsey, Hanson, Hanson, Halchenko, Poldrack,  and Glymour (2010) <doi:10.1016/j.neuroimage.2009.08.065>. The IMaGES algorithm uses a modified BIC score to compute goodness  of fit of edge additions, subtractions, and turns across all datasets and  returns a representative graph, along with structural equation modeling data for  the global graph and individual datasets, means, and standard errors. Functions  for plotting the resulting graph(s) are provided. This package is built upon the  'pcalg' package."
"imager",2,31,14,"2020-02-17 21:10","Fast image processing for images in up to 4 dimensions (two spatial    dimensions, one time/depth dimension, one colour dimension). Provides most    traditional image processing tools (filtering, morphology, transformations,    etc.) as well as various functions for easily analysing image data using R. The    package wraps 'CImg', <http://cimg.eu>, a simple, modern C++ library for image    processing."
"IDSpatialStats",0,1,6,"2018-06-12 20:56","Implements various novel and standard    clustering statistics and other analyses useful for understanding the    spread of infectious disease."
"icosa",1,1,3,"2017-04-18 18:04","Employs triangular tessellation to refine icosahedra    defined in 3d space. The procedures can be set to provide a grid with a    custom resolution. Both the primary triangular and their inverted penta-    hexagonal grids are available for implementation. Additional functions    are provided to position points (latitude-longitude data) on the grids,    to allow 2D and 3D plotting, use raster data and shapefiles."
"iCellR",0,25,17,"2020-07-03 18:30","A toolkit that allows scientists to work with data from single cell sequencing technologies such as scRNA-seq, scVDJ-seq and CITE-Seq. Single (i) Cell R package ('iCellR') provides unprecedented flexibility at every step of the analysis pipeline, including normalization, clustering, dimensionality reduction, imputation, visualization, and so on. Users can design both unsupervised and supervised models to best suit their research. In addition, the toolkit provides 2D and 3D interactive visualizations, differential expression analysis, filters based on cells, genes and clusters, data merging, normalizing for dropouts, data imputation methods, correcting for batch differences, pathway analysis, tools to find marker genes for clusters and conditions, predict cell types and pseudotime analysis. See Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.05.05.078550>  and Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.03.31.019109> for more details."
"ICDS",1,25,1,"2019-01-25","Identify Cancer Dysfunctional Sub-pathway by integrating gene expression, DNA methylation and copy number variation, and pathway topological information. 1)We firstly calculate the gene risk scores by integrating three kinds of data: DNA methylation, copy number variation, and gene expression.  2)Secondly, we perform a greedy search algorithm to identify the key dysfunctional sub-pathways within the pathways for which the discriminative scores were locally maximal. 3)Finally, the permutation test was used to calculate statistical significance level for these key dysfunctional sub-pathways."
"iCARH",1,25,6,"2020-06-16 16:10","Implements the integrative conditional autoregressive horseshoe model     discussed in Jendoubi, T., Ebbels, T.M. Integrative analysis of time course metabolic data and biomarker discovery.     BMC Bioinformatics 21, 11 (2020) <doi:10.1186/s12859-019-3333-0>.    The model consists in three levels: Metabolic pathways level modeling interdependencies between    variables via a conditional auto-regressive (CAR) component, integrative analysis level to identify    potential associations between heterogeneous omic variables via a Horseshoe prior and experimental    design level to capture experimental design conditions through a mixed-effects model.    The package also provides functions to simulate data from the model, construct pathway matrices,    post process and plot model parameters."
"huge",1,25,26,"2019-10-28 16:10","Provides a general framework for        high-dimensional undirected graph estimation. It integrates        data preprocessing, neighborhood screening, graph estimation,        and model selection techniques into a pipeline. In        preprocessing stage, the nonparanormal(npn) transformation is        applied to help relax the normality assumption. In the graph        estimation stage, the graph structure is estimated by        Meinshausen-Buhlmann graph estimation or the graphical lasso,        and both methods can be further accelerated by the lossy        screening rule preselecting the neighborhood of each variable        by correlation thresholding. We target on high-dimensional data        analysis usually d >> n, and the computation is        memory-optimized using the sparse matrix output. We also        provide a computationally efficient approach, correlation        thresholding graph estimation. Three        regularization/thresholding parameter selection methods are        included in this package: (1)stability approach for        regularization selection (2) rotation information criterion (3)        extended Bayesian information criterion which is only available        for the graphical lasso."
"HTSSIP",8,9,8,"2018-05-15 09:23","Functions for analyzing high throughput sequencing     stable isotope probing (HTS-SIP) data.    Analyses include high resolution stable isotope probing (HR-SIP),    multi-window high resolution stable isotope probing (MW-HR-SIP),     and quantitative stable isotope probing (q-SIP). "
"HLSM",0,9,9,"2018-05-27 00:01","Implements Hierarchical Latent Space Network Model (HLSM) for ensemble of networks as described in Sweet, Thomas & Junker (2013). <doi:10.3102/1076998612458702>. "
"highcharter",0,9,7,"2019-01-15 17:50","A wrapper for the 'Highcharts' library including    shortcut functions to plot R objects. 'Highcharts'    <http://www.highcharts.com/> is a charting library offering    numerous chart types with a simple configuration syntax."
"haploReconstruct",0,16,1,"2016-10-10","Reconstruction of founder haplotype blocks from time series data."
"gromovlab",0,16,2,"2015-05-25 15:20","Computing Gromov-Hausdorff type l^p distances for labeled metric spaces. These distances were introduced in V.Liebscher, Gromov meets Phylogenetics - new Animals for the Zoo of Metrics on Tree Space. preprint  arXiv:1504.05795,  for phylogenetic trees but may apply to much more situations. "
"gRim",1,16,20,"2020-06-26 18:40","Provides the following types of models: Models for contingency    tables (i.e. log-linear models) Graphical Gaussian models for multivariate    normal data (i.e. covariance selection models) Mixed interaction models.    Documentation about 'gRim' is provided by vignettes included in this    package and the book by Højsgaard, Edwards and Lauritzen (2012,    <doi:10.1007/978-1-4614-2299-0>); see 'citation(""gRim"")' for details."
"gRbase",2,16,69,"2020-06-14 16:50","The 'gRbase' package provides graphical modelling features    used by e.g. the packages 'gRain', 'gRim' and 'gRc'. 'gRbase' implements    graph algorithms including (i) maximum cardinality search (for marked    and unmarked graphs).    (ii) moralization, (iii) triangulation, (iv) creation of junction tree.    'gRbase' facilitates array operations,    'gRbase' implements functions for testing for conditional independence.    'gRbase' illustrates how hierarchical log-linear models may be    implemented and describes concept of graphical meta    data.     The facilities of the package are documented in the book by Højsgaard,    Edwards and Lauritzen (2012,    <doi:10.1007/978-1-4614-2299-0>) and in the paper by     Dethlefsen and Højsgaard, (2005, <doi:10.18637/jss.v014.i17>).    Please see 'citation(""gRbase"")' for citation details.     NOTICE  'gRbase' requires that the packages graph,    'Rgraphviz' and 'RBGL' are installed from 'bioconductor'; for    installation instructions please refer to the web page given below."
"graphTweets",0,3,12,"2019-07-10 16:50","Allows building an edge table from data frame of tweets,   also provides function to build nodes and another create a temporal graph."
"graphsim",2,3,3,"2020-07-02 09:40","Functions to develop simulated continuous data (e.g., gene expression) from a sigma covariance matrix derived from a graph structure in 'igraph' objects. Intended to extend 'mvtnorm' to take 'igraph'  structures rather than sigma matrices as input. This allows the use of simulated data that correctly accounts for pathway relationships and correlations. This allows the use of simulated data that correctly accounts for pathway relationships and correlations. Here we present a versatile statistical framework to simulate  correlated gene expression data from biological pathways, by sampling from a multivariate normal distribution derived from a graph structure. This package allows the simulation of biological pathways from a graph structure based on a statistical model of gene expression. For example methods to infer biological pathways and gene regulatory networks from gene expression data can be tested on simulated datasets using this framework. This also allows for pathway structures to be considered as a confounding variable when simulating gene expression data to test the performance of genomic analyses."
"graphlayouts",0,3,5,"2020-03-09 16:30","Several new layout algorithms to visualize networks are provided which are not part of 'igraph'.     Most are based on the concept of stress majorization by Gansner et al. (2004) <doi:10.1007/978-3-540-31843-9_25>.     Some more specific algorithms allow to emphasize hidden group structures in networks or focus on specific nodes."
"graphicalVAR",0,3,8,"2017-03-27 16:29","Estimates within and between time point interactions in experience sampling data, using the Graphical vector autoregression model in combination with regularization. See also Epskamp, Waldorp, Mottus & Borsboom (2018) <doi:10.1080/00273171.2018.1454823>."
"graphicalExtremes",0,7,1,"2019-11-08","Statistical methodology for sparse multivariate extreme value models. Methods are             provided for exact simulation and statistical inference for multivariate Pareto distributions             on graphical structures as described in the paper 'Graphical Models for Extremes' by             Engelke and Hitz (2018) <arXiv:1812.01734>. "
"graph4lg",4,7,6,"2020-07-22 09:10","Build graphs for landscape genetics analysis. This set of 	functions can be used to import and convert spatial and genetic data 	initially in different formats, import landscape graphs created with 	'GRAPHAB' software (Foltete et al., 2012) <doi:10.1016/j.envsoft.2012.07.002>, 	make diagnosis plots of isolation by distance relationships in order to 	choose how to build genetic graphs, create graphs with a large range of 	pruning methods, weight their links with several genetic distances, plot 	and analyse graphs,	compare them with other graphs. It uses functions from 	other packages such as 'adegenet' 	(Jombart, 2008) <doi:10.1093/bioinformatics/btn129> and 'igraph' (Csardi	et Nepusz, 2006) <https://igraph.org/>. It also implements methods 	commonly used in landscape genetics to create graphs, described by Dyer et 	Nason (2004) <doi:10.1111/j.1365-294X.2004.02177.x> and Greenbaum et 	Fefferman (2017) <doi:10.1111/mec.14059>, and to analyse distance data 	(van Strien et al., 2015) <doi:10.1038/hdy.2014.62>."
"grainscape",2,7,3,"2019-12-07 00:50","Given a landscape resistance surface, creates minimum planar graph    (Fall et al. (2007) <doi:10.1007/s10021-007-9038-7>) and grains of connectivity    (Galpern et al. (2012) <doi:10.1111/j.1365-294X.2012.05677.x>) models that can be    used to calculate effective distances for landscape connectivity at multiple scales.    Documentation is provided by several vignettes, and a paper    (Chubaty, Galpern & Doctolero (2020) <doi:10.1111/2041-210X.13350>)."
"gRain",1,7,46,"2020-06-14 18:10","Probability propagation in graphical independence networks, also    known as Bayesian networks or probabilistic expert systems. Documentation    of the package is provided in vignettes included in the package and in    the paper by Højsgaard (2012, <doi:10.18637/jss.v046.i10>).    See 'citation(""gRain"")' for details. "
"GMPro",0,44,1,"2020-06-25","Functions for graph matching via nodes' degree profiles are provided in this package. The models we can handle include Erdos-Renyi random graphs and stochastic block models(SBM). More details are in the reference paper: Yaofang Hu, Wanjie Wang and Yi Yu (2020) <arXiv:2006.03284>."
"gmat",0,44,4,"2019-09-13 12:30","Simulation of correlation matrices possibly constrained by a given undirected or acyclic directed graph. In particular, the package provides functions that implement the simulation methods described in Córdoba et al. (2018) <doi:10.1007/978-3-030-03493-1_13> and Córdoba et al. (2020) <doi:10.1016/j.ijar.2020.07.007>."
"gimme",1,44,24,"2020-09-03 22:02","Automated identification and estimation of group- and    individual-level relations in time series data from within a structural    equation modeling framework."
"ggraph",4,44,7,"2020-03-17 23:00","The grammar of graphics as implemented in ggplot2 is a poor fit for    graph and network visualizations due to its reliance on tabular data input.    ggraph is an extension of the ggplot2 API tailored to graph visualizations    and provides the same flexible approach to building up plots layer by layer."
"ggnetwork",1,8,2,"2016-03-25 01:15","Geometries to plot network objects with 'ggplot2'."
"ggm",0,13,21,"2015-01-21 12:47","Tools for marginalization, conditioning and fitting by maximum likelihood."
"ggenealogy",1,1,6,"2019-05-28 18:52","Methods for searching through genealogical data and displaying the results. Plotting algorithms assist with data exploration and publication-quality image generation. Includes interactive genealogy visualization tools. Provides parsing and calculation methods for variables in descendant branches of interest. Uses the Grammar of Graphics."
"ggdag",3,1,4,"2019-12-06 06:50","Tidy, analyze, and plot directed acyclic graphs    (DAGs). 'ggdag' is built on top of 'dagitty', an R package that uses    the 'DAGitty' web tool (<http://dagitty.net>) for creating and    analyzing DAGs. 'ggdag' makes it easy to tidy and plot 'dagitty'    objects using 'ggplot2' and 'ggraph', as well as common analytic and    graphical functions, such as determining adjustment sets and node    relationships."
"geonetwork",0,12,1,"2019-04-05","Provides classes and methods for handling networks or   graphs whose nodes are geographical (i.e. locations in the globe).  The functionality includes the creation of objects of class geonetwork  as a graph with node coordinates, the computation of network measures,  the support of spatial operations (projection to different Coordinate  Reference Systems, handling of bounding boxes, etc.) and the plotting of  the geonetwork object combined with supplementary cartography for spatial   representation."
"geneNetBP",1,12,3,"2016-05-14 23:21","Belief propagation methods in genotype-phenotype networks (Conditional Gaussian and Discrete Bayesian Networks) to propagate phenotypic evidence through the network."
"GeneClusterNet",0,12,1,"2017-01-13","Functions for clustering time-course gene expression and reconstructing of gene regulatory network based on Dynamic Bayesian Network."
"genBaRcode",2,12,5,"2019-10-25 17:10","Provides the necessary functions to identify and extract a selection of already available barcode constructs (Cornils, K. et al. (2014) <doi:10.1093/nar/gku081>) and freely choosable barcode designs from next generation sequence (NGS) data. Furthermore, it offers the possibility to account for sequence errors, the calculation of barcode similarities and provides a variety of visualisation tools (Thielecke, L. et al. (2017) <doi:10.1038/srep43249>)."
"gemtc",0,12,22,"2016-03-01 17:08","Network meta-analyses (mixed treatment comparisons) in the Bayesian    framework using JAGS. Includes methods to assess heterogeneity and    inconsistency, and a number of standard visualizations.    van Valkenhoef et al. (2012) <doi:10.1002/jrsm.1054>;    van Valkenhoef et al. (2015) <doi:10.1002/jrsm.1167>."
"gDefrag",0,12,3,"2019-11-18 17:40","Provides a set of tools to help the de-fragmentation process. It works by prioritizing the different sections of linear infrastructures (e.g. roads, power-lines) to increase the available amount of a given resource.   "
"gamCopula",0,12,6,"2019-12-04 22:30","Implementation of various inference and simulation tools to    apply generalized additive models to bivariate dependence structures and    non-simplified vine copulas."
"fusedest",0,12,3,"2019-09-21 01:20","Provides methods fusedest_normal() and fusedest_logit() for carrying out block splitting algorithms for fused penalty estimation. For details, please see Tso-Jung Yen (2019) <doi.10618600.2019.1660178>."
"fssemR",1,12,4,"2019-06-02 16:40","An optimizer of Fused-Sparse Structural Equation Models, which is  the state of the art jointly fused sparse maximum likelihood function  for structural equation models proposed by Xin Zhou and Xiaodong Cai (2018  <doi:10.1101/466623>)."
"FrF2",0,12,71,"2020-04-23 18:40","Regular and non-regular Fractional Factorial 2-level designs         can be created. Furthermore, analysis tools for Fractional        Factorial designs with 2-level factors are offered (main        effects and interaction plots for all factors simultaneously,        cube plot for looking at the simultaneous effects of three        factors, full or half normal plot, alias structure in a more        readable format than with the built-in function alias). "
"forestRK",1,1,1,"2019-07-19","Provides functions that calculates common types of splitting    criteria used in random forests for classification problems, as well as     functions that make predictions based on a single tree or a Forest-R.K. model;     the package also provides functions to generate importance plot for a     Forest-R.K. model, as well as the 2D multidimensional-scaling plot of     data points that are colour coded by their predicted class types by the     Forest-R.K. model. This package is based on:     Bernard, S., Heutte, L., Adam, S., (2008, ISBN:978-3-540-85983-3)     ""Forest-R.K.: A New Random Forest Induction Method"",     Fourth International Conference on Intelligent Computing,    September 2008, Shanghai, China, pp.430-437."
"ForestGapR",0,1,2,"2018-12-30 18:00","Set of tools for detecting and analyzing Airborne Laser Scanning-derived Tropical Forest Canopy Gaps."
"foodingraph",1,1,1,"2019-10-06","Displays a weighted undirected food graph from an adjacency matrix.    Can perform confidence-interval bootstrap inference with mutual information    or maximal information coefficient.    Based on my Master 1 internship at the Bordeaux Population Health center.    References : Reshef et al. (2011) <doi:10.1126/science.1205438>,    Meyer et al. (2008) <doi:10.1186/1471-2105-9-461>,    Liu et al. (2016) <doi:10.1371/journal.pone.0158247>."
"FindIt",0,1,12,"2017-08-14 20:33","The heterogeneous treatment effect estimation procedure         proposed by Imai and Ratkovic (2013)<doi:10.1214/12-AOAS593>.          The proposed method is applicable, for        example, when selecting a small number of most (or least)        efficacious treatments from a large number of alternative        treatments as well as when identifying subsets of the        population who benefit (or are harmed by) a treatment of        interest. The method adapts the Support Vector Machine        classifier by placing separate LASSO constraints over the        pre-treatment parameters and causal heterogeneity parameters of        interest. This allows for the qualitative distinction between        causal and other parameters, thereby making the variable        selection suitable for the exploration of causal heterogeneity. 	        The package also contains a class of functions, CausalANOVA,         which estimates the average marginal interaction effects (AMIEs)        by a regularized ANOVA as proposed by Egami and Imai (2019)<doi:10.1080/01621459.2018.1476246>.         It contains a variety of regularization techniques to facilitate 	analysis of large factorial experiments. "
"FedData",0,1,30,"2019-01-11 18:50","Functions to automate downloading geospatial data available from    several federated data sources (mainly sources maintained by the US Federal    government). Currently, the package enables extraction from seven datasets:    The National Elevation Dataset digital elevation models (1 and 1/3 arc-second;    USGS); The National Hydrography Dataset (USGS); The Soil Survey Geographic    (SSURGO) database from the National Cooperative Soil Survey (NCSS), which is    led by the Natural Resources Conservation Service (NRCS) under the USDA; the    Global Historical Climatology Network (GHCN), coordinated by National Climatic    Data Center at NOAA; the Daymet gridded estimates of daily weather parameters     for North America, version 3, available from the Oak Ridge National Laboratory's    Distributed Active Archive Center (DAAC); the International Tree Ring Data Bank;     and the National Land Cover Database (NLCD)."
"FCMapper",0,1,2,"2015-02-06 01:00","Provides several functions to create and manipulate fuzzy    cognitive maps. It is based on 'FCMapper' for Excel, distributed at <http://    www.fcmappers.net/joomla/>, developed by Michael Bachhofer and Martin Wildenberg.    Maps are inputted as adjacency matrices. Attributes of the maps and the    equilibrium values of the concepts (including with user-defined constrained    values) can be calculated. The maps can be graphed with a function that calls    'igraph'. Multiple maps with shared concepts can be aggregated."
"fbRanks",2,1,3,"2013-03-31 22:37","This package uses time dependent Poisson regression and a record of goals scored in matches to rank teams via estimated attack and defense strengths.  The statistical model is based on Dixon and Coles (1997) Modeling Association Football Scores and Inefficiencies in the Football Betting Market, Applied Statistics, Volume 46, Issue 2, 265-280.  The package has a some webscrapers to assist in the development and updating of a match database.  If the match database contains unconnected clusters (i.e. sets of teams that have only played each other and not played teams from other sets), each cluster is ranked separately relative to the median team strength in the cluster.  The package contains functions for predicting and simulating tournaments and leagues from estimated models. The package allows fitting via the glm(), speedglm(), and glmnet() functions.  The latter allows fast and efficient fitting of very large numbers of teams.  The fitting algorithm will analyze the match data and determine which teams form a cluster (a set of teams where there is a path of matches connecting every team) and fit each cluster separately."
"fastnet",0,1,6,"2018-07-05 08:50","We present an implementation of the algorithms required to simulate large-scale social networks and retrieve their most relevant metrics."
"fakemake",2,1,14,"2020-09-24 10:50","Use R as a minimal build system. This might come in    handy if you are developing R packages and can not use a proper build    system. Stay away if you can (use a proper build system)."
"factorEx",0,1,2,"2019-10-01 16:40","Provides design-based and model-based estimators for the population average marginal component effects in general factorial experiments, including conjoint analysis. The package also implements a series of recommendations offered in de la Cuesta, Egami, and Imai (2019+), and Egami and Imai (2019) <doi:10.1080/01621459.2018.1476246>."
"exp2flux",0,1,1,"2016-10-11","For a given metabolic model with well formed Gene-Protein-Reaction (GPR) associations and an expressionSet with their associated gene expression values, this package converts gene expression values to the FBA boundaries for each reaction based in the boolean rules described in its associated GPR."
"evolqg",0,1,12,"2019-04-08 20:42","Provides functions for covariance matrix comparisons, estimation    of repeatabilities in measurements and matrices, and general evolutionary    quantitative genetics tools."
"EstimateGroupNetwork",0,1,2,"2017-03-20 09:38","Can be used to simultaneously estimate networks (Gaussian Graphical Models) in data from different groups or classes via Joint Graphical Lasso. Tuning parameters are selected via information criteria (AIC / BIC / extended BIC) or cross validation."
"ess",0,1,3,"2018-03-02 17:56","An implementation of the ESS algorithm following Amol Deshpande, Minos Garofalakis,	     Michael I Jordan (2013) <arXiv:1301.2267>. The ESS algorithm	     is used for model selection in decomposable graphical models."
"erah",1,1,8,"2018-07-11 07:00","Automated compound deconvolution, alignment across samples, and identification of metabolites by spectral library matching in Gas Chromatography - Mass spectrometry (GC-MS) untargeted metabolomics. Outputs a table with compound names, matching scores and the integrated area of the compound for each sample. Package implementation is described in Domingo-Almenara et al. (2016) <doi:10.1021/acs.analchem.6b02927>."
"equSA",0,2,8,"2018-01-20 16:44","Provides an equivalent measure of partial correlation coefficients for high-dimensional Gaussian Graphical Models to learn and visualize the underlying relationships between variables from single or multiple datasets. You can refer to Liang, F., Song, Q. and Qiu, P. (2015) <doi:10.1080/01621459.2015.1012391> for more detail. Based on this method, the package also provides the method for constructing networks for Next Generation Sequencing Data, jointly estimating multiple Gaussian Graphical Models, constructing single graphical model for heterogeneous dataset, inferring graphical models from high-dimensional missing data and estimating moral graph for Bayesian network."
"EpiILMCT",0,2,7,"2018-03-22 06:21","Provides tools for simulating from continuous-time individual level models of disease transmission, and carrying out infectious disease data analyses with the same models. The epidemic models considered are distance-based and/or contact network-based models within Susceptible-Infectious-Removed (SIR) or Susceptible-Infectious-Notified-Removed (SINR) compartmental frameworks. An overview of the implemented continuous-time individual level models for epidemics is given by Almutiry and Deardon (2019) <doi:10.1515/ijb-2017-0092>."
"epicontacts",2,2,3,"2017-05-16 15:55","A collection of tools for representing epidemiological contact data, composed of case line lists and contacts between cases. Also contains procedures for data handling, interactive graphics, and statistics."
"EmbedSOM",1,1,3,"2019-08-19 11:50","Provides a smooth mapping of multidimensional points into    low-dimensional space defined by a self-organizing map. Designed to work    with 'FlowSOM' and flow-cytometry use-cases. See Kratochvil et al. (2019)    <doi:10.1101/496869>."
"emba",1,1,6,"2020-07-08 07:10","Analysis and visualization of an ensemble of boolean models for   biomarker discovery in cancer cell networks. The package allows to easily   load the simulation data results of the DrugLogics software pipeline which predicts synergistic drug   combinations in cancer cell lines (developed by the DrugLogics research group   in NTNU). It has generic functions that can be used to split a boolean model   dataset to model groups with regards to the models predictive performance (number of true   positive predictions/Matthews correlation coefficient score) or synergy prediction based on a given set   of gold standard synergies and find the average activity difference per network   node between all model group pairs. Thus, given user-specific thresholds,  important nodes (biomarkers) can be accessed in the sense that they make the   models predict specific synergies (synergy biomarkers) or have better   performance in general (performance biomarkers). Lastly, if the   boolean models have a specific equation form and differ only in their link operator,   link operator biomarkers can also be found."
"einet",1,1,1,"2020-04-23","Methods and utilities for causal emergence.    Used to explore and compute various information theory metrics for networks, such as effective information, effectiveness and causal emergence."
"egor",2,1,7,"2020-03-03 01:20","Tools for importing, analyzing and visualizing ego-centered    network data. Supports several data formats, including the export formats of    'EgoNet', 'EgoWeb 2.0' and 'openeddi'. An interactive (shiny) app for the    intuitive visualization of ego-centered networks is provided. Also included    are procedures for creating and visualizing Clustered Graphs     (Lerner 2008 <doi:10.1109/PACIFICVIS.2008.4475458>)."
"EGAnet",2,1,8,"2020-03-05 18:50","An implementation of the Exploratory Graph Analysis (EGA) framework for dimensionality assessment. EGA is part of a new area called network psychometrics that focuses on the estimation of undirected network models in psychological datasets. EGA estimates the number of dimensions or factors using graphical lasso or Triangulated Maximally Filtered Graph (TMFG) and a weighted network community analysis. A bootstrap method for verifying the stability of the estimation is also available. The fit of the structure suggested by EGA can be verified using confirmatory factor analysis and a direct way to convert the EGA structure to a confirmatory factor model is also implemented. Documentation and examples are available. Golino, H. F., & Epskamp, S. (2017) <doi:10.1371/journal.pone.0174035>. Golino, H. F., & Demetriou, A. (2017) <doi:10.1016/j.intell.2017.02.007> Golino, H., Shi, D., Garrido, L. E., Christensen, A. P., Nieto, M. D., Sadana, R., & Thiyagarajan, J. A. (2018) <doi:10.31234/osf.io/gzcre>. Christensen, A. P. & Golino, H.F. (2019)  <doi:10.31234/osf.io/9deay>."
"EDOIF",1,1,1,"2019-12-02","A non-parametric framework based on  estimation statistics principle. Its main purpose is to  infer orders of empirical distributions from different categories based on a probability of finding a value in one distribution that is greater than an expectation of another distribution. Given a set of ordered-pair of real-category values the framework is capable of 1) inferring orders of  domination  of  categories  and  representing  orders  in  the form of a graph; 2) estimating  magnitude  of  difference  between  a  pair  of categories in forms of mean-difference confidence intervals; and 3) visualizing  domination  orders  and  magnitudes  of  difference of categories. The publication of this package is at Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok, and Suttipong Thajchayapong (2019) <arXiv:1911.06723>."
"edgebundleR",0,1,2,"2015-11-16 08:20","Generates interactive circle plots with the nodes around the    circumference and linkages between the connected nodes using hierarchical    edge bundling via the D3 JavaScript library. See <http://d3js.org/> for more    information on D3."
"economiccomplexity",1,1,8,"2020-02-20 06:50","A wrapper of different methods from Linear Algebra for the equations  introduced in The Atlas of Economic Complexity and related literature. This  package provides standard matrix and graph output that can be used seamlessly  with other packages."
"econetwork",0,1,5,"2020-04-06 18:12","A collection of advanced tools, methods and models specifically  designed  for  analyzing different  types  of  ecological  networks - especially  antagonistic  (food  webs,  host-parasite),  mutualistic (plant-pollinator, plant-fungus, etc)  and competitive networks, as well as their variability in time and space. Statistical models are developed to describe and understand the mechanisms that determine species interactions, and to decipher the organization of these (multi-layer) ecological networks."
"EcoNetGen",0,1,5,"2018-10-05 19:52","Randomly generate a wide range of interaction networks with             specified size, average degree, modularity, and topological             structure. Sample nodes and links from within simulated networks             randomly, by degree, by module, or by abundance. Simulations             and sampling routines are implemented in 'FORTRAN', providing             efficient generation times even for large networks. Basic             visualization methods also included. Algorithms implemented             here are described in de Aguiar et al. (2017) <arXiv:1708.01242>."
"econet",1,1,5,"2020-08-28 09:40","Provides methods for estimating parameter-dependent network centrality measures with linear-in-means models. Both non linear least squares and maximum likelihood estimators are implemented. The methods allow for both link and node heterogeneity in network effects, endogenous network formation and the presence of unconnected nodes. The routines also compare the explanatory power of parameter-dependent network centrality measures with those of standard measures of network centrality. Benefits and features of the 'econet' package are illustrated using data from Battaglini and Patacchini (2018) and Battaglini, Patacchini, and Leone Sciabolazza (2020). For additional details, see the vignette."
"ECoL",0,1,3,"2019-02-26 22:10","Provides measures to characterize the complexity of classification     and regression problems based on aspects that quantify the linearity of the     data, the presence of informative feature, the sparsity and dimensionality     of the datasets. This package provides bug fixes, generalizations and     implementations of many state of the art measures. The measures are     described in the papers: Lorena et al. (2019) <doi:10.1145/3347711> and     Lorena et al. (2018) <doi:10.1007/s10994-017-5681-1>."
"EcoGenetics",0,2,13,"2017-03-03 18:01","Management and exploratory analysis of spatial data in landscape genetics. Easy integration of information from multiple sources with ""ecogen"" objects."
"dynwrap",4,2,3,"2020-03-09 16:10","Provides functionality to infer trajectories from single-cell data,  represent them into a common format, and adapt them. Other biological information  can also be added, such as cellular grouping, RNA velocity and annotation.  Saelens et al. (2019) <doi:10.1038/s41587-019-0071-9>."
"dyngen",2,1,1,"2020-09-10","A novel, multi-modal simulation engine for    studying dynamic cellular processes at single-cell resolution. 'dyngen'    is more flexible than current single-cell simulation engines. It    allows better method development and benchmarking, thereby stimulating    development and testing of novel computational methods. Cannoodt et    al. (2020) <doi:10.1101/2020.02.06.936971>."
"dynetNLAResistance",0,1,1,"2016-11-25","An anonymization algorithm to resist neighbor label attack in a dynamic network."
"DSviaDRM",0,1,1,"2015-05-12","Elucidation of human disease similarities has emerged as an active research area, which is highly relevant to etiology, disease classification, and drug repositioning. This package was designed and implemented for identifying disease similarities. It contains five functions which are 'DCEA', 'DCpathway', 'DS', 'comDCGL' and 'comDCGLplot'. In 'DCEA' function, differentially co-expressed genes and differentially co-expressed links are extracted from disease vs. health samples. Then 'DCpathway' function assigns differential co-expression values of pathways to be the average differential co-expression value of their component genes. Then 'DS' employs partial correlation coefficient of pathways as the disease similarity for each disease pairs. And 'DS' contains a permutation process for evaluating the statistical significant of observed disease partial correlation coefficients. At last, 'comDCGL' and 'comDCGLplot' sort out shared differentially co-expressed genes and differentially co-expressed links with regulation information and visualize them. "
"drake",0,1,37,"2020-06-29 19:20","A general-purpose computational engine for data  analysis, drake rebuilds intermediate data objects when their  dependencies change, and it skips work when the results are already up  to date.  Not every execution starts from scratch, there is native  support for parallel and distributed computing, and completed projects  have tangible evidence that they are reproducible.  Extensive  documentation, from beginner-friendly tutorials to practical examples  and more, is available at the reference website  <https://docs.ropensci.org/drake/> and the online manual  <https://books.ropensci.org/drake/>."
"dragon",1,1,7,"2020-08-31 16:00","Create, visualize, manipulate, and analyze bipartite mineral-chemistry    networks for set of focal element(s) across deep-time on Earth. The method is     described in Spielman and Moore (2020) <doi:10.3389/feart.2020.585087>."
"dnr",1,1,2,"2018-02-23 12:21","Functions are provided to fit temporal lag models to dynamic    networks. The models are build on top of exponential random graph models (ERGM) framework. There are    functions for simulating or forecasting networks for future time points.    Stable Multiple Time Step Simulation/Prediction from Lagged Dynamic Network Regression Models. Mallik, Almquist (2017, under review)."
"DNLC",0,1,1,"2016-12-11","Using Local Moran's I for detection of differential network local consistency."
"dnapath",2,1,2,"2020-06-25 15:20","Integrates pathway information into the differential network analysis of two gene expression datasets as described in Grimes, Potter, and Datta (2019) <doi:10.1038/s41598-019-41918-3>. Provides summary functions to break down the results at the pathway, gene, or individual connection level. The differential networks for each pathway of interest can be plotted, and the visualization will highlight any differentially expressed genes and all of the gene-gene associations that are significantly differentially connected."
"dm",12,1,7,"2020-07-30 02:20","Provides tools for working with multiple related    tables, stored as data frames or in a relational database.  Multiple    tables (data and metadata) are stored in a compound object, which can    then be manipulated with a pipe-friendly syntax."
"discretecdAlgorithm",0,1,7,"2019-12-23 17:45","Structure learning of Bayesian network using coordinate-descent    algorithm. This algorithm is designed for discrete network assuming a multinomial data set,    and we use a multi-logit model to do the regression.    The algorithm is described in Gu, Fu and Zhou (2016) <arXiv:1403.2310>."
"discourseGT",0,1,2,"2020-02-17 16:00","Analyzes group patterns using discourse analysis data with graph theory mathematics. Takes the order of which individuals talk and converts it to a network edge and weight list. Returns the density, centrality, centralization, and subgroup information for each group. Based on the analytical framework laid out in Chai et al. (2019) <doi:10.1187/cbe.18-11-0222>."
"DIscBIO",0,1,1,"2020-08-26","An open, multi-algorithmic pipeline for easy, fast and efficient	analysis of cellular sub-populations and the molecular signatures that	characterize them. The pipeline consists of four successive steps: data	pre-processing, cellular clustering with pseudo-temporal ordering, defining	differential expressed genes and biomarker identification. This package	implements extensions of the work published by Ghannoum et. al. (2019)	<doi:10.1101/700989>."
"DirectedClustering",0,1,1,"2018-01-11","Allows the computation of clustering coefficients for directed and weighted networks by using different approaches.     It allows to compute clustering coefficients that are not present in 'igraph' package.     A description of clustering coefficients can be found in ""Directed clustering in weighted networks: a new perspective"", Clemente, G.P., Grassi, R. (2017),  	    <doi:10.1016/j.chaos.2017.12.007>."
"diffusr",1,1,5,"2018-04-21 23:55","Implementation of network diffusion algorithms such as  heat diffusion or Markov random walks. Network diffusion algorithms generally  spread information in the form of node weights along the edges of a graph to other nodes.  These weights can for example be interpreted as temperature, an initial amount  of water, the activation of neurons in the brain, or the location of a random  surfer in the internet. The information (node weights) is iteratively propagated  to other nodes until a equilibrium state or stop criterion occurs."
"diffusionMap",0,2,6,"2014-02-20 07:08","Implements diffusion map method of data    parametrization, including creation and visualization of    diffusion map, clustering with diffusion K-means and	  regression using adaptive regression model.	  Richards (2009) <doi:10.1088/0004-637X/691/1/32>."
"DiffNet",0,59,1,"2017-02-28","Provides an implementation of statistically significant     differential sub-network analysis for paired biological networks.  "
"diffman",0,59,2,"2019-08-08 17:10","An algorithm based on graph theory tools to detect differentiation    problems. A differentiation problem occurs when aggregated     data are disseminated according to two different nomenclatures. By making the    difference for an additive variable X between an aggregate composed of categories    of the first nomenclature and an other aggregate, included in that first     aggregate, composed of categories of the second nomenclature,     it is sometimes possible to derive X on a small aggregate of records     which could then lead to a break of confidentiality.    The purpose of this package is to detect the set of aggregates composed of     categories of the first nomenclature which lead to a differentiation problem,     when given a confidentiality threshold. Reference: Vianney Costemalle (2019) <doi:10.3233/SJI-190564>. "
"DiagrammeR",5,59,17,"2020-01-16 18:20","    Build graph/network structures using functions for stepwise addition and    deletion of nodes and edges. Work with data available in tables for bulk    addition of nodes, edges, and associated metadata. Use graph selections    and traversals to apply changes to specific nodes or edges. A wide    selection of graph algorithms allow for the analysis of graphs. Visualize    the graphs and take advantage of any aesthetic properties assigned to    nodes and edges."
"dexterMST",1,1,4,"2019-08-20 12:20","Conditional Maximum Likelihood Calibration and data management of multistage tests.   Supports polytomous items and incomplete designs with linear as well as multistage tests.  Extended Nominal Response and Interaction models, DIF and profile analysis.  See Robert J. Zwitser and Gunter Maris (2015)<doi:10.1007/s11336-013-9369-6>."
"delayed",1,1,1,"2020-02-28","Mechanisms to parallelize dependent tasks in a manner that    optimizes the compute resources available. It provides access to ""delayed""    computations, which may be parallelized using futures. It is, to an extent,    a facsimile of the 'Dask' library (<https://dask.org/>), for the 'Python'    language."
"degreenet",0,1,6,"2015-04-06 07:43","Likelihood-based inference for skewed count distributions used in network modeling. ""degreenet"" is a part of the ""statnet"" suite of packages for network analysis."
"deepdep",2,1,2,"2020-03-06 14:20","Provides tools for exploration of R package dependencies.     The main deepdep() function allows to acquire deep dependencies of any package and plot them in an elegant way.    It also adds some popularity measures for the packages e.g. in the form of download count through the 'cranlogs' package.     Uses the CRAN metadata database <http://crandb.r-pkg.org> and Bioconductor metadata <http://bioconductor.org>.    Other data acquire functions are: get_dependencies(), get_downloads() and get_description().     The deepdep_shiny() function runs shiny application that helps to produce a nice 'deepdep' plot. "
"deaR",0,1,4,"2019-10-25 11:00","Set of functions for Data Envelopment Analysis. It runs both classic and fuzzy DEA models.See: Banker, R.; Charnes, A.; Cooper, W.W. (1984). <doi:10.1287/mnsc.30.9.1078>, Charnes, A.; Cooper, W.W.; Rhodes, E. (1978). <doi:10.1016/0377-2217(78)90138-8> and Charnes, A.; Cooper, W.W.; Rhodes, E. (1981). <doi:10.1287/mnsc.27.6.668>."
"DDPNA",0,1,4,"2020-04-01 16:10","Functions designed to connect disease-related differential proteins and   co-expression network. It provides the basic statics analysis included t test, ANOVA analysis.   The network construction is not offered by the package, you can used 'WGCNA' package which you   can learn in Peter et al. (2008) <doi:10.1186/1471-2105-9-559>. It also provides module analysis   included PCA analysis, two enrichment analysis, Planner maximally filtered graph extraction and   hub analysis."
"DCGL",0,1,9,"2013-12-31 07:13","Functions for 1) gene filtration; 2) link filtration; 3) differential co-expression analysis: DCG (Differential Coexpressed        Gene) identification and DCL (Differentially Coexpressed Link)        identification; and 4) differential regulation analysis: DRG (Differential Regulated        Gene) identification, DRL (Differential Regulated Link)        identification, DRL visualization and regulator ranking."
"DCD",0,1,1,"2017-09-12","A differential community detection (DCD) based approach to effectively locate differential sub-networks in paired scale-free biological networks, e.g. case vs control - Raghvendra Mall et al (2017) <doi:10.1145/3107411.3107418>."
"cytometree",2,1,7,"2019-08-19 20:30","Given the hypothesis of a bi-modal distribution of cells for    each marker, the algorithm constructs a binary tree, the nodes of which are    subpopulations of cells. At each node, observed cells and markers are modeled    by both a family of normal distributions and a family of bi-modal normal mixture    distributions. Splitting is done according to a normalized difference of AIC    between the two families. Method is detailed in: Commenges, Alkhassim, Gottardo,     Hejblum & Thiebaut (2018) <doi:10.1002/cyto.a.23601>. "
"crimelinkage",2,1,2,"2015-04-18 17:19","Statistical Methods for Crime Series Linkage. This package provides       code for criminal case linkage, crime series identification, crime series       clustering, and suspect identification."
"cRegulome",2,1,6,"2019-05-25 06:20","Builds a 'SQLite' database file of pre-calculated transcription     factor/microRNA-gene correlations (co-expression) in cancer from the     Cistrome Cancer Liu et al. (2011) <doi:10.1186/gb-2011-12-8-r83> and     'miRCancerdb' databases (in press). Provides custom classes and functions     to query, tidy and plot the correlation data."
"cranly",4,1,6,"2019-07-23 00:20","Core visualizations and summaries for the CRAN package database. The package provides comprehensive methods for cleaning up and organizing the information in the CRAN package database, for building package directives networks (depends, imports, suggests, enhances, linking to) and collaboration networks, producing package dependence trees, and for computing useful summaries and producing interactive visualizations from the resulting networks and summaries. The resulting networks can be coerced to 'igraph' <https://CRAN.R-project.org/package=igraph> objects for further analyses and modelling."
"crandep",3,1,4,"2020-08-11 16:10","The dependencies of CRAN packages can be analysed in a network fashion. For each package we can obtain the packages that it depends, imports, suggests, etc. By iterating this procedure over a number of packages, we can build, visualise, and analyse the dependency network, enabling us to have a bird's-eye view of the CRAN ecosystem. One aspect of interest is the number of reverse dependencies of the packages, or equivalently the in-degree distribution of the dependency network. This can be fitted by the power law and/or an extreme value mixture distribution <arXiv:2008.03073>, of which functions are provided."
"corTest",0,1,6,"2020-07-19 16:40","There are 6 novel robust tests for equal correlation. They are all based on logistic regressions. The score statistic U is proportion to difference of two correlations based on different types of correlation in  6 methods. The ST1() is based on Pearson correlation. ST2() improved ST1() by using median absolute deviation. ST3() utilized type M correlation and ST4() used Spearman correlation.  ST5() and ST6() used two different ways to combine ST3() and ST4().  We highly recommend ST5() according to the article titled ”New Statistical Methods for Constructing Robust Differential Correlation Networks to characterize the interactions among microRNAs” published in Scientific Reports.  Please see the reference: Yu et al. (2019) <doi:10.1038/s41598-019-40167-8>. "
"corpustools",1,1,5,"2019-11-20 00:10","Provides text analysis in R, focusing on the use of a tokenized text format. In this format, the positions of tokens are maintained, and each token can be annotated (e.g., part-of-speech tags, dependency relations).    Prominent features include advanced Lucene-like querying for specific tokens or contexts (e.g., documents, sentences),    similarity statistics for words and documents, exporting to DTM for compatibility with many text analysis packages,    and the possibility to reconstruct original text from tokens to facilitate interpretation."
"corHMM",1,2,16,"2020-08-05 23:20","Fits hidden Markov models of discrete character evolution which allow different transition rate classes on different portions of a phylogeny. Beaulieu et al (2013) <doi:10.1093/sysbio/syt034>."
"cooccurNet",1,2,7,"2017-01-11 09:00","Read and preprocess fasta format data, and construct the co-occurrence network for downstream analyses. This R package is to construct the co-occurrence network with the algorithm developed by Du (2008) <doi:10.1101/gr.6969007>. It could be used to transform the data with high-dimension, such as DNA or protein sequence, into co-occurrence networks. Co-occurrence network could not only capture the co-variation pattern between variables, such as the positions in DNA or protein sequences, but also reflect the relationship between samples. Although it is originally used in DNA and protein sequences, it could be also used to other kinds of data, such as RNA, SNP, etc."
"contact",1,2,6,"2020-06-02 13:00","Process spatially- and temporally-discrete data into contact and    social networks, and facilitate network analysis by randomizing    individuals' movement paths and/or related categorical variables. To use    this package, users need only have a dataset containing spatial data    (i.e., latitude/longitude, or planar x & y coordinates), individual IDs    relating spatial data to specific individuals, and date/time information    relating spatial locations to temporal locations. The functionality of this    package ranges from data ""cleaning"" via multiple filtration functions, to    spatial and temporal data interpolation, and network creation and analysis.    Functions within this package are not limited to describing interpersonal    contacts. Package functions can also identify and quantify ""contacts""    between individuals and fixed areas (e.g., home ranges, water bodies,    buildings, etc.). As such, this package is an incredibly useful resource    for facilitating epidemiological, ecological, ethological and sociological    research."
"concorR",0,2,1,"2020-06-03","Contains the CONCOR (CONvergence of iterated CORrelations)     algorithm and a series of supplemental functions for easy running,     plotting, and blockmodeling. The CONCOR algorithm is used on social network    data to identify network positions based off a definition of structural     equivalence; see Breiger, Boorman, and Arabie (1975)     <doi:10.1016/0022-2496(75)90028-0> and Wasserman and Faust's book Social     Network Analysis: Methods and Applications (1994). This version allows     multiple relationships for the same set of nodes and uses both incoming and    outgoing ties to find positions."
"comato",0,2,2,"2014-03-18 11:55","Provides methods for the import/export and automated analysis of concept maps and concept landscapes (sets of concept maps)."
"CoDiNA",0,2,4,"2018-07-06 18:30","Categorize links and nodes from multiple networks in 3 categories: Common links (alpha) specific links (gamma), and different links (beta). Also categorizes the links into sub-categories and groups. The package includes a visualization tool for the networks. More information about the methodology can be found at: Gysi et. al., 2018 <arXiv:1802.00828>."
"clustringr",0,2,1,"2019-03-30","Returns an edit-distance based clusterization of an input vector of strings.    Each cluster will contain a set of strings w/ small mutual edit-distance    (e.g., Levenshtein, optimum-sequence-alignment, Damerau-Levenshtein), as computed by    stringdist::stringdist(). The set of all mutual edit-distances is then used by    graph algorithms (from package 'igraph') to single out subsets of high connectivity."
"clustree",1,2,10,"2020-01-29 13:30","Deciding what resolution to use can be a difficult question when    approaching a clustering analysis. One way to approach this problem is to    look at how samples move as the number of clusters increases. This package    allows you to produce clustering trees, a visualisation for interrogating    clusterings as resolution increases."
"CliquePercolation",1,2,1,"2019-10-28","Clique percolation community detection for weighted and     unweighted networks as well as threshold and plotting functions.    For more information see Farkas et al. (2007) <doi:10.1088/1367-2630/9/6/180>    and Palla et al. (2005) <doi:10.1038/nature03607>."
"CINNA",1,2,11,"2019-02-02 23:13","Functions for computing, comparing and demonstrating top informative centrality measures within a network."
"cholera",7,2,8,"2019-06-11 06:30","Amends errors, augments data and aids analysis of John Snow's map  of the 1854 London cholera outbreak."
"CeRNASeek",0,2,5,"2019-07-20 08:20","Provides several functions to identify and analyse miRNA sponge, including             popular methods for identifying miRNA sponge interactions, two types              of global ceRNA regulation prediction methods and four types of context-specific              prediction methods( Li Y et al.(2017) <doi:10.1093/bib/bbx137>), which are based              on miRNA-messenger RNA regulation alone, or by integrating heterogeneous data,              respectively. In addition, For predictive ceRNA relationship pairs, this package              provides several downstream analysis algorithms, including regulatory network              analysis and functional annotation analysis, as well as survival prognosis analysis             based on expression of ceRNA ternary pair. "
"CePa",3,2,7,"2018-06-04 21:40","Use pathway topology information to assign weight to pathway nodes."
"CDVine",0,2,21,"2015-07-30 11:59","Functions for statistical inference of canonical vine (C-vine)        and D-vine copulas. Tools for bivariate exploratory data analysis and for bivariate        as well as vine copula selection are provided. Models can be estimated        either sequentially or by joint maximum likelihood estimation.        Sampling algorithms and plotting methods are also included.        Data is assumed to lie in the unit hypercube (so-called copula        data)."
"causaleffect",2,1,17,"2018-11-29 07:00","Functions for identification and transportation of causal effects. Provides a conditional causal effect identification algorithm (IDC) by Shpitser, I. and Pearl, J. (2006) <http://ftp.cs.ucla.edu/pub/stat_ser/r329-uai.pdf>, an algorithm for transportability from multiple domains with limited experiments by Bareinboim, E. and Pearl, J. (2014) <http://ftp.cs.ucla.edu/pub/stat_ser/r443.pdf> and a selection bias recovery algorithm by Bareinboim, E. and Tian, J. (2015) <http://ftp.cs.ucla.edu/pub/stat_ser/r445.pdf>. All of the previously mentioned algorithms are based on a causal effect identification algorithm by Tian , J. (2002) <http://ftp.cs.ucla.edu/pub/stat_ser/r309.pdf>. "
"causact",0,1,3,"2020-07-09 07:00","Accelerate Bayesian analytics workflows in 'R' through interactive modelling,    visualization, and inference. Define probabilistic graphical models using directed    acyclic graphs (DAGs) as a unifying language for business stakeholders, statisticians,     and programmers. This package relies on the sleek and elegant 'greta' package for     Bayesian inference. 'greta', in turn, is an interface into 'TensorFlow' from 'R'.     Install 'greta' using instructions available here: <http://www.causact.com/install-tensorflow-greta-and-causact.html>.    See <http://github.com/flyaflya/causact> or <http://causact.com> for more documentation."
"Cascade",3,1,2,"2019-02-18 15:51","A modeling tool allowing gene selection, reverse engineering, and prediction in cascade networks. Jung, N., Bertrand, F., Bahram, S., Vallat, L., and Maumy-Bertrand, M. (2014) <doi:10.1093/bioinformatics/btt705>."
"cartograflow",1,3,4,"2020-06-03 12:10","Functions to prepare and filter an origin-destination matrix for thematic flow mapping purposes.                This comes after Bahoken, Francoise (2016), Mapping flow matrix a contribution, PhD in Geography - Territorial sciences. See Bahoken (2017) <doi:10.4000/netcom.2565>."
"cancerGI",0,3,1,"2016-04-01","Functions to perform the following analyses: i) inferring epistasis from RNAi double knockdown data; ii) identifying gene pairs of multiple mutation patterns; iii) assessing association between gene pairs and survival; and iv) calculating the smallworldness of a graph (e.g., a gene interaction network).  Data and analyses are described in Wang, X., Fu, A. Q., McNerney, M. and White, K. P. (2014). Widespread genetic epistasis among breast cancer genes. Nature Communications. 5 4828. <doi:10.1038/ncomms5828>."
"C443",0,3,3,"2020-04-25 17:30","Get insight into a forest of classification trees, by calculating similarities between the trees, and subsequently clustering them. Each cluster is represented by it's most central cluster member. The package implements the methodology described in Sies & Van Mechelen (2020) <doi:10.1007/s00357-019-09350-4>."
"bwsTools",3,3,4,"2020-06-14 16:30","Tools to design best-worst scaling designs (i.e., balanced incomplete block designs) and    to analyze data from these designs, using aggregate and individual methods such as: difference     scores, Louviere, Lings, Islam, Gudergan, & Flynn (2013) <doi:10.1016/j.ijresmar.2012.10.002>;     analytical estimation, Lipovetsky & Conklin (2014) <doi:10.1016/j.jocm.2014.02.001>; empirical     Bayes, Lipovetsky & Conklin (2015) <doi:10.1142/S1793536915500028>; Elo, Hollis (2018)     <doi:10.3758/s13428-017-0898-2>; and network-based measures."
"btergm",0,3,15,"2020-04-07 14:20","Temporal Exponential Random Graph Models (TERGM) estimated by maximum pseudolikelihood with bootstrapped confidence intervals or Markov Chain Monte Carlo maximum likelihood. Goodness of fit assessment for ERGMs, TERGMs, and SAOMs. Micro-level interpretation of ERGMs and TERGMs."
"BPEC",1,2,6,"2018-08-29 22:56","Model-based clustering for phylogeographic data comprising mtDNA sequences and geographical locations along with optional environmental characteristics, aiming to identify migration events that led to homogeneous population clusters. The package vignette, I. Manolopoulou, A. Hille, B. C. Emerson (2020) <doi:10.18637/jss.v092.i03>, provides detailed descriptions of the package. "
"bootnet",0,2,16,"2020-05-10 21:30","Bootstrap methods to assess accuracy and stability of estimated network structures              and centrality indices <doi:10.3758/s13428-017-0862-1>. Allows for flexible               specification of any undirected network estimation procedure in R, and offers               default sets for various estimation routines."
"BoolNet",1,2,22,"2018-06-07 19:44","Provides methods to reconstruct and generate synchronous,        asynchronous, probabilistic and temporal Boolean networks, and to        analyze and visualize attractors in Boolean networks."
"bnviewer",0,1,7,"2020-07-22 06:30","Bayesian networks provide an intuitive framework for probabilistic reasoning              and its graphical nature can be interpreted quite clearly. Graph based methods              of machine learning are becoming more popular because they offer a richer model              of knowledge that can be understood by a human in a graphical format. The 'bnviewer'              is an R Package that allows the interactive visualization of Bayesian Networks.              The aim of this package is to improve the Bayesian Networks visualization over              the basic and static views offered by existing packages."
"bnma",1,6,6,"2020-07-06 14:10","Network meta-analyses using Bayesian framework following Dias et al. (2013) <doi:10.1177/0272989X12458724>. Based on the data input, creates prior, model file, and initial values needed to run models in 'rjags'. Able to handle binomial, normal and multinomial arm-based data. Can handle multi-arm trials and includes methods to incorporate covariate and baseline risk effects. Includes standard diagnostics and visualization tools to evaluate the results."
"bipartite",1,6,50,"2020-01-09 00:02","Functions to visualise webs and calculate a series of indices commonly used to describe pattern in (ecological) webs. It focuses on webs consisting of only two levels (bipartite), e.g. pollination webs or predator-prey-webs. Visualisation is important to get an idea of what we are actually looking at, while the indices summarise different aspects of the web's topology. "
"Bios2cor",0,1,5,"2020-01-30 18:30","Utilities for computation and analysis of correlation/covariation in multiple sequence alignments and in side chain motions during molecular dynamics simulations. Features include the computation of correlation/covariation scores using a variety of scoring functions between either sequence positions in alignments or side chain dihedral angles in molecular dynamics simulations and utilities to analyze the correlation/covariation matrix through a variety of tools including network representation and principal components analysis. In addition, several utility functions are based on the R graphical environment to provide friendly tools for help in data interpretation. Examples of sequence covariation analysis are provided in: (1) Pele J, Moreau M, Abdi H, Rodien P, Castel H, Chabbert M (2014) <doi:10.1002/prot.24570> and (2) Taddese B, Deniaud M, Garnier A, Tiss A, Guissouma H, Abdi H, Henrion D, Chabbert M (2018) <doi:10.1371/journal.pcbi.1006209>. An example of side chain correlated motion analysis is provided in: Taddese B, Garnier A, Abdi H, Henrion D, Chabbert M (2020) <doi:10.1038/s41598-020-72766-1>. This work was supported by the French National Research Agency (Grant number: ANR-11-BSV2-026) and by GENCI (Grant number: 100567)."
"Bioi",1,2,3,"2018-01-22 12:59","Single linkage clustering and connected component analyses are often performed on biological images. 'Bioi' provides a set of functions for performing these tasks. This functionality is implemented in several key functions that can extend to from 1 to many dimensions. The single linkage clustering method implemented here can be used on n-dimensional data sets, while connected component analyses are limited to 3 or fewer dimensions."
"bibliometrix",2,2,34,"2020-06-17 12:00","Tool for quantitative research in scientometrics and bibliometrics.    It provides various routines for importing bibliographic data from 'SCOPUS' (<https://scopus.com>),    'Clarivate Analytics Web of Science' (<https://www.webofknowledge.com/>), 'Digital Science Dimensions' 	(<https://www.dimensions.ai/>), 'Cochrane Library' (<https://www.cochranelibrary.com/>) 	and 'PubMed' (<https://pubmed.ncbi.nlm.nih.gov/>) databases, performing bibliometric analysis     and building networks for co-citation, coupling, scientific collaboration and co-word analysis."
"betalink",0,5,4,"2015-12-22 18:12","Measures of beta-diversity in networks, and easy visualization of why two networks are different."
"beam",0,5,6,"2020-01-28 10:40","Fast Bayesian inference of marginal and conditional independence structures from high-dimensional data. Leday and Richardson (2019), Biometrics, <doi:10.1111/biom.13064>."
"BDgraph",1,5,67,"2019-12-05 19:40","Statistical tools for Bayesian structure learning in undirected graphical models for continuous, discrete, and mixed data. The package is implemented the recent improvements in the Bayesian graphical models literature, including Mohammadi and Wit (2015) <doi:10.1214/14-BA889>, Mohammadi and Wit (2019) <doi:10.18637/jss.v089.i03>. "
"bcgam",0,1,1,"2018-03-18","Fits generalised partial linear regression models using a Bayesian approach, where shape and			smoothness constraints are imposed on nonparametrically modelled predictors through shape-restricted splines, and 			no constraints are imposed on optional parametrically modelled covariates. See Meyer et al. (2011) <doi/10.1080/10485252.2011.597852> 			for more details. IMPORTANT: before installing 'bcgam', you need to install 'Rtools' (Windows) or 'Xcode' (Mac OS X). These are				required for the correct installation of 'nimble' (<https://r-nimble.org/download>). 			"
"BayesSUR",1,1,10,"2020-07-19 06:22","Bayesian seemingly unrelated regression with general variable selection and dense/sparse covariance matrix. The sparse seemingly unrelated regression is described in Banterle et al. (2018) <doi:10.1101/467019>."
"BayesNetBP",0,1,6,"2020-04-14 11:00","Belief propagation methods in Bayesian Networks to propagate evidence through the network. The implementation of these methods are based on the article: Cowell, RG (2005). Local Propagation in Conditional Gaussian Bayesian Networks <http://www.jmlr.org/papers/volume6/cowell05a/>. For details please see Yu et. al. (2020) BayesNetBP: An R Package for Probabilistic Reasoning in Bayesian Networks <doi:10.18637/jss.v094.i03>. The optional 'cyjShiny' package for running the Shiny app is available at <https://github.com/cytoscape/cyjShiny>. Please see the example in the documentation of 'runBayesNetApp' function for installing 'cyjShiny' package from GitHub. "
"BayesMRA",1,1,1,"2020-08-18","Software for fitting sparse Bayesian multi-resolution spatial models using Markov Chain Monte Carlo."
"BayesMallows",1,1,11,"2020-08-07 10:32","An implementation of the Bayesian version of the Mallows rank model     (Vitelli et al., Journal of Machine Learning Research, 2018 <https://jmlr.org/papers/v18/15-481.html>;     Crispino et al., Annals of Applied Statistics, 2019 <doi:10.1214/18-AOAS1203>). Both Cayley, footrule,     Hamming, Kendall, Spearman, and Ulam distances are supported in the models. The rank data to be     analyzed can be in the form of complete rankings, top-k rankings, partially missing rankings, as well     as consistent and inconsistent pairwise preferences. Several functions for plotting and studying the     posterior distributions of parameters are provided. The package also provides functions for estimating     the partition function (normalizing constant) of the Mallows rank model, both with the importance     sampling algorithm of Vitelli et al. and asymptotic approximation with the IPFP algorithm     (Mukherjee, Annals of Statistics, 2016 <doi:10.1214/15-AOS1389>)."
"baycn",0,1,3,"2020-03-10 20:50","A Bayesian hybrid approach for inferring Directed Acyclic Graphs    (DAGs) for continuous, discrete, and mixed data. The algorithm can use the     graph inferred by another more efficient graph inference method as input;    the input graph may contain false edges or undirected edges but can help    reduce the search space to a more manageable size. A Bayesian Markov chain    Monte Carlo algorithm is then used to infer the probability of direction and    absence for the edges in the network.    References:    Martin and Fu (2019) <arXiv:1909.10678>."
"basket",1,1,4,"2020-03-11 06:40","Implementation of multisource exchangeability models for Bayesian analyses of prespecified subgroups arising in the context of basket trial design and monitoring.  The R 'basket' package facilitates implementation of the binary, symmetric multi-source exchangeability model (MEM) with posterior inference arising from both exact computation and Markov chain Monte Carlo sampling. Analysis output includes full posterior samples as well as posterior probabilities, highest posterior density (HPD) interval boundaries, effective sample sizes (ESS), mean and median estimations, posterior exchangeability probability matrices, and maximum a posteriori MEMs. In addition to providing ""basketwise"" analyses, the package includes similar calculations for ""clusterwise"" analyses for which subgroups are combined into meta-baskets, or clusters, using graphical clustering algorithms that treat the posterior exchangeability probabilities as edge weights. In addition plotting tools are provided to visualize basket and cluster densities as well as their exchangeability.  References include Hyman, D.M., Puzanov, I., Subbiah, V., Faris, J.E., Chau, I., Blay, J.Y., Wolf, J., Raje, N.S., Diamond, E.L., Hollebecque, A. and Gervais, R (2015) <doi:10.1056/NEJMoa1502309>; Hobbs, B.P. and Landin, R. (2018) <doi:10.1002/sim.7893>; Hobbs, B.P., Kane, M.J., Hong, D.S. and Landin, R. (2018) <doi:10.1093/annonc/mdy457>; and Kaizer, A.M., Koopmeiners, J.S. and Hobbs, B.P. (2017) <doi:10.1093/biostatistics/kxx031>."
"BASiNET",1,1,3,"2018-04-19 00:27","It makes the creation of networks from sequences of RNA, with this is done the abstraction of characteristics of these networks with a methodology of threshold for the purpose of making a classification between the classes of the sequences. There are four data present in the 'BASiNET' package, ""sequences"", ""sequences2"", ""sequences-predict"" and ""sequences2-predict"" with 11, 10, 11 and 11 sequences respectively. These sequences were taken from the data set used in the article (LI, Aimin; ZHANG, Junying; ZHOU, Zhongyin, 2014) <doi:10.1186/1471-2105-15-311>, these sequences are used to run examples. The BASiNET was published on Nucleic Acids Research, (ITO, Eric; KATAHIRA, Isaque; VICENTE, Fábio; PEREIRA, Felipe; LOPES, Fabrício, 2018) <doi:10.1093/nar/gky462>."
"BallMapper",0,1,2,"2019-07-26 13:10","The core algorithm is described in ""Ball mapper: a shape summary for topological data analysis"" by Pawel Dlotko, (2019) <arXiv:1901.07410>. Please consult the following youtube video <https://www.youtube.com/watch?v=M9Dm1nl_zSQfor> the idea of functionality. Ball Mapper provide a topologically accurate summary of a data in a form of an abstract graph. To create it, please provide the coordinates of points (in the points array), values of a function of interest at those points (can be initialized randomly if you do not have it) and the value epsilon which is the radius of the ball in the Ball Mapper construction. It can be understood as the minimal resolution on which we use to create the model of the data. "
"backShift",1,1,6,"2017-09-07 12:52","Code for 'backShift', an algorithm to estimate the connectivity    matrix of a directed (possibly cyclic) graph with hidden variables. The    underlying system is required to be linear and we assume that observations    under different shift interventions are available. For more details,    see <arXiv:1506.02494>."
"backbone",1,4,5,"2020-07-03 06:40","Provides methods for extracting from a weighted graph     a binary or signed backbone that retains only the significant edges.     The user may input a weighted graph, or a bipartite graph     from which a weighted graph is first constructed via projection.    Backbone extraction methods include the stochastic degree sequence model (Neal, Z. P. (2014). <doi:10.1016/j.socnet.2014.06.001>),    hypergeometric model (Neal, Z. (2013). <doi:10.1007/s13278-013-0107-y>),     the fixed degree sequence model (Zweig, K. A., and Kaufmann, M. (2011). <doi:10.1007/s13278-011-0021-0>),    as well as a universal threshold method. "
"BacArena",1,4,9,"2019-02-15 12:50","Can be used for simulation of organisms living in    communities (Bauer and Zimmermann (2017) <doi:10.1371/journal.pcbi.1005544>).     Each organism is represented individually and genome scale    metabolic models determine the uptake and release of compounds. Biological    processes such as movement, diffusion, chemotaxis and kinetics are available    along with data analysis techniques."
"automultinomial",1,4,2,"2016-10-05 09:29","Fits the autologistic model described in Besag's famous 1974 paper on auto- models <http://www.jstor.org/stable/2984812>. Fits a multicategory generalization of the autologistic model when there are more than 2 response categories. Provides support for both asymptotic and bootstrap confidence intervals. For full model descriptions and a guide to the use of this package, please see the vignette."
"arulesViz",1,4,23,"2018-12-05 08:40","Extends package 'arules' with various visualization techniques for association rules and itemsets. The package also includes several interactive visualizations for rule exploration."
"AnimalHabitatNetwork",0,5,1,"2019-11-25","Functions for generating and visualising networks for characterising the physical attributes and spatial organisations of habitat components (i.e. habitat physical configurations). The network generating algorithm first determines the X and Y coordinates of N nodes within a rectangle with a side length of L and an area of A. Then it computes the pair-wise Euclidean distance Dij between node i and j, and then a complete network with 1/Dij as link weights is constructed. Then, the algorithm removes links from the complete network with the probability as shown in the function ahn_prob(). Such link removals can make the network disconnected whereas a connected network is wanted. In such cases, the algorithm rewires one network component to its spatially nearest neighbouring component and repeat doing this until the network is connected again. Finally, it outputs an undirected network (weighted or unweighted, connected or disconnected). This package came with a manuscript on modelling the physical configurations of animal habitats using networks (in preparation)."
"alakazam",5,5,14,"2020-05-11 20:10","Provides methods for high-throughput adaptive immune     receptor repertoire sequencing (AIRR-Seq; Rep-Seq) analysis. In     particular, immunoglobulin (Ig) sequence lineage reconstruction,     lineage topology analysis, diversity profiling, amino acid property     analysis and gene usage.    Citations:     Gupta and Vander Heiden, et al (2017) <doi:10.1093/bioinformatics/btv359>,    Stern, Yaari and Vander Heiden, et al (2014) <doi:10.1126/scitranslmed.3008879>."
"akc",2,19,4,"2020-01-26 07:20","A tidy framework for automatic knowledge classification and visualization. Currently, the core functionality of the framework is mainly supported by modularity-based clustering (community detection) in keyword co-occurrence network, and focuses on co-word analysis of bibliometric research. However, the designed functions in 'akc' are general, and could be extended to solve other tasks in text mining as well. "
"AFM",0,19,7,"2018-04-28 08:21","Provides Atomic Force Microscope images analysis such as Gaussian mixes identification, Power    Spectral Density, roughness against lengthscale, experimental variogram and variogram models,    fractal dimension and scale, 2D network analysis. The AFM images can be exported to STL format for 3D    printing."
"adegenet",0,19,36,"2020-01-20 20:50","Toolset for the exploration of genetic and genomic    data. Adegenet provides formal (S4) classes for storing and handling    various genetic data, including genetic markers with varying ploidy    and hierarchical population structure ('genind' class), alleles counts    by populations ('genpop'), and genome-wide SNP data ('genlight'). It    also implements original multivariate methods (DAPC, sPCA), graphics,    statistical tests, simulation tools, distance and similarity measures,    and several spatial methods. A range of both empirical and simulated    datasets is also provided to illustrate various methods."
"adapr",1,3,4,"2016-11-07 08:12","Tracks reading and writing within R scripts that are organized into    a directed acyclic graph. Contains an interactive shiny application adaprApp().    Uses git2r package, Git and file hashes to track version histories of input    and output. See package vignette for how to get started. V1.02 adds parallel    execution of project scripts and function map in vignette. Makes project    specification argument last in order. V2.0 adds project specific libraries, packrat option, and adaprSheet()."
"wfg",0,3,1,"2016-02-25","Implementation of Weighted Fast Greedy algorithm for community detection in networks with mixed types of attributes."
"VertexSort",0,3,2,"2017-03-04 08:21","Permits to apply the 'Vertex Sort' algorithm (Jothi et al. (2009) <10.1038/msb.2009.52>) to a graph in order to elucidate its hierarchical structure. It also allows graphic visualization of the sorted graph by exporting the results to a cytoscape friendly format. Moreover, it offers five different algorithms of graph randomization: 1) Randomize a graph with preserving node degrees, 2) with preserving similar node degrees, 3) without preserving node degrees, 4) with preserving node in-degrees and 5) with preserving node out-degrees."
"tnet",0,3,25,"2011-02-01 17:47","Binary ties limit the richness of network analyses as relations are unique. The two-mode structure contains a number of features lost when projection it to a one-mode network. Longitudinal datasets allow for an understanding of the causal relationship among ties, which is not the case in cross-sectional datasets as ties are dependent upon each other."
"timeordered",0,7,10,"2011-03-28 16:35","Approaches for incorporating time into network analysis. Methods include: construction of time-ordered networks (temporal graphs); shortest-time and shortest-path-length analyses; resource spread calculations; data resampling and rarefaction for null model construction; reduction to time-aggregated networks with variable window sizes; application of common descriptive statistics to these networks; vector clock latencies; and plotting functionalities. The package supports <doi:10.1371/journal.pone.0020298>. "
"threejs",0,7,4,"2017-08-13 07:42","Create interactive 3D scatter plots, network plots, and    globes using the 'three.js' visualization library (<https://threejs.org>)."
"tcR",1,2,14,"2019-03-25 16:20","Platform for the advanced analysis of T cell receptor and    Immunoglobulin repertoires data and visualisation of the analysis results.    Publication: Nazarov et al. (2015) <doi:10.1186/s12859-015-0613-1>."
"synRNASeqNet",0,2,1,"2015-04-20","It implements various estimators of mutual information, such as	the maximum likelihood and the Millow-Madow estimator, various Bayesian	estimators, the shrinkage estimator, and the Chao-Shen estimator. It also	offers wrappers to the kNN and kernel density estimators. Furthermore, it	provides various index of performance evaluation such as precision, recall,	FPR, F-Score, ROC-PR Curves and so on. Lastly, it provides a brand new way	of generating synthetic RNA-Seq Network with known dependence structure."
"SVN",0,2,2,"2019-04-10 20:25","Determines networks of significant synchronization between the discrete states of nodes; see Tumminello et al <doi:10.1371/journal.pone.0017994>."
"SubpathwayLNCE",1,2,1,"2016-01-19","Identify dysfunctional subpathways competitively regulated by lncRNAs through integrating lncRNA-mRNA expression profile and pathway topologies. "
"SubpathwayGMir",1,2,1,"2015-05-20","Routines for identifying metabolic subpathways mediated by microRNAs (miRNAs) through topologically locating miRNAs and genes within reconstructed Kyoto Encyclopedia of Genes and Genomes (KEGG) metabolic pathway graphs embedded by miRNAs. (1) This package can obtain the reconstructed KEGG metabolic pathway graphs with genes and miRNAs as nodes, through converting KEGG metabolic pathways to graphs with genes as nodes and compounds as edges, and then integrating miRNA-target interactions verified by low-throughput experiments from four databases (TarBase, miRecords, mirTarBase and miR2Disease) into converted pathway graphs. (2) This package can locate metabolic subpathways mediated by miRNAs by topologically analyzing the ""lenient distance"" of miRNAs and genes within reconstructed KEGG metabolic pathway graphs.(3) This package can identify significantly enriched miRNA-mediated metabolic subpathways based on located subpathways by hypergenomic test. (4) This package can support six species for metabolic subpathway identification, such as caenorhabditis elegans, drosophila melanogaster, danio rerio, homo sapiens, mus musculus and rattus norvegicus, and user only need to update interested organism-specific environment variables."
"SteinerNet",1,2,8,"2018-08-19 06:20","A set of functions for finding and analysing Steiner trees. It has applications in    biological pathway network analysis. Sadeghi (2013) <doi:10.1186/1471-2105-14-144>."
"soptdmaeA",0,2,1,"2017-06-10","Computes sequential A-, MV-, D- and E-optimal or near-optimal block and row-column designs for two-colour cDNA microarray experiments using the linear fixed effects and mixed effects models where the interest is in a comparison of all possible elementary treatment contrasts. The package also provides an optional method of using the graphical user interface (GUI) R package 'tcltk' to ensure that it is user friendly."
"SOMbrero",5,2,9,"2020-06-08 17:00","The stochastic (also called on-line) version of the Self-Organising             Map (SOM) algorithm is provided. Different versions of the             algorithm are implemented, for numeric and relational data and for             contingency tables as described, respectively, in Kohonen (2001)             <isbn:3-540-67921-9>, Olteanu & Villa-Vialaneix (2005)             <doi:10.1016/j.neucom.2013.11.047> and Cottrell et al (2004)             <doi:10.1016/j.neunet.2004.07.010>. The package also contains many             plotting features (to help the user interpret the results) and a             graphical user interface based on 'shiny'."
"sindyr",0,1,4,"2018-08-16 11:30","    This implements the Brunton et al (2016; PNAS <doi:10.1073/pnas.1517384113>) sparse    identification algorithm for finding ordinary differential equations for a measured     system from raw data (SINDy). The package includes a set of additional tools for     working with raw data, with an emphasis on cognitive science applications (Dale     and Bhat, in press <doi:10.1016/j.cogsys.2018.06.020>)."
"simule",0,1,6,"2017-11-01 07:29","This is an R implementation of a constrained l1 minimization approach for estimating multiple Sparse Gaussian or Nonparanormal Graphical Models (SIMULE). The SIMULE algorithm can be used to estimate multiple related precision matrices. For instance, it can identify context-specific gene networks from multi-context gene expression datasets. By performing data-driven network inference from high-dimensional and heterogenous data sets, this tool can help users effectively translate aggregated data into knowledge that take the form of graphs among entities. Please run demo(simuleDemo) to learn the basic functions provided by this package. For further details, please read the original paper: Beilun Wang, Ritambhara Singh, Yanjun Qi (2017) <doi:10.1007/s10994-017-5635-7>."
"shp2graph",0,1,5,"2018-04-08 06:04","Functions for converting network data from a        SpatialLinesDataFrame object to an 'igraph'-Class object."
"ShapePattern",0,1,6,"2020-04-09 16:10","This is an evolving and growing collection of tools for the quantification, assessment, and comparison of shape and pattern. This collection provides tools for: (1) the spatial decomposition of planar shapes using 'ShrinkShape' to incrementally shrink shapes to extinction while computing area, perimeter, and number of parts at each iteration of shrinking; the spectra of results are returned in graphic and tabular formats (Remmel 2015) <doi:10.1111/cag.12222>, (2) simulating landscape patterns, (3) provision of tools for estimating composition and configuration parameters from a categorical (binary) landscape map (grid) and then simulates a selected number of statistically similar landscapes. Class-focused pattern metrics are computed for each simulated map to produce empirical distributions against which statistical comparisons can be made. The code permits the analysis of single maps or pairs of maps (Remmel and Fortin 2013) <doi:10.1007/s10980-013-9905-x>, (4) counting the number of each first-order pattern element and converting that information into both frequency and empirical probability vectors (Remmel 2020) <doi:10.3390/e22040420>, and (5) computing the porosity of raster patches <doi:10.3390/su10103413>. NOTE: This is a consolidation of existing packages ('PatternClass', 'ShapePattern') to begin warehousing all shape and pattern code in a common package. Additional utility tools for handling data are provided and this package will be added to as more tools are created, cleaned-up, and documented.  Note that all future developments will appear in this package and that 'PatternClass' will eventually be archived."
"sglasso",0,1,9,"2015-05-04 23:28","RCON(V, E) models are a kind of restriction of the Gaussian Graphical Models defined by a set of equality constraints on the entries of the concentration matrix. 'sglasso' package implements the structured graphical lasso (sglasso) estimator proposed in Abbruzzo et al. (2014) for the weighted l1-penalized RCON(V, E) model. Two cyclic coordinate algorithms are implemented to compute the sglasso estimator, i.e. a cyclic coordinate minimization (CCM) and a cyclic coordinate descent (CCD) algorithm."
"secrlinear",1,1,5,"2017-05-03 09:33","Tools for spatially explicit capture-recapture analysis of animal populations in linear habitats, extending package 'secr'."
"SDDE",0,1,2,"2015-03-06 17:41","Compares the evolution of an original network X to an augmented network Y by counting the number of Shortcuts, Detours, Dead Ends (SDDE), equal paths and disconnected nodes. "
"SARP.compo",0,1,6,"2019-03-18 15:23","Provides a set of functions to interpret changes in compositional data based on a network representation of all pairwise ratio comparisons: computation of all pairwise ratio, construction of a p-value matrix of all pairwise tests of these ratios between conditions, conversion of this matrix to a network."
"sand",0,1,3,"2017-03-02 08:09","Data sets and code blocks for the book 'Statistical Analysis of   Network Data with R, 2nd Edition'."
"RWBP",0,1,1,"2014-06-24","a Bipartite graph and is constructed based on the spatial and/or non-spatial attributes of the spatial objects in the dataset. Secondly, RW techniques are utilized on the graphs to compute the outlierness for each point (the differences between spatial objects and their spatial neighbours). The top k objects with higher outlierness are recognized as outliers."
"robin",1,1,4,"2020-02-17 20:00","Assesses the robustness of the community structure of a network found by one or more community detection algorithm to give indications about their reliability. It detects if the community structure found by a set of algorithms is statistically significant and compares the different selected detection algorithms on the same network. robin helps to choose among different community detection algorithms the one that better fits the network of interest. Reference in Annamaria Carissimo, Luisa Cutillo, Italia De Feis (2018) <doi:10.1016/j.csda.2017.10.006>."
"RNewsflow",1,1,7,"2020-02-17 17:40","A collection of tools for measuring the similarity of text messages and tracing the flow of messages over    time and across media. "
"RNetLogo",1,1,13,"2017-01-21 23:08","Interface to use and access Wilensky's 'NetLogo' (Wilensky 1999) from R using either headless (no GUI) or interactive GUI mode. Provides functions to load models, execute commands, and get values from reporters. Mostly analogous to the 'NetLogo' 'Mathematica' Link <https://github.com/NetLogo/Mathematica-Link>."
"rEMM",1,1,12,"2015-07-02 16:07","Implements TRACDS (Temporal Relationships     between Clusters for Data Streams), a generalization of     Extensible Markov Model (EMM). TRACDS adds a temporal or order model    to data stream clustering by superimposing a dynamically adapting    Markov Chain. Also provides an implementation of EMM (TRACDS on top of tNN     data stream clustering). Development of this     package was supported in part by NSF IIS-0948893 and R21HG005912 from     the National Human Genome Research Institute."
"RCA",0,1,6,"2013-05-23 22:47","Relational Class Analysis (RCA) is a method for detecting        heterogeneity in attitudinal data (as described in Goldberg        A., 2011, Am. J. Soc, 116(5))."
"ragt2ridges",0,1,11,"2019-12-05 14:10","The ragt2ridges-package provides ridge maximum likelihood estimation of vector auto-regressive processes: the VAR(1), VAR(2) and VARX(1) model (more to be added). Prior knowledge may be incorporated in the estimation through a) specification of the edges believed to be absent in the time series chain graph, and b) a shrinkage target towards which the parameter estimate is shrunken for large penalty parameter values. Estimation functionality is accompanied by methodology for penalty parameter selection. In addition, the package offers supporting functionality for the exploitation of estimated models. Among others, i) a procedure to infer the support of the non-sparse ridge estimate (and thereby of the time series chain graph) is implemented, ii) a table of node-wise network summary statistics, iii) mutual information analysis, and iv) impulse response analysis. Cf. Miok et al. (2017) <doi:10.1002/bimj.201500269> and Miok et al. (2019) <doi:10.1002/bimj.201700195> for details on the implemented methods."
"qtlnet",1,1,7,"2018-04-05 23:50","Functions to Simultaneously Infer Causal Graphs and Genetic Architecture.  Includes acyclic and cyclic graphs for data from an experimental cross with a modest number (<10) of phenotypes driven by  a few genetic loci (QTL).  Chaibub Neto E, Keller MP, Attie AD, Yandell BS (2010)  Causal Graphical Models in Systems Genetics: a unified framework for joint inference of causal network and genetic architecture for correlated phenotypes.  Annals of Applied Statistics 4: 320-339.  <doi:10.1214/09-AOAS288>."
"postHoc",1,2,2,"2020-03-24 16:30","Implements a range of facilities for post-hoc analysis and             summarizing linear models, generalized linear models and              generalized linear mixed models, including grouping and clustering              via pairwise comparisons using graph representations and efficient             algorithms for finding maximal cliques of a graph.              Includes also non-parametric toos for post-hoc analysis.             It has S3 methods for printing summarizing, and producing plots,              line and barplots suitable for post-hoc analyses. "
"pcSteiner",1,2,1,"2020-08-31","The Prize-Collecting Steiner Tree problem asks to find a subgraph    connecting a given set of vertices with the most expensive nodes and least    expensive edges. Since it is proven to be NP-hard, exact and efficient algorithm    does not exist. This package provides convenient functionality for obtaining an    approximate solution to this problem using loopy belief propagation algorithm."
"parsec",0,2,7,"2019-03-30 17:30","Implements tools for the analysis of partially ordered data, with a particular focus on the evaluation of multidimensional systems of indicators and on the analysis of poverty. References, Fattore M. (2016) <doi:10.1007/s11205-015-1059-6> Fattore M., Arcagni A. (2016) <doi:10.1007/s11205-016-1501-4> Arcagni A. (2017) <doi:10.1007/978-3-319-45421-4_19>."
"PAGI",1,2,1,"2013-11-01","The package can identify the dysregulated KEGG pathways        based on global influence from the internal effect of pathways        and crosstalk between pathways. (1) The PAGI package can        prioritize the pathways associated with two biological states        by statistical significance or FDR. (2) The PAGI package can        evaluated the global influence factor (GIF) score in the global        gene-gene network constructed based on the relationships of        genes extracted from each pathway in KEGG database and the        overlapped genes between pathways."
"OutrankingTools",0,2,1,"2014-12-24","Functions to process ”outranking” ELECTRE methods existing in the literature. See, e.g.,			 <http://en.wikipedia.org/wiki/ELECTRE> about the outranking approach and the foundations of ELECTRE methods."
"optrees",0,2,1,"2014-09-02","Finds optimal trees in weighted graphs. In    particular, this package provides solving tools for minimum cost spanning    tree problems, minimum cost arborescence problems, shortest path tree    problems and minimum cut tree problem."
"optrcdmaeAT",0,1,1,"2017-04-12","Computes A-, MV-, D- and E-optimal or near-optimal row-column designs for two-colour cDNA microarray experiments using the linear fixed effects and mixed effects models where the interest is in a comparison of all pairwise treatment contrasts. The algorithms used in this package are based on the array exchange and treatment exchange algorithms adopted from Debusho, Gemechu and Haines (2016, unpublished) algorithms after adjusting for the row-column designs setup. The package also provides an optional method of using the graphical user interface (GUI) R package tcltk to ensure that it is user friendly."
"optbdmaeAT",0,1,1,"2017-02-09","Computes A-, MV-, D- and E-optimal or near-optimal block designs for two-colour cDNA microarray experiments using the linear fixed effects and mixed effects models where the interest is in a comparison of all possible elementary treatment contrasts. The algorithms used in this package are based on the treatment exchange and array exchange algorithms of Debusho, Gemechu and Haines (2016, unpublished). The package also provides an optional method of using the graphical user interface (GUI) R package tcltk to ensure that it is user friendly."
"NetSwan",0,1,1,"2015-11-01","A set of functions for studying network robustness, resilience and vulnerability. "
"netassoc",0,1,6,"2015-11-24 07:46","Infers species associations from community matrices. Uses local and (optional) regional-scale co-occurrence data by comparing observed partial correlation coefficients between species to those estimated from regional species distributions. Extends Gaussian graphical models to a null modeling framework. Provides interface to a variety of inverse covariance matrix estimation methods. "
"NEpiC",0,1,2,"2015-11-26 14:15","Package for a Network assisted algorithm for Epigenetic studies using mean and variance Combined signals: NEpiC. NEpiC combines both signals in mean and variance differences in methylation level between case and control groups searching for differentially methylated sub-networks (modules) using the protein-protein interaction network."
"multinet",0,1,17,"2020-05-09 00:50","Functions for the creation/generation and analysis of multilayer social networks."
"multichull",0,1,1,"2017-11-21","Given a set of models for which a measure of model (mis)fit and model complexity is provided, CHull(), developed by Ceulemans and Kiers (2006) <doi:10.1348/000711005X64817>, determines the models that are located on the boundary of the convex hull and selects an optimal model by means of the scree test values. "
"mRMRe",1,4,10,"2019-12-02 07:57","""Computes mutual information matrices from continuous, categorical and survival variables, as well as feature selection with minimum redundancy, maximum relevance (mRMR) and a new ensemble mRMR technique with DOI: N De Jay et al. (2013) <doi:10.1093/bioinformatics/btt383>."""
"modMax",0,2,2,"2015-02-18 13:58","The algorithms implemented here are used to detect the community structure of a network.             These algorithms follow different approaches, but are all based on the concept of modularity maximization."
"MNS",1,2,1,"2015-12-08","An implementation of the mixed neighbourhood selection (MNS) algorithm. The MNS algorithm can be used to estimate multiple related precision matrices. In particular, the motivation behind this work was driven by the need to understand functional connectivity networks across multiple subjects. This package also contains an implementation of a novel algorithm through which to simulate multiple related precision matrices which exhibit properties frequently reported in neuroimaging analysis. "
"micropan",1,2,8,"2020-01-19 19:30","A collection of functions for computations and visualizations of microbial pan-genomes."
"MetaLandSim",3,2,21,"2017-12-21 17:53","Tools to generate random landscape graphs, evaluate species    occurrence in dynamic landscapes, simulate future landscape occupation and    evaluate range expansion when new empty patches are available (e.g. as a    result of climate change)."
"MEGENA",1,2,5,"2017-01-10 19:03","Co-Expression Network Analysis by adopting network embedding technique. Song W.-M., Zhang B. (2015) Multiscale Embedded Gene Co-expression Network Analysis. PLoS Comput Biol 11(11): e1004574. <doi:10.1371/journal.pcbi.1004574>."
"manet",0,1,2,"2017-09-19 12:27","Mixture model with overlapping clusters for binary actor-event data. Parameters are estimated in a Bayesian framework. Model and inference are described in Ranciati, Vinciotti, Wit (2017) Modelling actor-event network data via a mixture model under overlapping clusters. Submitted."
"LPKsample",0,5,3,"2018-09-20 16:10","LP nonparametric high-dimensional K-sample comparison method that includes     (i) confirmatory test, (ii) exploratory analysis, and (iii) options to output a 	data-driven LP-transformed matrix for classification. The primary reference is 	Mukhopadhyay, S. and Wang, K. (2020, Biometrika); <arXiv:1810.01724>."
"locits",0,5,3,"2016-03-30 20:35","Provides test of second-order stationarity for time	series (for dyadic and arbitrary-n length data). Provides	localized autocovariance, with confidence intervals,	for locally stationary (nonstationary) time series."
"LncPath",1,5,2,"2016-05-16 14:45","Identifies pathways synergisticly regulated by the interested lncRNA(long non-coding RNA) sets based on a lncRNA-mRNA(messenger RNA) interaction network. 1) The lncRNA-mRNA interaction network was built from the protein-protein interactions and the lncRNA-mRNA co-expression relationships in 28 RNA-Seq data sets. 2) The interested lncRNAs can be mapped into networks as seed nodes and a random walk strategy will be performed to evaluate the rate of each coding genes influenced by the seed lncRNAs. 3) Pathways regulated by the lncRNA set will be evaluated by a weighted Kolmogorov-Smirnov statistic as an ES Score. 4) The p value and false discovery rate value will also be calculated through a permutation analysis. 5) The running score of each pathway can be plotted and the heat map of each pathway can also be plotted if an expression profile is provided. 6) The rank and scores of the gene list of each pathway can be printed."
"linkcomm",1,5,13,"2014-08-13 19:00","Link communities reveal the nested and overlapping structure in networks, and uncover the key nodes that form connections to multiple communities. linkcomm provides a set of tools for generating, visualizing, and analysing link communities in networks of arbitrary size and type. The linkcomm package also includes tools for generating, visualizing, and analysing Overlapping Cluster Generator (OCG) communities."
"LEANR",2,4,1,"2016-11-12","Implements the method described in ""Network-based analysis of omics data: The LEAN method"" [Gwinner Boulday (2016) <doi:10.1093/bioinformatics/btw676>] Given a protein interaction network and a list of p-values describing a measure of interest (as e.g. differential gene expression) this method computes an enrichment p-value for the protein neighborhood of each gene and compares it to a background distribution of randomly drawn p-values. The resulting scores are corrected for multiple testing and significant hits are returned in tabular format."
"JointNets",0,4,3,"2019-07-29 01:30","An end-to-end package for learning multiple sparse Gaussian graphical models and nonparanormal models from Heterogeneous Data with Additional Knowledge. It is able to simulate multiple related graphs as well as produce samples drawn from them. Multiple state-of-the-art sparse Gaussian graphical model estimators are included to both multiple and difference estimation. Graph visualization is available in 2D as well as 3D, designed specifically for brain. Moreover, a set of evaluation metrics are integrated for easy exploration with model validity. Finally, classification using graphical model is achieved with Quadratic Discriminant Analysis. The package comes with multiple demos with datasets from various fields. Methods references: SIMULE (Wang B et al. (2017) <doi:10.1007/s10994-017-5635-7>), WSIMULE (Singh C et al. (2017) <arXiv:1709.04090v2>), DIFFEE (Wang B et al. (2018) <arXiv:1710.11223>), JEEK (Wang B et al. (2018) <arXiv:1806.00548>), JGL(Danaher P et al. (2012) <arXiv:1111.0324>) and kdiffnet (Sekhon A et al, preprint for publication)."
"JGL",0,4,4,"2013-04-16 21:27","The Joint Graphical Lasso is a generalized method for        estimating Gaussian graphical models/ sparse inverse covariance        matrices/ biological networks on multiple classes of data.  We        solve JGL under two penalty functions: The Fused Graphical        Lasso (FGL), which employs a fused penalty to encourage inverse        covariance matrices to be similar across classes, and the Group        Graphical Lasso (GGL), which encourages similar network        structure between classes.  FGL is recommended over GGL for        most applications. Reference: Danaher P, Wang P, Witten DM. (2013)         <doi:10.1111/rssb.12033>."
"jeek",0,1,3,"2018-07-03 21:30","Provides a fast and scalable joint estimator for integrating additional knowledge in learning multiple related sparse Gaussian Graphical Models (JEEK). The JEEK algorithm can be used to fast estimate multiple related precision matrices in a large-scale. For instance, it can identify multiple gene networks from multi-context gene expression datasets. By performing data-driven network inference from high-dimensional and heterogeneous data sets, this tool can help users effectively translate aggregated data into knowledge that take the form of graphs among entities. Please run demo(jeek) to learn the basic functions provided by this package. For further details, please read the original paper: Beilun Wang, Arshdeep Sekhon, Yanjun Qi ""A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models"" (ICML 2018) <arXiv:1806.00548>."
"InteractiveIGraph",0,1,3,"2013-02-06 20:08","An extension of the package 'igraph'. This package create        possibly to work with 'igraph' objects interactively."
"iDINGO",0,1,5,"2019-09-06 11:20","Fits covariate dependent partial correlation matrices for integrative models to identify differential networks between two groups. The methods are described in Class et. al., (2018) <doi:10.1093/bioinformatics/btx750> and Ha et. al., (2015) <doi:10.1093/bioinformatics/btv406>."
"hglasso",0,1,3,"2014-05-05 07:16","Implements the hub graphical lasso and hub covariance graph proposal by Tan, KM., London, P., Mohan, K., Lee, S-I., Fazel, M., and Witten, D. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research. arXiv.org/pdf/1402.7349.pdf."
"graphkernels",0,4,7,"2018-04-26 20:37","A fast C++ implementation for computing various graph kernels including (1) simple kernels between vertex and/or edge label histograms, (2) graphlet kernels, (3) random walk kernels (popular baselines), and (4) the Weisfeiler-Lehman graph kernel (state-of-the-art)."
"gRapfa",0,4,1,"2014-04-11","gRapfa is for modelling discrete longitudinal data using acyclic probabilistic finite automata (APFA). The package contains functions for constructing APFA models from a given data using penalized likelihood methods. For graphical display of APFA models, gRapfa depends on 'igraph package'. gRapfa also contains an interface function to Beagle software that implements an efficient model selection algorithm. "
"GNAR",0,4,5,"2019-10-29 17:30","Simulation of, and fitting models for, Generalised Network Autoregressive (GNAR) time series models which take account of network structure.  Such models are described in Knight et al. (2016), see <arXiv:1603.03221>."
"genlasso",1,4,6,"2019-07-11 15:32","Computes the solution path for generalized lasso problems. Important use cases are the fused lasso over an arbitrary graph, and trend fitting of any given polynomial order. Specialized implementations for the latter two subproblems are given to improve stability and speed."
"gdistance",0,20,13,"2020-02-29 09:10","Provides classes and functions to calculate various distance measures and routes in heterogeneous geographic spaces represented as grids. The package implements measures to model dispersal histories first presented by van Etten and Hijmans (2010) <doi:10.1371/journal.pone.0012060>. Least-cost distances as well as more complex distances based on (constrained) random walks can be calculated. The distances implemented in the package are used in geographical genetics, accessibility indicators, and may also have applications in other fields of geospatial analysis."
"GADAG",0,1,1,"2017-04-11","Sparse large Directed Acyclic Graphs learning with a combination of a convex program and a tailored genetic algorithm (see Champion et al. (2017) <https://hal.archives-ouvertes.fr/hal-01172745v2/document>). "
"G1DBN",0,1,6,"2012-05-23 18:17","G1DBN performs DBN inference using 1st order conditional        dependencies."
"func2vis",0,3,1,"2020-08-10","Provides functions to have visualization and clean-up of enriched gene ontologies (GO) terms,     protein complexes and pathways (obtained from multiple databases) using 'ConsensusPathDB'     from gene set over-expression analysis. Performs clustering of pathway based on similarity     of over-expressed gene sets and visualizations similar to Ingenuity Pathway Analysis (IPA)     when up and down regulated genes are known. The methods are described in a paper currently    submitted by Orecchioni et al, 2020 in Nanoscale."
"FisHiCal",0,3,2,"2014-04-13 23:26","FisHiCal integrates Hi-C and FISH data, offering a modular and easy-to-use tool for chromosomal spatial analysis. "
"fastclime",1,3,12,"2016-04-19 01:03","Provides a method of recovering the precision matrix efficiently         and solving for the dantzig selector by applying the parametric         simplex method.  The computation is based on a linear optimization        solver. It also contains a generic LP solver and a parameterized LP         solver using parametric simplex method."
"fasjem",0,3,4,"2017-07-12 18:11","This is an R implementation of ""A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse Gaussian Graphical Models"" (FASJEM). The FASJEM algorithm can be used to estimate multiple related precision matrices. For instance, it can identify context-specific gene networks from multi-context gene expression datasets. By performing data-driven network inference from high-dimensional and heterogonous data sets, this tool  can help users effectively translate aggregated data into knowledge that take the form of graphs among entities. Please run demo(fasjem) to learn the basic functions provided by this package. For more details, please see <http://proceedings.mlr.press/v54/wang17e/wang17e.pdf>."
"ESEA",1,3,1,"2015-01-22","The package can identify the dysregulated canonical pathways by investigating the changes of biological relationships of pathways in the context of gene expression data. (1) The ESEA package constructs a background set of edges by extracting pathway structure (e.g. interaction, regulation, modification, and binding etc.) from the seven public databases (KEGG; Reactome; Biocarta; NCI; SPIKE; HumanCyc; Panther) and the edge sets of pathways for each of the above databases. (2) The ESEA package can can quantify the change of correlation between genes for each edge based on gene expression data with cases and controls. (3) The ESEA package uses the weighted Kolmogorov-Smirnov statistic to calculate an edge enrichment score (EES), which reflects the degree to which a given pathway is associated the specific phenotype. (4) The ESEA package can provide the visualization of the results."
"epistasis",0,3,2,"2016-11-07 17:22","An efficient multi-core package to reconstruct an underlying network of             genomic signatures of high-dimensional epistatic selection from              partially observed genotype data. The phenotype that we consider is viability. 			 The network captures the conditional dependent short- and long-range linkage 			 disequilibrium structure of genomes and thus reveals aberrant marker-marker 			 associations that are due to epistatic selection. We target on high-dimensional			 genotype data where number of variables (markers) is larger than number of 			 sample sizes (p >> n). The computations is memory-optimized using the sparse 			 matrix output."
"editrules",1,3,31,"2015-06-11 11:16","Facilitates reading and manipulating (multivariate) data restrictions    (edit rules) on numerical and categorical data. Rules can be defined with common R syntax    and parsed to an internal (matrix-like format). Rules can be manipulated with    variable elimination and value substitution methods, allowing for feasibility checks    and more. Data can be tested against the rules and erroneous fields can be found based    on Fellegi and Holt's generalized principle. Rules dependencies can be visualized with     using the 'igraph' package."
"EditImputeCont",0,1,8,"2019-05-29 19:20","An integrated editing and imputation method for continuous microdata under linear constraints is implemented. It relies on a Bayesian nonparametric hierarchical modeling approach as described in Kim et al. (2015) <doi:10.1080/01621459.2015.1040881>. In this approach, the joint distribution of the data is estimated by a flexible joint probability model. The generated edit-imputed data are guaranteed to satisfy all imposed edit rules, whose types include ratio edits, balance edits and range restrictions."
"ebdbNet",0,1,7,"2011-09-07 15:07","Infer the adjacency matrix of a	network from time course data using an empirical Bayes	estimation procedure based on Dynamic Bayesian Networks."
"DynComm",0,5,2,"2020-08-24 17:20","Used for evolving network analysis regarding   community detection. Implements several algorithms that calculate communities   for graphs whose nodes and edges change over time.  Edges, which can have new nodes, can be added or deleted. Changes in the   communities are calculated without recalculating communities for the entire   graph.  REFERENCE: M. Cordeiro et al. (2016) <doi:10.1007/s13278-016-0325-1>	G. Rossetti et al. (2017) <doi:10.1007/s10994-016-5582-8>	G. Rossetti (2017) <doi:10.1093/comnet/cnx016>	R. Sarmento (2019) <arXiv:1904.12593>."
"DrInsight",1,5,3,"2018-04-18 10:08","This is a connectivity mapping-based drug repurposing tool that identifies drugs that can potentially reverse query disease phenotype or have similar functions with query drugs."
"dpa",0,5,1,"2012-10-29","A GUI or command-line operated data analysis tool, for        analyzing time-dependent simulation data in which multiple        instantaneous or time-lagged relations are assumed. This        package uses Structural Equation Modeling (the sem package). It        is aimed to deal with time-dependent data and estimate whether        a causal diagram fits data from an (agent-based) simulation        model."
"dnet",0,5,19,"2020-01-23 14:40","The focus of the dnet by Fang and Gough (2014) <doi:10.1186/s13073-014-0064-8> is to make sense of omics data (such as gene expression and mutations) from different angles including: integration with molecular networks, enrichments using ontologies, and relevance to gene evolutionary ages. Integration is achieved to identify a gene subnetwork from the whole gene network whose nodes/genes are labelled with informative data (such as the significant levels of differential expression or survival risks). To help make sense of identified gene networks, enrichment analysis is also supported using a wide variety of pre-compiled ontologies and phylostratific gene age information in major organisms including: human, mouse, rat, chicken, C.elegans, fruit fly, zebrafish and arabidopsis. Add-on functionalities are supports for calculating semantic similarity between ontology terms (and between genes) and for calculating network affinity based on random walk; both can be done via high-performance parallel computing."
"disparityfilter",0,1,2,"2015-02-19 22:34","The disparity filter algorithm is a network reduction technique to    identify the 'backbone' structure of a weighted network without destroying    its multi-scale nature. The algorithm is documented in M. Angeles Serrano,    Marian Boguna and Alessandro Vespignani in ""Extracting the multiscale    backbone of complex weighted networks"", Proceedings of the National Academy    of Sciences 106 (16), 2009. This implementation of the algorithm supports    both directed and undirected networks."
"dils",0,1,2,"2013-11-01 08:31","Combine multiple-relationship networks into a single weighted    network.  The approach is similar to factor analysis in the that    contribution from each constituent network varies so as to maximize the    information gleaned from the multiple-relationship networks.    This implementation uses Principal Component Analysis calculated using    'prcomp' with bootstrap subsampling.  Missing links are imputed using    the method of Chen et al.    (2012)."
"diffee",0,1,2,"2018-03-06 19:59","This is an R implementation of Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model Structure (DIFFEE). The DIFFEE algorithm can be used to fast estimate the differential network between two related datasets. For instance, it can identify differential gene network from datasets of case and control. By performing data-driven network inference from two high-dimensional data sets, this tool can help users effectively translate two aggregated data blocks into knowledge of the changes among entities between two Gaussian Graphical Model. Please run demo(diffeeDemo) to learn the basic functions provided by this package. For further details, please read the original paper: Beilun Wang, Arshdeep Sekhon, Yanjun Qi (2018) <arXiv:1710.11223>."
"DiffCorr",0,1,1,"2015-04-02","A method for identifying pattern changes between 2 experimental         conditions in correlation networks (e.g., gene co-expression networks),         which builds on a commonly used association measure, such as Pearson's         correlation coefficient. This package includes functions to calculate         correlation matrices for high-dimensional dataset and to test         differential correlation, which means the changes in the correlation         relationship among variables (e.g., genes and metabolites) between 2         experimental conditions. "
"Diderot",0,1,4,"2017-11-06 07:17","Enables the user to build a citation network/graph from bibliographic data and, based on modularity and heterocitation metrics, assess the degree of awareness/cross-fertilization between two corpora/communities. This toolset is optimized for Scopus data. "
"dcGOR",0,1,6,"2015-07-20 19:19","There lacks a package for analysing domain-centric ontologies and annotations, particularly those in the dcGO database. The dcGO (http://supfam.org/SUPERFAMILY/dcGO) is a comprehensive domain-centric database for annotating protein domains using a panel of ontologies including Gene Ontology. With the package, users are expected to analyse and visualise domain-centric ontologies and annotations. Supported analyses include but are not limited to: easy access to a wide range of ontologies and their domain-centric annotations; able to build customised ontologies and annotations; domain-based enrichment analysis and visualisation; construction of a domain (semantic similarity) network according to ontology annotations; significance analysis for estimating a contact (statistical significance) network via Random Walk with Restart; and high-performance parallel computing. The new functionalities are: 1) to create domain-centric ontologies; 2) to predict ontology terms for input protein sequences (precisely domain content in the form of architectures) plus to assess the predictions; 3) to reconstruct ancestral discrete characters using maximum likelihood/parsimony."
"dc3net",0,1,3,"2015-10-02 00:31","Performs differential network analysis to infer disease specific gene networks."
"cvxclustr",0,1,3,"2014-07-26 08:27","Alternating Minimization Algorithm (AMA) and Alternating Direction    Method of Multipliers (ADMM) splitting methods for convex clustering."
"cvxbiclustr",0,1,1,"2015-06-28","An iterative algorithm for solving a convex    formulation of the biclustering problem."
"corkscrew",0,1,1,"2015-10-31","Includes binning categorical variables into lesser number of categories based on t-test, converting categorical variables into continuous features 	using the mean of the response variable for the respective categories, understanding the relationship between the response variable and predictor variables 	using data transformations."
"coreCT",1,1,5,"2019-09-23 22:30","Computed tomography (CT) imaging is a powerful tool for understanding the composition of sediment cores. This package streamlines and accelerates the analysis of CT data generated in the context of environmental science. Included are tools for processing raw DICOM images to characterize sediment composition (sand, peat, etc.). Root analyses are also enabled, including measures of external surface area and volumes for user-defined root size classes. For a detailed description of the application of computed tomography imaging for sediment characterization, see: Davey, E., C. Wigand, R. Johnson, K. Sundberg, J. Morris, and C. Roman. (2011) <doi:10.1890/10-2037.1>."
"corclass",0,1,3,"2013-12-10 08:36","Perform a correlational class analysis of the data, resulting in a partition of the data into separate modules."
"cooptrees",0,1,1,"2014-09-04","Computes several cooperative games and allocation rules associated with minimum cost spanning tree problems and minimum cost arborescence problems."
"clickstream",0,1,15,"2017-12-14 11:31","A set of tools to read, analyze and write lists of click sequences    on websites (i.e., clickstream). A click can be represented by a number,    character or string. Clickstreams can be modeled as zero- (only computes    occurrence probabilities), first- or higher-order Markov chains."
"cglasso",0,1,4,"2019-06-11 09:20","The l1-penalized censored Gaussian graphical model is an extension of the graphical lasso estimator developed to handle datasets with censored observations. An EM-like algorithm is implemented to estimate the parameters of the censored Gaussian graphical models."
"centiserve",0,1,1,"2017-07-15","Calculates centrality indices additional to the 'igraph' package centrality functions."
"cccd",0,2,7,"2013-10-29 16:58","Class Cover Catch Digraphs, neighborhood graphs, and        relatives."
"causaloptim",3,4,3,"2020-03-25 19:30","When causal quantities are not identifiable from the observed data, it still may be possible             to bound these quantities using the observed data. We outline a class of problems for which the             derivation of tight bounds is always a linear programming problem and can therefore, at least             theoretically, be solved using a symbolic linear optimizer. We extend and generalize the             approach of Balke and Pearl (1994) <doi:10.1016/B978-1-55860-332-5.50011-0> and we provide             a user friendly graphical interface for setting up such problems via directed acyclic             graphs (DAG), which only allow for problems within this class to be depicted. The user can             then define linear constraints to further refine their assumptions to meet their specific             problem, and then specify a causal query using a text interface. The program converts this             user defined DAG, query, and constraints, and returns tight bounds. The bounds can be             converted to R functions to evaluate them for specific datasets, and to latex code for             publication. The methods and proofs of tightness and validity of the bounds are described in            a preprint by Sachs, Gabriel, and Sjölander (2020)             <https://sachsmc.github.io/causaloptim/articles/CausalBoundsMethods.pdf>."
"c3net",0,4,2,"2011-01-27 10:53","This package allows inferring gene regulatory networks        with direct physical interactions from microarray expression        data using C3NET."
"brainGraph",0,1,11,"2019-11-07 14:40","A set of tools for performing graph theory analysis of brain MRI    data. It works with data from a Freesurfer analysis (cortical thickness,    volumes, local gyrification index, surface area), diffusion tensor    tractography data (e.g., from FSL) and resting-state fMRI data (e.g., from    DPABI). It contains a graphical user interface for graph visualization and    data exploration, along with several functions for generating useful    figures."
"Boptbd",0,2,1,"2020-01-13","Computes Bayesian A- and D-optimal block designs under the linear mixed effects model settings 	using block/array exchange algorithm of Debusho, Gemechu and Haines (2018) <doi:10.1080/03610918.2018.1429617> where the interest is in a 	comparison of all possible elementary treatment contrasts. The package also provides an optional 	method of using the graphical user interface (GUI) R package 'tcltk' to ensure that it is user friendly."
"bnstruct",1,2,8,"2015-07-30 09:59","Bayesian Network Structure Learning from Data with Missing Values.    The package implements the Silander-Myllymaki complete search,    the Max-Min Parents-and-Children, the Hill-Climbing, the    Max-Min Hill-climbing heuristic searches, and the Structural    Expectation-Maximization algorithm. Available scoring functions are    BDeu, AIC, BIC. The package also implements methods for generating and using    bootstrap samples, imputed data, inference."
"BNSL",0,2,5,"2017-09-18 09:06","From a given data frame, this package learns its Bayesian network structure based on a selected score."
"bc3net",0,2,5,"2015-12-31 00:35","Implementation of the BC3NET algorithm for gene regulatory network inference (de Matos Simoes and Frank Emmert-Streib, Bagging Statistical Network Inference from Large-Scale Gene Expression Data, PLoS ONE 7(3): e33624, <doi:10.1371/journal.pone.0033624>)."
"bayesloglin",1,6,2,"2016-12-19 08:33","The function MC3() searches for log-linear models with the highest posterior probability. The function gibbsSampler() is a blocked Gibbs sampler for sampling from the posterior distribution of the log-linear parameters. The functions findPostMean() and findPostCov() compute the posterior mean and covariance matrix for decomposable models which, for these models, is available in closed form."
"AurieLSHGaussian",0,6,2,"2017-09-13 10:27","Uses locality sensitive hashing and creates a neighbourhood graph for a data set and calculates the adjusted rank index value for the same. It uses Gaussian random planes to decide the nature of a given point. Datar, Mayur, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni(2004) <doi:10.1145/997817.997857>."
"labdsv",0,6,12,"2016-01-24 15:45","A variety of ordination and community analyses   useful in analysis of data sets in community ecology.     Includes many of the common ordination methods, with    graphical routines to facilitate their interpretation,    as well as several novel analyses."
"yaImpute",0,8,32,"2019-01-09 20:30","Performs nearest neighbor-based imputation using one or more alternative  approaches to processing multivariate data. These include methods based on canonical  correlation analysis, canonical correspondence analysis, and a multivariate adaptation  of the random forest classification and regression techniques of Leo Breiman and Adele  Cutler. Additional methods are also offered. The package includes functions for  comparing the results from running alternative techniques, detecting imputation targets  that are notably distant from reference observations, detecting and correcting  for bias, bootstrapping and building ensemble imputations, and mapping results."
"WeightedCluster",3,4,6,"2019-03-21 11:23","Clusters state sequences and weighted data. It provides an optimized weighted PAM algorithm as well as functions for aggregating replicated cases, computing cluster quality measures for a range of clustering solutions and plotting clusters of state sequences."
"vegtable",0,4,7,"2020-01-13 20:30","Import and handling data from vegetation-plot databases, especially    data stored in 'Turboveg' (<https://www.synbiosys.alterra.nl/turboveg>).    Also import/export routines for exchange of data with 'Juice'    (<http://www.sci.muni.cz/botany/juice>) are implemented."
"vegdata",1,4,41,"2016-07-15 10:31","Handling of vegetation data from different sources (	Turboveg <http://www.synbiosys.alterra.nl/turboveg/>; 	the German national repository <http://www.vegetweb.de> and others.     Taxonomic harmonization (given appropriate taxonomic lists,    e.g. the German taxonomic standard list ""GermanSL"", <http://germansl.infinitenature.org>)."
"themetagenomics",2,22,4,"2020-04-14 11:00","A means to explore the structure of 16S rRNA surveys using a Structural   Topic Model coupled with functional prediction. The user provides an abundance   table, sample metadata, and taxonomy information, and themetagenomics infers   associations between topics and sample features, as well as topics and predicted   functional content. Functional prediction can be accomplished via Tax4Fun (for   Silva references) and PICRUSt (for GreenGeenes references). See   <doi:10.1371/journal.pone.0219235>."
"taxize",4,22,44,"2020-06-09 08:00","Interacts with a suite of web 'APIs' for taxonomic tasks,    such as getting database specific taxonomic identifiers, verifying    species names, getting taxonomic hierarchies, fetching downstream and    upstream taxonomic names, getting taxonomic synonyms, converting    scientific to common names and vice versa, and more."
"spaa",0,1,6,"2013-08-23 10:05","Miscellaneous functions for analysing species association        and niche overlap."
"shipunov",0,1,12,"2020-07-10 11:10","A collection of functions for data manipulation, plotting and statistical computing, to use separately or with the book ""Visual Statistics. Use R!"": Shipunov (2020) <http://ashipunov.info/shipunov/software/r/r-en.htm>. Most useful functions: Bclust(), Jclust() and BootA() which bootstrap hierarchical clustering; Recode() which does multiple recoding in a fast, simple and flexible way; Misclass() which outputs confusion matrix even if classes are not concerted; Overlap() which measures group separation on any projection; Biarrows() which converts any scatterplot into biplot; and Pleiad() which is fast and flexible correlogram."
"sads",1,1,10,"2017-10-24 23:50","Maximum likelihood tools to fit and compare models of species    abundance distributions and of species rank-abundance distributions."
"RRphylo",7,5,6,"2020-07-14 02:00","Functions for phylogenetic analysis (Castiglione et al, 2018 <doi:10.1111/2041-210X.12954>). The functions perform the estimation of phenotypic evolutionary rates, identification of phenotypic evolutionary rate shifts, quantification of direction and size of evolutionary change in multivariate traits, the computation of ontogenetic shape vectors and test for morphological convergence."
"Rraven",1,5,11,"2020-02-12 17:50","A tool to exchange data between R and 'Raven' sound analysis software (Cornell Lab of Ornithology). Functions work on data formats compatible with the R package 'warbleR'."
"raptr",1,5,10,"2020-03-11 12:20","Biodiversity is in crisis. The overarching aim of conservation    is to preserve biodiversity patterns and processes. To this end, protected    areas are established to buffer species and preserve biodiversity processes.    But resources are limited and so protected areas must be cost-effective.    This package contains tools to generate plans for protected areas    (prioritizations), using spatially explicit targets for biodiversity    patterns and processes. To obtain solutions in a feasible amount  of time,    this package uses the commercial 'Gurobi' software package (obtained from    <http://www.gurobi.com/>). For more information on using    this package, see Hanson et al. (2018) <doi:10.1111/2041-210X.12862>."
"propr",6,5,21,"2019-07-10 07:00","The bioinformatic evaluation of gene co-expression often begins with    correlation-based analyses. However, correlation lacks validity when    applied to relative data, including count data generated by next-generation    sequencing. This package implements several metrics for proportionality, including    phi [Lovell et al (2015) <doi:10.1371/journal.pcbi.1004075>] and    rho [Erb and Notredame (2016) <doi:10.1007/s12064-015-0220-8>]. This package also    implements several metrics for differential proportionality. Unlike correlation,    these measures give the same result for both relative and absolute data."
"ProcMod",0,28,3,"2020-09-21 16:40","Estimates corrected Procrustean correlation between matrices for removing overfitting effect. Coissac Eric and Gonindard-Melodelima Christelle (2019) <bioarXiv:10.1101/842070>."
"primer",0,28,4,"2012-05-16 13:49","Functions are primarily functions for systems of ordinary differential equations, difference equations, and eigenanalysis and projection of demographic matrices;     data are for examples. Documentation of methods is provided in     Stevens, MHH (2009, <https://www.springer.com/gp/book/9780387898810>)."
"phylotools",0,28,5,"2012-10-29 08:59","A collection of tools for building RAxML supermatrix using             PHYLIP or aligned FASTA files. These functions will be             useful for building large phylogenies using multiple markers."
"permute",1,28,10,"2016-09-09 10:13","A set of restricted permutation designs for freely exchangeable, line transects (time series), and spatial grid designs plus permutation of blocks (groups of samples) is provided. 'permute' also allows split-plot designs, in which the whole-plots or split-plots or both can be freely-exchangeable or one of the restricted designs. The 'permute' package is modelled after the permutation schemes of 'Canoco 3.1' (and later) by Cajo ter Braak."
"MiRKAT",1,2,4,"2020-04-14 10:50","Test for overall association between microbiome composition data and phenotypes via phylogenetic kernels.   The phenotype can be univariate continuous or binary (Zhao et al. (2015) <doi:10.1016/j.ajhg.2015.04.003>),   survival outcomes (Plantinga et al. (2017) <doi:10.1186/s40168-017-0239-9>),   multivariate (Zhan et al. (2017) <doi:10.1002/gepi.22030>) and   structured phenotypes (Zhan et al. (2017) <doi:10.1111/biom.12684>).   The package can also use robust and quantile regression (Fu et al. (2020+), in preparation).   In each case, the microbiome community effect is modeled nonparametrically through a kernel function,   which can incorporate phylogenetic tree information. "
"mefa",1,2,28,"2015-11-22 11:48","A framework package aimed to provide standardized computational environment for specialist work via object classes to represent the data coded by samples, taxa and segments (i.e. subpopulations, repeated measures). It supports easy processing of the data along with cross tabulation and relational data tables for samples and taxa. An object of class ‘mefa’ is a project specific compendium of the data and can be easily used in further analyses. Methods are provided for extraction, aggregation, conversion, plotting, summary and reporting of ‘mefa’ objects. Reports can be generated in plain text or LaTeX format. Vignette contains worked examples."
"loe",0,2,2,"2014-01-31 12:04","Local Ordinal embedding (LOE) is one of graph embedding methods for unweighted graphs."
"liayson",1,3,3,"2019-04-28 23:40","Given an RNA-seq derived cell-by-gene matrix and an DNA-seq derived copy number segmentation, LIAYSON predicts the number of clones present in a tumor, their size, the copy number profile of each clone and the clone membership of each single cell (Andor, N. & Lau, B., et al. (2018) <doi:10.1101/445932>)."
"idar",0,3,2,"2017-05-04 09:22","Computes and tests individual (species, phylogenetic and functional) diversity-area relationships, i.e., how species-, phylogenetic- and functional-diversity varies with spatial scale around the individuals of some species in a community. See applications of these methods in Wiegand et al. (2007) <doi:10.1073/pnas.0705621104> or Chacon-Labella et al. (2016) <doi:10.1007/s00442-016-3547-z>."
"GCalignR",2,3,5,"2018-07-16 12:00","Aligns peak based on peak retention times and matches homologous peaks    across samples. The underlying alignment procedure comprises three sequential steps.    (1) Full alignment of samples by linear transformation of retention times to     maximise similarity among homologous peaks (2) Partial alignment of peaks within     a user-defined retention time window to cluster homologous peaks (3) Merging rows    that are likely representing homologous substances (i.e. no sample shows peaks in     both rows and the rows have similar retention time means). The algorithm is described in detail    in Ottensmann et al., 2018 <doi:10.1371/journal.pone.0198311>. "
"epiphy",2,3,1,"2018-05-24","A toolbox to make it easy to analyze plant disease epidemics. It    provides a common framework for plant disease intensity data recorded over    time and/or space. Implemented statistical methods are currently mainly    focused on spatial pattern analysis (e.g., aggregation indices, Taylor and    binary power laws, distribution fitting, SADIE and mapcomp methods). See    Madden LV, Hughes G, van den Bosch F (2007, ISBN: 978-089054-354-2) for    further information on these methods. Several data sets that were mainly    published in plant disease epidemiology literature are also included in this    package."
"enveomics.R",0,3,21,"2019-11-20 00:20","A collection of functions for microbial ecology and other   applications of genomics and metagenomics. Companion package for the   Enveomics Collection (Rodriguez-R, L.M. and Konstantinidis, K.T., 2016   <doi:10.7287/peerj.preprints.1900v1>)."
"eHOF",1,3,14,"2017-01-06 11:29","Extended and enhanced hierarchical logistic regression models (called Huisman-Olff-Fresco in biology, see Huisman et al. 1993 Journal of Vegetation Science <doi:10.1111/jvs.12050>) models. Response curves along one-dimensional gradients including no response, monotone, plateau, unimodal and bimodal models. "
"ecospace",1,3,7,"2018-03-05 19:56","Implements stochastic simulations of community assembly (ecological    diversification) using customizable ecospace frameworks (functional trait    spaces). Provides a wrapper to calculate common ecological disparity and    functional ecology statistical dynamics as a function of species richness.    Functions are written so they will work in a parallel-computing environment."
"ecolottery",2,3,1,"2017-07-03","Coalescent-Based Simulation of Ecological Communities as proposed    by Munoz et al. (2017) <doi:10.13140/RG.2.2.31737.26728>. The package includes    a tool for estimating parameters of community assembly by using Approximate     Bayesian Computation."
"EcoIndR",0,3,6,"2019-03-10 21:32","Calculates several indices, such as of diversity, fluctuation, etc., and they are used to estimate ecological indicators."
"EcoGenetics",0,3,13,"2017-03-03 18:01","Management and exploratory analysis of spatial data in landscape genetics. Easy integration of information from multiple sources with ""ecogen"" objects."
"divDyn",1,3,4,"2019-02-19 09:20","Functions to describe sampling and diversity dynamics of fossil occurrence datasets (e.g. from the Paleobiology Database). The package includes methods to calculate range- and occurrence-based metrics of taxonomic richness, extinction and origination rates, along with traditional sampling measures. A powerful subsampling tool is also included that implements frequently used sampling standardization methods in a multiple bin-framework. The plotting of time series and the occurrence data can be simplified by the functions incorporated in the package, as well other calculations, such as environmental affinities and extinction selectivity testing. Details can be found in: Kocsis, A.T.; Reddin, C.J.; Alroy, J. and Kiessling, W. (2019) <doi:10.1101/423780>."
"dimRed",1,3,6,"2018-11-13 18:50","A collection of dimensionality reduction    techniques from R packages and a common    interface for calling the methods."
"codep",0,3,9,"2018-04-16 18:59","Computation of Multiscale Codependence Analysis and spatial eigenvector maps, as an additional feature."
"BIRDS",3,3,1,"2020-03-20","It helps making the evaluation and preparation of biodiversity data   easy, systematic and reproducible. It also helps the users to overlay the    point observations into a custom grid that is useful for further analysis.    The review summarise statistics that helps evaluate whether a set of species    observations is fit-for-use and take decisions upon its use of on further    analyses. It does so by quantifying the sampling effort (amount of effort   expended during an event) and data completeness (data gaps) to help judge    whether the data is representative, valid and fit for any intended purpose.    The 'BIRDS' package is most useful when working with heterogeneous data sets    with variation in the sampling process, i.e. where data have been collected    and reported in various ways and therefore varying in sampling effort    and data completeness (i.e. how well the reported observations describe the    true state). Primary biodiversity data (PBD) combining data from different    data sets, like e.g. Global Biodiversity Information Facility (GBIF) mediated   data, commonly vary in the ways data has been generated - containing    opportunistically collected presence-only data together with and data from    systematic monitoring programs. The set of tools provided is aimed at    understanding the process that generated the data (i.e. observing, recording    and reporting species into databases). There is a non-vital function on this    package (makeDggrid()) that depends the package 'dggridR' that is no longer on CRAN.    You can find it here <https://github.com/r-barnes/dggridR>. References:    Ruete (2015) <doi:10.3897/BDJ.3.e5361>; Szabo, Vesk, Baxter & Possingham (2010)    <doi:10.1890/09-0877.1>; Telfer, Preston 6 Rothery (2002) <doi:10.1016/S0006-3207(02)00050-2>."
"bipartiteD3",1,3,2,"2018-11-26 21:10","Generates interactive bipartite graphs using the D3 library.    Designed for use with the 'bipartite' analysis package.    Sources open source 'vis-js' library (<http://visjs.org/>).    Adapted from examples at <https://bl.ocks.org/NPashaP> (released under GPL-3)."
"betapart",0,3,9,"2018-10-18 13:40","Functions to compute pair-wise dissimilarities (distance matrices) and multiple-site dissimilarities, separating the turnover and nestedness-resultant components of taxonomic (incidence and abundance based), functional and phylogenetic beta diversity."
"bcp",0,3,28,"2018-06-08 23:37","Provides an implementation of the Barry and Hartigan (1993) product    partition model for the normal errors change point problem using Markov Chain    Monte Carlo. It also extends the methodology to regression models on a connected    graph (Wang and Emerson, 2015); this allows estimation of change point models    with multivariate responses. Parallel MCMC, previously available in bcp v.3.0.0,    is currently not implemented."
"balance",1,1,3,"2018-12-10 10:20","Balances have become a cornerstone of compositional data analysis. However, conceptualizing balances is difficult, especially for high-dimensional data. Most often, investigators visualize balances with ""balance dendrograms"". However, this visualization tool does not scale well for large data. This package provides an alternative scheme for visualizing balances, described in [Quinn (2018) <doi:10.12688/f1000research.15858.1>]. This package also provides a method for principal balance analysis."
"ALA4R",1,1,6,"2019-04-02 07:20","The Atlas of Living Australia (ALA) provides tools to enable users    of biodiversity information to find, access, combine and visualise data on    Australian plants and animals; these have been made available from    <https://ala.org.au/>. ALA4R provides a subset of the tools to be    directly used within R. It enables the R community to directly access data    and resources hosted by the ALA."
"abdiv",0,1,1,"2020-01-20","A collection of measures for measuring ecological diversity.  Ecological diversity comes in two flavors: alpha diversity measures the  diversity within a single site or sample, and beta diversity measures the  diversity across two sites or samples. This package overlaps considerably  with other R packages such as 'vegan', 'gUniFrac', 'betapart', and 'fossil'.  We also include a wide range of functions that are implemented in software  outside the R ecosystem, such as 'scipy', 'Mothur', and 'scikit-bio'.  The  implementations here are designed to be basic and clear to the reader."
"zetadiv",0,1,6,"2018-05-17 10:46","Functions to compute compositional turnover using zeta-diversity,    the number of species shared by multiple assemblages. The package includes    functions to compute zeta-diversity for a specific number of    assemblages and to compute zeta-diversity for a range of numbers of    assemblages. It also includes functions to explain how zeta-diversity    varies with distance and with differences in environmental variables    between assemblages, using generalised linear models, linear models    with negative constraints, generalised additive models,shape    constrained additive models, and I-splines."
"vegclust",3,1,8,"2018-05-29 09:37","A set of functions to: (1) perform fuzzy clustering of vegetation data [De Caceres et al. (2010) <doi:10.1111/j.1654-1103.2010.01211.x>]; (2) to assess ecological community ressemblance on the basis of structure and composition [De Caceres et al. (2013): <doi:10.1111/2041-210X.12116>]; and (3) to perform community trajectory analysis [De Caceres et al. (2019): <doi:10.1002/ecm.1350>]."
"vcfR",4,8,13,"2020-06-05 14:50","Facilitates easy manipulation of variant call format (VCF) data.    Functions are provided to rapidly read from and write to VCF files. Once    VCF data is read into R a parser function extracts matrices of data. This    information can then be used for quality control or other purposes. Additional    functions provide visualization of genomic data. Once processing is complete    data may be written to a VCF file (*.vcf.gz). It also may be converted into    other popular R objects (e.g., genlight, DNAbin). VcfR provides a link between    VCF data and familiar R software."
"tspmeta",0,1,3,"2014-02-25 22:33","Instance feature calculation and evolutionary instance generation    for the traveling salesman problem. Also contains code to ""morph"" two TSP    instances into each other. And the possibility to conveniently run a couple    of solvers on TSP instances."
"tidyMicro",1,1,2,"2020-03-28 15:20","A reliable alternative to popular microbiome analysis R packages. We provide standard tools as well as novel extensions on standard analyses to improve interpretability and the analyst’s ability to communicate results, all while maintaining object malleability to encourage open source collaboration."
"SYNCSA",0,1,9,"2013-02-22 15:03","Analysis of metacommunities based on functional traits and    phylogeny of the community components. The functions that are offered here    implement for the R environment methods that have been available in the    SYNCSA application written in C++ (by Valerio Pillar, available at     <http://ecoqua.ecologia.ufrgs.br/SYNCSA.html>)."
"SSP",1,1,1,"2020-03-28","Simulation-based sampling protocol (SSP) is an R package design to estimate sampling effort in studies of    ecological communities based on the definition of pseudo-multivariate standard error (MultSE) (Anderson & Santana-Garcon, 2015) <doi:10.1111/ele.12385> and simulation    of ecological data. The theoretical background is described in Guerra-Castro et al. (2020) <doi:10.1101/2020.03.19.996991>."
"spmoran",2,1,15,"2020-05-20 16:20","Functions for estimating Moran eigenvector-based    spatial additive mixed models, and other spatial regression models.    For details see Murakami (2020) <arXiv:1703.04467>."
"SparseLPM",0,1,1,"2018-08-31","Models the nonnegative entries of a rectangular adjacency matrix using a sparse latent position model, as illustrated in Rastelli, R. (2018) ""The Sparse Latent Position Model for nonnegative weighted networks"" <arXiv:1808.09262>."
"spaceNet",0,1,3,"2018-03-22 15:21","Latent space models for multivariate networks (multiplex) estimated via MCMC algorithm. See D Angelo et al. (2018) <arXiv:1803.07166> and D Angelo et al. (2018) <arXiv:1807.03874>."
"soundecology",3,1,8,"2015-05-02 17:55","Functions to calculate indices for soundscape ecology and other ecology research that uses audio recordings."
"smartR",2,1,2,"2018-12-01 00:40","A tool for assessing bio-economic feedback in   different management scenarios (D'Andrea et al., 2020   <doi:10.1111/2041-210X.13394>). 'smartR' (Spatial Management and Assessment   of demersal Resources for Trawl fisheries) combines information from different   tasks gathered within the European Data Collection Framework for the fishery  sector. The 'smartR' package implements the SMART model (Russo et al., 2014   <doi:10.1371/journal.pone.0086222>), through the object-oriented   programming paradigm, and within this package it is possible to achieve the   complete set of analyses required by the SMART approach: from the editing and   formatting of the raw data; the construction and maintenance of coherent   datasets; the numerical and visual inspection of the generated metadata; to   the final simulation of management scenarios and the forecast of their   effects. The interaction between the user and the application could take   place through invocation of methods via the command line or could be entirely   committed to the graphical user interfaces (GUI)."
"SigTree",1,1,10,"2017-02-08 01:28","Provides tools to identify and visualize branches in a phylogenetic tree that are significantly responsive to some intervention, taking as primary inputs a phylogenetic tree (of class phylo) and a data frame (or matrix) of corresponding tip (OTU) labels and p-values."
"sharpshootR",0,1,8,"2019-11-21 00:40","Miscellaneous soil data management, summary, visualization, and conversion utilities to support soil survey."
"shapeR",0,2,2,"2015-04-03 22:48","Studies otolith shape variation among fish populations.   Otoliths are calcified structures found in the inner ear of teleost fish and their shape has   been known to vary among several fish populations and stocks, making them very useful in taxonomy,   species identification and to study geographic variations. The package extends previously described   software used for otolith shape analysis by allowing the user to automatically extract closed   contour outlines from a large number of images, perform smoothing to eliminate pixel noise,   choose from conducting either a Fourier or wavelet transform to the outlines and visualize   the mean shape. The output of the package are independent Fourier or wavelet coefficients   which can be directly imported into a wide range of statistical packages in R. The package   might prove useful in studies of any two dimensional objects."
"RVenn",1,2,2,"2019-05-08 12:30","Set operations for many sets. The base functions for set operations    in R can be used for only two sets. This package uses 'purr' to find the    union, intersection and difference of three or more sets. This package also    provides functions for pairwise set operations among several sets. Further,     based on 'ggplot2' and 'ggforce', a Venn diagram can be drawn for two or     three sets. For bigger data sets, a clustered heatmap showing presence/absence     of the elements of the sets can be drawn based on the 'pheatmap' package.     Finally, enrichment test can be applied to two sets whether an overlap is     statistically significant or not."
"RVAideMemoire",0,2,49,"2011-03-14 08:37","Contains miscellaneous functions useful in biostatistics, mostly univariate and multivariate testing procedures with a special emphasis on permutation tests. Many functions intend to simplify user's life by shortening existing procedures or by implementing plotting functions that can be used with as many methods from different packages as possible."
"RTCC",0,2,2,"2020-03-31 12:00","The Randomized Trait Community Clustering method (Triado-Margarit et al., 2019,    <doi:10.1038/s41396-019-0454-4>) is a statistical approach which allows to determine whether    if an observed trait clustering pattern is related to an increasing environmental constrain.    The method 1) determines whether exists or not a trait clustering on the sampled communities    and 2) assess if the observed clustering signal is related or not to an increasing environmental    constrain along an environmental gradient. Also, when the effect of the environmental gradient    is not linear, allows to determine consistent thresholds on the community assembly based on trait-values."
"rioja",0,2,21,"2017-06-18 15:35","Constrained clustering, transfer functions, and other methods for analysing Quaternary science data."
"rich",1,2,4,"2012-11-21 17:31","Computes rarefaction curves, cumulated and mean species        richness. Compares these estimates by means of randomization        tests."
"recluster",0,2,8,"2015-03-06 21:11","The analysis of different aspects of biodiversity requires specific algorithms. 	For example, in regionalisation analyses, the high frequency of ties and zero values in 	dissimilarity matrices produced by Beta-diversity turnover produces hierarchical 	cluster dendrograms whose topology and bootstrap supports are affected by the order of 	rows in the original matrix. Moreover, visualisation of biogeographical regionalisation 	can be facilitated by a combination of hierarchical clustering and multi-dimensional 	scaling. The recluster package provides robust techniques to visualise and analyse 	pattern of biodiversity and to improve occurrence data for cryptic taxa. 	Other functions 	related to recluster (e.g. the biodecrypt family) are currently 	available in GitHub at <https://github.com/leondap/recluster>."
"Rarefy",0,2,1,"2020-05-27","Includes functions for the calculation of spatially and non-spatially explicit rarefaction curves using different indices of taxonomic, functional and phylogenetic diversity. The user can also rarefy any biodiversity metric as provided by a self-written function (or an already existent one) that gives as output a vector with the values of a certain index of biodiversity calculated per plot (Ricotta, C., Acosta, A.,  Bacaro, G., Carboni, M., Chiarucci, A.,  Rocchini, D., Pavoine, S. (2019) <doi:10.1016/j.ecolind.2019.105606>; Bacaro, G., Altobelli, A., Cameletti, M., Ciccarelli, D., Martellos, S., Palmer, M. W., … Chiarucci, A. (2016) <doi:10.1016/j.ecolind.2016.04.026>; Bacaro, G., Rocchini, D., Ghisla, A., Marcantonio, M., Neteler, M., & Chiarucci, A. (2012) <doi:10.1016/j.ecocom.2012.05.007>)."
"RaceID",1,2,13,"2020-05-20 16:30","Application of 'RaceID' allows inference of cell types and prediction of lineage trees by he StemID2 algorithm. Herman, J.S., Sagar, Grün D. (2018) <doi:10.1038/nmeth.4662>."
"ProjectionBasedClustering",0,2,10,"2020-03-29 12:50","A clustering approach applicable to every projection method is proposed here. The two-dimensional scatter plot of any projection method can construct a topographic map which displays unapparent data structures by using distance and density information of the data. The generalized U*-matrix renders this visualization in the form of a topographic map, which can be used to automatically define the clusters of high-dimensional data. The whole system is based on Thrun and Ultsch, ""Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data"" <doi:10.1007/s00357-020-09373-2>. Selecting the correct projection method will result in a visualization in which mountains surround each cluster. The number of clusters can be determined by counting valleys on the topographic map. Most projection methods are wrappers for already available methods in R. By contrast, the neighbor retrieval visualizer (NeRV) is based on C++ source code of the 'dredviz' software package, and the Curvilinear Component Analysis (CCA) is translated from 'MATLAB' ('SOM Toolbox' 2.0) to R."
"poppr",3,4,34,"2020-02-25 11:10","Population genetic analyses for hierarchical analysis of partially    clonal populations built upon the architecture of the 'adegenet' package.     Originally described in Kamvar, Tabima, and Grünwald (2014)     <doi:10.7717/peerj.281> with version 2.0 described in Kamvar, Brooks, and     Grünwald (2015) <doi:10.3389/fgene.2015.00208>."
"PopGenReport",3,1,11,"2017-02-01 08:34","Provides beginner friendly framework to analyse population genetic    data. Based on 'adegenet' objects it uses 'knitr' to create comprehensive reports on spatial genetic data.     For detailed information how to use the package refer to the comprehensive    tutorials or visit <http://www.popgenreport.org/>."
"phyloregion",2,1,2,"2020-03-30 18:20","Computational infrastructure for biogeography, community ecology,     and biodiversity conservation (Daru et al. 2020) <doi:10.1101/2020.02.12.945691>.     It is based on the methods described in Daru et al. (2020) <doi:10.1038/s41467-020-15921-6>.     The original conceptual work is described in Daru et al. (2017) <doi:10.1016/j.tree.2017.08.013>     on patterns and processes of biogeographical regionalization. Additionally, the package     contains fast and efficient functions to compute more standard conservation measures     such as phylogenetic diversity, phylogenetic endemism, evolutionary distinctiveness     and global endangerment, as well as compositional turnover (e.g., beta diversity). "
"pez",2,1,6,"2019-02-27 06:40","Eco-phylogenetic and community phylogenetic analyses.    Keeps community ecological and phylogenetic data matched up and    comparable using 'comparative.comm' objects. Wrappers for common    community phylogenetic indices ('pez.shape', 'pez.evenness',    'pez.dispersion', and 'pez.dissimilarity' metrics). Implementation    of Cavender-Bares (2004) correlation of phylogenetic and    ecological matrices ('fingerprint.regression'). Phylogenetic    Generalised Linear Mixed Models (PGLMMs; 'pglmm') following Ives &    Helmus (2011) and Rafferty & Ives (2013). Simulation of null    assemblages, traits, and phylogenies ('scape', 'sim.meta.comm')."
"PCPS",0,29,8,"2014-04-30 17:21","Set of functions for analysis of Principal Coordinates of Phylogenetic Structure (PCPS)."
"patternize",0,29,2,"2017-03-29 08:38","Quantification of variation in organismal color patterns as    obtained from image data. Patternize defines homology between pattern positions    across images either through fixed landmarks or image registration. Pattern    identification is performed by categorizing the distribution of colors using RGB    thresholds or image segmentation."
"palaeoSig",1,29,5,"2015-02-17 19:05","Several tests of quantitative palaeoenvironmental reconstructions   from microfossil assemblages, including the null model tests of the   statistically significant of reconstructions developed by Telford and Birks  (2011) <doi:10.1016/j.quascirev.2011.03.002>, and tests of the effect of   spatial autocorrelation on transfer function model performance using methods   from Telford and Birks (2009) <doi:10.1016/j.quascirev.2008.12.020> and   Trachsel and Telford (2016) <doi:10.5194/cp-12-1215-2016>. Age-depth models with   generalized mixed-effect regression from Heegaard et al (2005)  <doi:10.1191/0959683605hl836rr> are also included."
"paco",0,29,9,"2019-05-20 20:50","Procrustes analyses to infer co-phylogenetic    matching between pairs of phylogenetic trees."
"NST",0,29,3,"2019-10-08 21:40","To estimate ecological stochasticity in community assembly. Understanding the community assembly mechanisms controlling biodiversity patterns is a central issue in ecology. Although it is generally accepted that both deterministic and stochastic processes play important roles in community assembly, quantifying their relative importance is challenging. The new index, normalized stochasticity ratio (NST), is to estimate ecological stochasticity, i.e. relative importance of stochastic processes, in community assembly. With functions in this package, NST can be calculated based on different similarity metrics and/or different null model algorithms, as well as some previous indexes, e.g. previous Stochasticity Ratio (ST), Standard Effect Size (SES), modified Raup-Crick metrics (RC). Functions for permutational test and bootstrapping analysis are also included. Previous ST is published by Zhou et al (2014) <doi:10.1073/pnas.1324044111>. NST is modified from ST by considering two alternative situations and normalizing the index to range from 0 to 1 (Ning et al 2019) <doi:10.1073/pnas.1904623116>. A modified version, MST, is a special case of NST, used in some recent or upcoming publications, e.g. Liang et al (2019) <doi:10.1101/638908>. SES is calculated as described in Kraft et al (2011) <doi:10.1126/science.1208584>. RC is calculated as reported by Chase et al (2011) <doi:10.1890/es10-00117.1> and Stegen et al (2013) <doi:10.1038/ismej.2013.93>. Version 3 added NST based on phylogenetic beta diversity, used by Ning et al (2020) <doi:10.1101/2020.02.22.960872>."
"nodiv",0,29,19,"2018-11-07 11:00","An implementation of the nodiv algorithm, see Borregaard, M.K., Rahbek, C., Fjeldsaa, J., Parra, J.L., Whittaker, R.J. & Graham, C.H. 2014. Node-based analysis of species distributions. Methods in Ecology and Evolution 5(11): 1225-1235. <doi:10.1111/2041-210X.12283>. Package for phylogenetic analysis of species distributions. The main function goes through each node in the phylogeny, compares the distributions of the two descendant nodes, and compares the result to a null model. This highlights nodes where major distributional divergence have occurred. The distributional divergence for these nodes is mapped using the SOS statistic."
"netcom",0,29,2,"2017-07-08 05:32","Functions to take two networks stored as matrices and return a node-level injection between them (bijection if the input networks are of the same size). The alignment is made by comparing diffusion kernels originating from each node in one network to those originating from each node in the other network. This creates a cost matrix where rows are nodes from one network and columns are nodes from the other network. Optimal node pairings are then found using the Hungarian algorithm."
"netassoc",0,29,6,"2015-11-24 07:46","Infers species associations from community matrices. Uses local and (optional) regional-scale co-occurrence data by comparing observed partial correlation coefficients between species to those estimated from regional species distributions. Extends Gaussian graphical models to a null modeling framework. Provides interface to a variety of inverse covariance matrix estimation methods. "
"mobsim",4,29,1,"2017-11-02","Tools for the simulation, analysis and sampling of spatial    biodiversity data (May et al. 2017) <doi:10.1101/209502>.    In the simulation tools user define the numbers of    species and individuals, the species abundance distribution and species    aggregation. Functions for analysis include species rarefaction     and accumulation curves, species-area relationships and the distance    decay of similarity. "
"mixKernel",0,29,4,"2018-11-26 12:00","Kernel-based methods are powerful methods for integrating     heterogeneous types of data. mixKernel aims at providing methods to combine    kernel for unsupervised exploratory analysis. Different solutions are     provided to compute a meta-kernel, in a consensus way or in a way that     best preserves the original topology of the data. mixKernel also integrates    kernel PCA to visualize similarities between samples in a non linear space    and from the multiple source point of view. Functions to assess and display    important variables are also provided in the package. Jerome Mariette and     Nathalie Villa-Vialaneix (2017) <doi:10.1093/bioinformatics/btx682>."
"mirt",1,29,58,"2020-04-24 07:20","Analysis of dichotomous and polytomous response data using    unidimensional and multidimensional latent trait models under the Item    Response Theory paradigm (Chalmers (2012) <doi:10.18637/jss.v048.i06>).     Exploratory and confirmatory models can be estimated with quadrature (EM)     or stochastic (MHRM) methods. Confirmatory    bi-factor and two-tier analyses are available for modeling item testlets.    Multiple group analysis and mixed effects designs also are available for    detecting differential item and test functioning as well as modeling    item and person covariates. Finally, latent class models such as the DINA,    DINO, multidimensional latent class, and several other discrete latent    variable models, including mixture and zero-inflated response models,     are supported."
"metacoder",1,1,10,"2019-07-18 08:35","A set of tools for parsing, manipulating, and graphing data    classified by a hierarchy (e.g. a taxonomy)."
"memgene",1,1,3,"2014-06-06 02:10","Can detect relatively weak spatial genetic patterns by using Moran's Eigenvector Maps (MEM) to extract only the spatial component of genetic variation.  Has applications in landscape genetics where the movement and dispersal of organisms are studied using neutral genetic variation."
"MEDITS",0,1,1,"2019-12-22","Set of functions working with survey data in the format of the MEDITS project <https://www.sibm.it/SITO%20MEDITS/principaleprogramme.htm>. In this version, functions use TA, TB and TC tables respectively containing haul, catch and aggregated biological data."
"MEclustnet",0,1,5,"2018-05-04 18:09","Functions to facilitate model-based clustering of nodes in a network in a mixture of experts setting, which incorporates covariate information on the nodes in the modelling process. Isobel Claire Gormley and Thomas Brendan Murphy (2010) <doi:10.1016/j.stamet.2010.01.002>."
"mcMST",2,1,2,"2017-07-11 23:48","Algorithms to approximate the Pareto-front of multi-criteria minimum spanning tree problems. Additionally, a modular toolbox for the generation of multi-objective benchmark graph problems is included."
"iCAMP",0,1,2,"2020-09-09 11:50","To implement a general framework to quantitatively infer Community Assembly Mechanisms by Phylogenetic-bin-based null model analysis, abbreviated as 'iCAMP' (Ning et al 2020) <doi:10.1038/s41467-020-18560-z>. It can quantitatively assess the relative importance of different community assembly processes, such as selection, dispersal, and drift, for both communities and each phylogenetic group ('bin'). Each bin usually consists of different taxa from a family or an order. The package also provides functions to implement some other published methods, including neutral taxa percentage (Burns et al 2016) <doi:10.1038/ismej.2015.142> based on neutral theory model (Sloan et al 2006) <doi:10.1111/j.1462-2920.2005.00956.x> and quantifying assembly processes based on entire-community null models (Stegen et al 2013) <doi:10.1038/ismej.2013.93>. It also includes some handy functions, particularly for big datasets, such as phylogenetic and taxonomic null model analysis at both community and bin levels, between-taxa niche difference and phylogenetic distance calculation, phylogenetic signal test within phylogenetic groups, midpoint root of big trees, etc."
"HTSSIP",8,1,8,"2018-05-15 09:23","Functions for analyzing high throughput sequencing     stable isotope probing (HTS-SIP) data.    Analyses include high resolution stable isotope probing (HR-SIP),    multi-window high resolution stable isotope probing (MW-HR-SIP),     and quantitative stable isotope probing (q-SIP). "
"HMP",1,1,11,"2019-08-26 23:50","Using Dirichlet-Multinomial distribution to provide several functions for formal hypothesis testing, power and sample size calculations for human microbiome experiments."
"hilldiv",0,1,1,"2019-10-01","Tools for analysing, comparing, visualising and partitioning diversity based on Hill numbers.  'hilldiv' is an R package that provides a set of functions to assist analysis of diversity for  diet reconstruction, microbial community profiling or more general ecosystem characterisation  analyses based on Hill numbers, using OTU/ASV tables and associated phylogenetic trees as  inputs. The package includes functions for (phylo)diversity measurement, (phylo)diversity  profile plotting, (phylo)diversity comparison between samples and groups, (phylo)diversity  partitioning and (dis)similarity measurement. All of these grounded in abundance-based and  incidence-based Hill numbers.  The statistical framework developed around Hill numbers encompasses many of the most  broadly employed diversity (e.g. richness, Shannon index, Simpson index),  phylogenetic diversity (e.g. Faith's PD, Allen's H, Rao's quadratic entropy) and  dissimilarity (e.g. Sorensen index, Unifrac distances) metrics. This enables the most  common analyses of diversity to be performed while grounded in a single statistical  framework. The methods are described in Jost et al. (2007) <doi:10.1890/06-1736.1>,  Chao et al. (2010) <doi:10.1098/rstb.2010.0272> and Chiu et al. (2014)  <doi:10.1890/12-0960.1>; and reviewed in the framework of molecularly characterised  biological systems in Alberdi & Gilbert (2019) <doi:10.1111/1755-0998.13014>."
"HierDpart",1,1,4,"2019-02-13 10:00","Miscellaneous R functions for calculating and decomposing hierarchical diversity metrics, including hierarchical allele richness, hierarchical exponential Shannon entropy (true diversity of order q=1), hierarchical heterozygosity and genetic differentiation (Jaccard dissimilarity, Delta D,Fst and Jost's D). In addition,a new approach to identify population structure based on the homogeneity of multivariate variances of Shannon differentiation is presented. This package allows you to analyse spatial structured genetic data or species data under a unifying framework (Gaggiotti, O. E. et al, 2018, Evol Appl, 11:1176-1193; <doi:10.1111/eva.12593>), which partitions diversity and differentiation into any hierarchical levels. It helps you easily structure and format your data. In summary,it implements the analyses of true diversity profiles (q=0,1,2), hierarchical diversities and differentiation decomposition, visualization of population structure, as well as the estimation of correlation between geographic distance and genetic differentiation."
"grapherator",3,1,1,"2017-12-21","Set of functions for step-wise generation of (weighted) graphs. Aimed for research in the field of single- and multi-objective combinatorial optimization. Graphs are generated adding nodes, edges and weights. Each step may be repeated multiple times with different predefined and custom generators resulting in high flexibility regarding the graph topology and structure of edge weights."
"graph4lg",4,1,6,"2020-07-22 09:10","Build graphs for landscape genetics analysis. This set of 	functions can be used to import and convert spatial and genetic data 	initially in different formats, import landscape graphs created with 	'GRAPHAB' software (Foltete et al., 2012) <doi:10.1016/j.envsoft.2012.07.002>, 	make diagnosis plots of isolation by distance relationships in order to 	choose how to build genetic graphs, create graphs with a large range of 	pruning methods, weight their links with several genetic distances, plot 	and analyse graphs,	compare them with other graphs. It uses functions from 	other packages such as 'adegenet' 	(Jombart, 2008) <doi:10.1093/bioinformatics/btn129> and 'igraph' (Csardi	et Nepusz, 2006) <https://igraph.org/>. It also implements methods 	commonly used in landscape genetics to create graphs, described by Dyer et 	Nason (2004) <doi:10.1111/j.1365-294X.2004.02177.x> and Greenbaum et 	Fefferman (2017) <doi:10.1111/mec.14059>, and to analyse distance data 	(van Strien et al., 2015) <doi:10.1038/hdy.2014.62>."
"goeveg",0,1,5,"2017-01-25 09:02","A collection of functions useful in (vegetation) community analyses and ordinations. Includes automatic species selection for ordination diagrams, species response curves and rank-abundance curves as well as calculation and sorting of synoptic tables."
"gdm",1,1,23,"2020-02-05 12:00","A toolkit with functions to fit, plot, summarize, and apply Generalized Dissimilarity Models."
"forams",0,1,8,"2012-08-19 07:23","SHE, FORAM Index and ABC Method analyses and custom plot        functions for community data."
"fieldRS",1,1,5,"2019-07-29 20:30","In remote sensing, designing a field campaign to collect ground-truth data can be a challenging task. We need to collect representative samples while accounting for issues such as budget constraints and limited accessibility created by e.g. poor infrastructure. As suggested by Olofsson et al. (2014) <doi:10.1016/j.rse.2014.02.015>, this demands the establishment of best-practices to collect ground-truth data that avoid the waste of time and funds. 'fieldRS' addresses this issue by helping scientists and practitioners design field campaigns through the identification of priority sampling sites, the extraction of potential sampling plots and the conversion of plots into consistent training and validation samples that can be used in e.g. land cover classification."
"evolqg",0,1,12,"2019-04-08 20:42","Provides functions for covariance matrix comparisons, estimation    of repeatabilities in measurements and matrices, and general evolutionary    quantitative genetics tools."
"EvaluateCore",0,4,1,"2020-06-03","Implements various quality evaluation statistics to assess the    value of plant germplasm core collections using qualitative and     quantitative phenotypic trait data according to Odong et al. (2015)     <doi:10.1007/s00122-012-1971-y>."
"entropart",1,4,18,"2020-01-09 00:01","Measurement and partitioning of diversity, based on Tsallis entropy, following Marcon and Herault (2015) <doi:10.18637/jss.v067.i08>.             'entropart' provides functions to calculate alpha, beta and gamma diversity of communities, including phylogenetic and functional diversity.             Estimation-bias corrections are available."
"ecospat",1,4,8,"2018-06-27 18:21","Collection of R functions and data sets for the support of spatial ecology analyses with a focus on pre, core and post modelling analyses of species distribution, niche quantification and community assembly. Written by current and former members and collaborators of the ecospat group of Antoine Guisan, Department of Ecology and Evolution (DEE) and Institute of Earth Surface Dynamics (IDYST), University of Lausanne, Switzerland. Read Di Cola et al. (2016) <doi:10.1111/ecog.02671> for details."
"dynRB",1,1,12,"2018-07-13 15:40","Improves the concept of multivariate range boxes, which is highly susceptible for outliers and does not consider the distribution of the data. The package uses dynamic range boxes to overcome these problems."
"DiversityOccupancy",1,1,7,"2016-06-19 09:47","Predictions of alpha diversity are fitted from presence data, first abundance is modeled from occupancy models and then, several diversity indices are calculated and finally GLM models are used to predict diversity in different environments and select priority areas."
"dispRity",0,1,11,"2020-09-04 12:42","A modular package for measuring disparity (multidimensional space occupancy). Disparity can be calculated from any matrix defining a multidimensional space. The package provides a set of implemented metrics to measure properties of the space and allows users to provide and test their own metrics (Guillerme (2018) <doi:10.1111/2041-210X.13022>). The package also provides functions for looking at disparity in a serial way (e.g. disparity through time - Guillerme and Cooper (2018) <doi:10.1111/pala.12364>) or per groups as well as visualising the results. Finally, this package provides several statistical tests for disparity analysis."
"Demerelate",0,1,6,"2016-05-31 11:10","Functions to calculate pairwise relatedness on diploid genetic datasets. Different estimators for relatedness can be combined with information on geographical distances. Information on heterozygosity, allele- and genotype diversity as well as genetic F-statistics are provided for each population."
"dartR",1,1,9,"2019-02-07 15:13","Functions are provided that facilitate the import and analysis of    SNP (single nucleotide polymorphism) and silicodart (presence/absence) data. The main focus is on data generated    by DarT (Diversity Arrays Technology). However, once SNP or related fragment    presence/absence data from any source is imported into a genlight object many    of the functions can be used. Functions are available for input and output of    SNP and silicodart data, for reporting on and filtering on various criteria    (e.g. CallRate, Heterozygosity, Reproducibility, maximum allele frequency).    Advanced filtering is based on Linkage Disequilibrium and HWE (Hardy-Weinberg equilibrium). Other functions    are available for visualization after PCoA (Principle Coordinate Analysis), or to facilitate transfer of data    between genlight/genind objects and newhybrids, related, phylip, structure, faststructure packages."
"DarkDiv",0,1,3,"2020-02-12 16:20","Estimation of dark diversity and site-specific species pools using species co-occurrences.     It includes implementations of probabilistic dark diversity based on the     Hypergeometric distribution, as well as estimations based on the Beals index,    which can be transformed to binary predictions using different thresholds,     or transformed into a favorability index. All methods include the possibility of    using a calibration dataset that is used to estimate the indication matrix     between pairs of species, or to estimate dark diversity directly on a single     dataset. See De Caceres and Legendre (2008) <doi:10.1007/s00442-008-1017-y>,     Lewis et al. (2016) <doi:10.1111/2041-210X.12443>,     Partel et al. (2011) <doi:10.1016/j.tree.2010.12.004>, Real et al. (2017) <doi:10.1093/sysbio/syw072>    for further information."
"codyn",3,1,8,"2019-09-17 20:50","Univariate and multivariate temporal and spatial diversity indices,     rank abundance curves, and community stability measures. The functions     implement measures that are either explicitly temporal and include the     option to calculate them over multiple replicates, or spatial and include     the option to calculate them over multiple time points. Functions fall into     five categories: static diversity indices, temporal diversity indices,     spatial diversity indices, rank abundance curves, and community stability     measures. The diversity indices are temporal and spatial analogs to     traditional diversity indices. Specifically, the package includes functions     to calculate community richness, evenness and diversity at a given point in     space and time. In addition, it contains functions to calculate species     turnover, mean rank shifts, and lags in community similarity between two     time points."
"CNVRG",0,1,1,"2020-09-16","Implements Dirichlet multinomial modelling of relative abundance data using functionality provided by the 'Stan' software. The purpose of this package is to provide a user friendly way to interface with 'Stan' that is suitable for those new to modelling. For more regarding the modelling mathematics and computational techniques we use see our publication in Molecular Ecology Resources titled ""Dirichlet multinomial modelling outperforms alternatives for analysis of ecological count data"" (Harrison et al. 2020 <doi:10.1111/1755-0998.13128>)."
"cati",0,1,6,"2015-05-29 08:43","Detect and quantify community assembly processes using trait values of individuals or populations, the T-statistics (Violle et al. (2012) <doi:10.1016/j.tree.2011.11.014>) and other metrics, and dedicated null models described in Taudiere & Violle (2016) <doi:10.1111/ecog.01433>."
"cassandRa",1,1,1,"2019-07-03","Provides methods to deal with under sampling in ecological bipartite networks. Includes      tools to fit a variety of statistical network models and sample coverage estimators to highlight most likely      missing links. Also includes simple functions to resample from observed networks to generate confidence      intervals for common ecological network metrics. "
"biogeo",0,1,1,"2016-04-08","Functions for error detection and correction in point data quality datasets that are used in species distribution modelling. Includes functions for parsing and converting coordinates into decimal degrees from various formats."
"bingat",0,1,4,"2016-01-15 22:34","Tools to analyze binary graph objects."
"BAT",0,1,21,"2020-08-29 12:00","Includes algorithms to assess alpha and beta    diversity in all their dimensions (taxon, phylogenetic and functional    diversity), whether communities are completely sampled or not. It allows    performing a number of analyses based on either species identities or    phylogenetic/functional trees or functional kernel n-dimensional hypervolumes    depicting species relationships."
"Arothron",0,1,5,"2019-10-27 00:30","Tools for geometric morphometric analysis. The package includes tools of virtual anthropology to align two not articulated parts belonging to the same specimen, to build virtual cavities as endocast (Profico et al, 2018 <doi:10.1002/ajpa.23493>)."
"aPCoA",0,1,3,"2020-06-11 06:30","In fields such as ecology, microbiology, and genomics, non-Euclidean distances are widely applied to describe pairwise dissimilarity between samples. Given these pairwise distances, principal coordinates analysis (PCoA) is commonly used to construct a visualization of the data. However, confounding covariates can make patterns related to the scientific question of interest difficult to observe. We provide 'aPCoA' as an easy-to-use tool to improve data visualization in this context, enabling enhanced presentation of the effects of interest. Details are described in Yushu Shi, Liangliang Zhang, Kim-Anh Do, Christine Peterson and Robert Jenq (2020) <arXiv:2003.09544>."
"adespatial",1,1,15,"2019-06-04 14:30","Tools for the multiscale spatial analysis of multivariate data.    Several methods are based on the use of a spatial weighting matrix and its    eigenvector decomposition (Moran's Eigenvectors Maps, MEM).     Several approaches are described in the review Dray et al (2012) <doi:10.1890/11-1183.1>."
"adegenet",0,19,36,"2020-01-20 20:50","Toolset for the exploration of genetic and genomic    data. Adegenet provides formal (S4) classes for storing and handling    various genetic data, including genetic markers with varying ploidy    and hierarchical population structure ('genind' class), alleles counts    by populations ('genpop'), and genome-wide SNP data ('genlight'). It    also implements original multivariate methods (DAPC, sPCA), graphics,    statistical tests, simulation tools, distance and similarity measures,    and several spatial methods. A range of both empirical and simulated    datasets is also provided to illustrate various methods."
"SRS",0,18,2,"2020-08-31 12:00","Analysis of species count data in ecology often requires normalization to an identical sample size. Rarefying (random subsampling without replacement), which is a popular method for normalization, has been widely criticized for its poor reproducibility and potential distortion of the community structure. In the context of microbiome count data, researchers explicitly advised against the use of rarefying. An alternative to rarefying is scaling with ranked subsampling (SRS). SRS consists of two steps. In the first step, the total counts for all OTUs (operational taxonomic units) or species in each sample are divided by a scaling factor chosen in such a way that the sum of the scaled counts Cscaled equals Cmin. In the second step, the non-integer Cscaled values are converted into integers by an algorithm that we dub ranked subsampling. The Cscaled value for each OTU or species is split into the integer part Cint  (Cint = floor(Cscaled)) and the fractional part Cfrac (Cfrac = Cscaled - Cints). Since the sum of Cint is smaller or equal to Cmin, additional  delta C = Cmin - the sum of Cint counts have to be added to the library to reach the total count of Cmin. This is achieved as follows. OTUs are ranked in the descending order of their Cfrac values. Beginning with the OTU of the highest rank, single count per OTU is added to the normalized library until the total number of added counts reaches delta C and the sum of all counts in the normalized library equals Cmin. When the lowest Cfrag involved in picking delta C counts is shared by several OTUs, the OTUs used for adding a single count to the library are selected in the order of their Cint values. This selection minimizes the effect of normalization on the relative frequencies of OTUs. OTUs with identical Cfrag as well as Cint are sampled randomly without replacement. See Beule & Karlovsky (2020) <doi:10.7717/peerj.9593> for details."
"simba",0,18,6,"2012-04-03 23:02","Besides functions for the calculation of similarity and        multiple plot similarity measures with binary data (for        instance presence/absence species data) the package contains        some simple wrapper functions for reshaping species lists into        matrices and vice versa and some other functions for further        processing of similarity data (Mantel-like permutation        procedures) as well as some other useful stuff for vegetation        analysis."
"segRDA",1,18,1,"2019-07-31","Tools for modeling non-continuous linear responses of ecological communities to environmental data. The package is straightforward through three steps: (1) data ordering (function OrdData()), (2) split-moving-window analysis (function SMW()) and (3) piecewise redundancy analysis (function pwRDA()). Relevant references include Cornelius and Reynolds (1991) <doi:10.2307/1941559> and Legendre and Legendre (2012, ISBN: 9780444538697)."
"rareNMtests",0,18,2,"2014-06-06 14:08","Randomization tests for the statistical comparison of \emph{i} = two or more individual-based, sample-based or coverage-based rarefaction curves. The ecological null hypothesis is that the \emph{i} samples were all drawn randomly from a single assemblage, with (necessarily) a single underlying species abundance distribution. The biogeographic null hypothesis is that the \emph{i} samples were all drawn from different assemblages that, nonetheless, share similar species richness and species abundance distributions"
"RAM",0,18,7,"2015-05-23 08:42","Characterizing environmental microbiota diversity using amplicon-based next generation sequencing (NGS) data. Functions are developed to manipulate operational taxonomic unit (OTU) table, perform descriptive and inferential statistics, and generate publication-quality plots."
"picante",1,18,28,"2019-03-21 16:30","Functions for phylocom integration, community analyses, null-models, traits and evolution. Implements numerous ecophylogenetic approaches including measures of community phylogenetic and trait diversity, phylogenetic signal, estimation of trait values for unobserved taxa, null models for community and phylogeny randomizations, and utility functions for data input/output and phylogeny plotting. A full description of package functionality and methods are provided by Kembel et al. (2010) <doi:10.1093/bioinformatics/btq166>."
"paleoMAS",0,1,3,"2011-02-04 07:44","Transfer functions and statistical operations for        paleoecology"
"otuSummary",0,1,2,"2018-06-19 11:01","Summarizes the taxonomic composition, diversity contribution of the rare and abundant community by using OTU (operational taxonomic unit) table which was generated by analyzing pipeline of 'QIIME' or 'mothur'. The rare biosphere in this package is subset by the relative abundance threshold (for details about rare biosphere please see Lynch and Neufeld (2015) <doi:10.1038/nrmicro3400>)."
"ordiBreadth",0,1,1,"2015-12-04","Calculates ordinated diet breadth with some plotting functions."
"mpmcorrelogram",0,1,5,"2013-03-13 21:28","Functions to compute and plot multivariate (partial) Mantel correlograms."
"MiSPU",0,1,1,"2016-03-18","There is an increasing interest in investigating how the compositions of microbial communities are associated with human health and disease. In this package, we present a novel global testing method called aMiSPU, that is highly adaptive and thus high powered across various scenarios, alleviating the issue with the choice of a phylogenetic distance. Our simulations and real data analysis demonstrated that aMiSPU test was often more powerful than several competing methods while correctly controlling type I error rates."
"metacom",0,1,15,"2019-10-13 06:20","Functions to analyze coherence, boundary clumping, and turnover    following the pattern-based metacommunity analysis of Leibold and Mikkelson    2002  <doi:10.1034/j.1600-0706.2002.970210.x>. The package also includes 		functions to visualize ecological networks, and to calculate modularity 		as a replacement to boundary clumping."
"KnowBR",0,1,9,"2018-12-06 00:00","It uses species accumulation curves and diverse estimators to assess, at the same time, the levels of survey coverage in multiple geographic cells of a size defined by the user or polygons. It also enables the geographical depiction of observed species richness, survey effort and completeness values including a background with administrative areas."
"isopam",0,1,8,"2012-11-01 22:14","Isopam clustering algorithm and utilities.  Isopam optimizes clusters and optionally cluster numbers in  a brute force style and aims at an optimum separation   by all or some descriptors (typically species).         "
"iDynoR",0,1,1,"2014-01-14","iDynoMiCS is a computer program, developed by an international team of researchers, whose purpose is to model and simulate microbial communities in an individual-based way. It is described in detail in the paper ""iDynoMiCS: next-generation individual-based modelling of biofilms"" by Lardon et al, published in Environmental Microbiology in 2011. The simulation produces results in XML file format, describing the state of each species in each timestep (agent_State), a summary of the species statistics for a timepoint (agent_Sum), the state of each solute grid in each timestep (env_State) and a summary of the solutes for a timestep (env_Sum). This R package provides a means of reading this XML data into R such that the simulation response can be statistically analysed. iDynoMiCS is available from the website iDynoMiCS.org, where a full tutorial on using both the simulation and this R package is provided."
"GUniFrac",0,1,2,"2012-04-28 18:47","Generalized UniFrac distances for comparing microbial communities. Permutational multivariate analysis of variance using multiple distance matrices."
"FreeSortR",0,12,4,"2016-05-24 18:46","Provides tools for describing and analysing free sorting data. Main methods are computation of consensus partition and factorial analysis of the dissimilarity matrix between stimuli (using multidimensional scaling approach)."
"flowDiv",0,12,2,"2015-10-20 21:13","Uses multidimensional contingency tables to calculate ecological diversity indices for gated populations."
"FD",0,12,7,"2011-03-21 07:46","FD is a package to compute different multidimensional FD indices. It implements a distance-based framework to measure FD that allows any number and type of functional traits, and can also consider species relative abundances. It also contains other useful tools for functional ecology."
"easyCODA",0,6,5,"2019-03-10 21:42","Univariate and multivariate methods for compositional data     analysis, based on logratios. The package implements the approach in the    book Compositional Data Analysis in Practice by Michael Greenacre (2018),    where accent is given to simple pairwise logratios. Selection can be made    of logratios that account for a maximum percentage of logratio variance.    Various multivariate analyses of logratios are included in the package. "
"dave",0,6,4,"2014-08-05 07:49","A collection of functions accompanying the book ""Data Analysis in Vegetation Ecology"". 3rd ed. CABI, Oxfordshire, Boston."
"CommunityCorrelogram",0,6,1,"2014-06-19","The CommunityCorrelogram package is designed for the geostatistical analysis of ecological community datasets with either a spatial or temporal distance component."
"CommEcol",0,6,3,"2017-11-20 18:25","Autosimilarity curves, standardization of spatial extent, dissimilarity indexes that overweight rare species, phylogenetic and functional (pairwise and multisample) dissimilarity indexes and nestedness for phylogenetic, functional and other diversity metrics. This should be a complement to available packages, particularly 'vegan'. "
"collpcm",0,6,2,"2017-03-13 17:11","Markov chain Monte Carlo based inference routines for collapsed latent position cluster models or social networks, which includes searches over the model space (number of clusters in the latent position cluster model). The label switching algorithm used is that of Nobile and Fearnside (2007) <doi:10.1007/s11222-006-9014-7> which relies on the algorithm of Carpaneto and Toth (1980) <doi:10.1145/355873.355883>. "
"cocorresp",1,6,11,"2019-05-11 23:50","Fits predictive and symmetric co-correspondence analysis (CoCA) models to relate one data matrix to another data matrix. More specifically, CoCA maximises the weighted covariance between the weighted averaged species scores of one community and the weighted averaged species scores of another community. CoCA attempts to find patterns that are common to both communities."
"blender",0,6,3,"2012-03-20 06:52","Tools for assessing exotic species' contributions to        landscape homogeneity using average pairwise Jaccard similarity        and an analytical approximation derived in Harris et al. (2011,        ""Occupancy is nine-tenths of the law,"" The American        Naturalist). Also includes a randomization method for assessing        sources of model error."
"bipartite",1,6,50,"2020-01-09 00:02","Functions to visualise webs and calculate a series of indices commonly used to describe pattern in (ecological) webs. It focuses on webs consisting of only two levels (bipartite), e.g. pollination webs or predator-prey-webs. Visualisation is important to get an idea of what we are actually looking at, while the indices summarise different aspects of the web's topology. "
"BiodiversityR",0,2,34,"2019-10-03 08:50","Graphical User Interface (via the R-Commander) and utility functions (often based on the vegan package) for statistical analysis of biodiversity and ecological communities, including species accumulation curves, diversity indices, Renyi profiles, GLMs for analysis of species abundance and presence-absence, distance matrices, Mantel tests, and cluster, constrained and unconstrained ordination analysis. A book on biodiversity and community ecology analysis is available for free download from the website. In 2012, methods for (ensemble) suitability modelling and mapping were expanded in the package."
"betaper",0,3,3,"2012-10-29 08:58","Permutational method to incorporate taxonomic uncertainty  and some functions to assess its effects on parameters of some widely used multivariate methods in ecology, as explained in Cayuela et al. (2011) <doi:10.1111/j.1600-0587.2009.05899.x>."
"BBI",0,3,2,"2018-05-18 18:19","Set of functions to calculate Benthic Biotic Indices from	composition data, obtained whether from morphotaxonomic inventories or	sequencing data. Based on reference ecological weights publicly available for	a set of commonly used marine biotic indices, such as AMBI (A Marine Biotic Index, Borja et al., 2000) <doi:10.1016/S0025-326X(00)00061-8>	NSI (Norwegian Sensitivity Index) and ISI (Indicator Species Index) (Rygg 2013, <ISBN:978-82-577-6210-0>). It provides the ecological quality status of the samples based on each BBI as well as the normalized Ecological Quality Ratio."
"analogue",1,3,28,"2020-02-06 07:20","Fits Modern Analogue Technique and Weighted Averaging transfer   	     function models for prediction of environmental data from species 	     data, and related methods used in palaeoecology."
"yum",0,3,1,"2019-03-13","Provides a number of functions to facilitate  extracting information in 'YAML' fragments from one or   multiple files, optionally structuring the information  in a 'data.tree'. 'YAML' (recursive acronym for ""YAML ain't  Markup Language"") is a convention for specifying structured  data in a format that is both machine- and human-readable.  'YAML' therefore lends itself well for embedding (meta)data  in plain text files, such as Markdown files. This principle  is implemented in 'yum' with minimal dependencies (i.e. only  the 'yaml' packages, and the 'data.tree' package can be  used to enable additional functionality)."
"wrTopDownFrag",1,3,1,"2020-09-08","Top-Down mass spectrometry aims to identify entire proteins as well as their (post-translational) modifications or ions bound (eg Chen et al (2018) <doi:10.1021/acs.analchem.7b04747>).     The pattern of internal fragments (Haverland et al (2017) <doi:10.1007/s13361-017-1635-x>) may reveal important information about the original structure of the proteins studied     (Skinner et al (2018) <doi:10.1038/nchembio.2515> and Li et al (2018) <doi:10.1038/nchem.2908>).     However, the number of possible internal fragments gets huge with longer proteins and subsequent identification of internal fragments remains challenging,     in particular since the the accuracy of measurements with current mass spectrometers represents a limiting factor.       This package attempts to deal with the complexity of internal fragments and allows identification of terminal and internal fragments from deconvoluted mass-spectrometry data. "
"wrMisc",1,3,7,"2020-07-24 09:10","The efficient treatment and convenient analysis of experimental high-throughput (omics) data gets facilitated through this collection of diverse functions.   Several functions address advanced object-conversions, like manipulating lists of lists or lists of arrays, reorganizing lists to arrays or into separate vectors, merging of multiple entries, etc.    Another set of functions provides speed-optimized calculation of standard deviation (sd), coefficient of variance (CV) or standard error of the mean (SEM) for data in matrixes or means per line with respect to additional grouping (eg n groups of replicates).   Other functions facilitate dealing with non-redundant information, by indexing unique, adding counters to redundant or eliminating lines with respect redundancy in a given reference-column, etc.   Help is provided to identify very closely matching numeric values to generate (partial) distance matrixes for very big data in a memory efficient manner or to reduce the complexity of large data-sets by combining very close values.   Many times large experimental datasets need some additional filtering, adequate functions are provided.   Batch reading (or writing) of sets of files and combining data to arrays is supported, too.   Convenient data normalization is supported in various different modes, parameter estimation via permutations or boot-strap as well as flexible testing of multiple pair-wise combinations using the framework of 'limma' is provided, too."
"tidygraph",0,25,5,"2019-02-18 23:30","A graph, while not ""tidy"" in itself, can be thought of as two tidy    data frames describing node and edge data respectively. 'tidygraph'    provides an approach to manipulate these two virtual data frames using the    API defined in the 'dplyr' package, as well as provides tidy interfaces to     a lot of common graph algorithms."
"styler",5,17,9,"2020-02-13 18:10","Pretty-prints R code without changing the user's    formatting intent."
"Rodam",1,2,4,"2018-10-11 18:10","'ODAM' (Open Data for Access and Mining) is a framework that implements a simple way to make research data broadly accessible and fully available for reuse, including by a script language such as R. The main purpose is to make a data set accessible online with a minimal effort from the data provider, and to allow any scientists or bioinformaticians to be able to explore the data set and then extract a subpart or the totality of the data according to their needs. The Rodam package has only one class, 'odamws', that provides methods to allow you to retrieve online data using 'ODAM' Web Services. This obviously requires that data are implemented according the 'ODAM' approach , namely that the data subsets were deposited in the suitable data repository in the form of TSV files associated with  their metadata also described  in TSV files. See <http://www.slideshare.net/danieljacob771282/odam-open-data-access-and-mining>."
"magclass",3,2,9,"2019-12-13 12:40","Data class for increased interoperability working with spatial-    temporal data together with corresponding functions and methods (conversions,    basic calculations and basic data manipulation). The class distinguishes    between spatial, temporal and other dimensions to facilitate the development    and interoperability of tools build for it. Additional features are name-based    addressing of data and internal consistency checks (e.g. checking for the right    data order in calculations)."
"finbif",6,1,3,"2020-01-16 14:00","A programmatic interface to the 'Finnish Biodiversity Information    Facility' ('FinBIF') API (<https://api.laji.fi>). 'FinBIF' aggregates    Finnish biodiversity data from multiple sources in a single open access    portal for researchers, citizen scientists, industry and government.    'FinBIF' allows users of biodiversity information to find, access, combine    and visualise data on Finnish plants, animals and microorganisms. The    'finbif' package makes the publicly available data in 'FinBIF' easily    accessible to programmers. Biodiversity information is available on taxonomy    and taxon occurrence. Occurrence data can be filtered by taxon, time,    location and other variables. The data accessed are conveniently    preformatted for subsequent analyses."
"covid19dbcand",1,1,2,"2020-06-25 18:30","Provides different datasets parsed from 'Drugbank'     <https://www.drugbank.ca/covid-19> database using 'dbparser' package.     It is a smaller version from 'dbdataset' package. It contains only information    about COVID-19 possible treatment."
"webchem",1,1,13,"2020-03-02 18:00","Chemical information from around the web. This package interacts     with a suite of web services for chemical information. Sources include: Alan    Wood's Compendium of Pesticide Common Names, Chemical Identifier Resolver,    ChEBI, Chemical Translation Service, ChemIDplus, ChemSpider, ETOX,    Flavornet, NIST Chemistry WebBook, OPSIN, PAN Pesticide Database, PubChem,    SRS, Wikidata."
"voronoiTreemap",0,2,1,"2019-01-11","The d3.js framework with the plugins d3-voronoi-map, d3-voronoi-treemap and d3-weighted-voronoi        are used to generate Voronoi treemaps in R and in a shiny application.        The computation of the Voronoi treemaps are based on Nocaj and Brandes (2012)        <doi:10.1111/j.1467-8659.2012.03078.x>."
"userfriendlyscience",0,2,16,"2018-05-02 09:43","Contains a number of functions that serve    two goals. First, to make R more accessible to people migrating    from SPSS by adding a number of functions that behave roughly like    their SPSS equivalents (also see <https://rosettastats.com>). Second,    to make a number of slightly more advanced functions more user    friendly to relatively novice users. The package also conveniently    houses a number of additional functions that are intended to    increase the quality of methodology and statistics in psychology,    not by offering technical solutions, but by shifting perspectives,    for example towards reasoning based on sampling distributions as    opposed to on point estimates."
"UniprotR",0,1,14,"2020-06-13 17:40","Connect to Uniprot <https://www.uniprot.org/> to retrieve information about proteins using their accession number such information could be name or taxonomy information, For detailed information kindly read the publication <https://www.sciencedirect.com/science/article/pii/S1874391919303859>."
"stoRy",1,1,6,"2020-04-30 14:50","An implementation of 1) the hypergeometric test for over-representation of literary themes in a storyset (a list of stories) relative to a background list of stories, and 2) a recommendation system that takes a user-selected story as input and returns a ranked list of similar stories on the basis of shared themes. The package is currently implemented for the episodes of the Star Trek television franchise series The Original Series (TOS), The Animated Series (TAS), The Next Generation (TNG), and Voyager (VOY)."
"starvz",0,1,1,"2020-09-01","Performance analysis workflow that combines the power of the R    language (and the tidyverse realm) and many auxiliary tools to    provide a consistent, flexible, extensible, fast, and versatile    framework for the performance analysis of task-based applications    that run on top of the StarPU runtime (with its MPI (Message    Passing Interface) layer for multi-node support).  Its goal is to    provide a fruitful prototypical environment to conduct performance    analysis hypothesis-checking for task-based applications that run    on heterogeneous (multi-GPU, multi-core) multi-node HPC    (High-performance computing) platforms."
"SACCR",0,1,7,"2019-01-13 13:20","Computes the Exposure-At-Default based on  the standardized approach    of CRR2 (SA-CCR). The simplified version of SA-CCR has been included, as well as the OEM methodology.	Multiple trade types of all the five major asset classes are being supported including the 'Other' Exposure and, given the inheritance-    based structure of the application, the addition of further trade types    is straightforward. The application returns a list of trees per Counterparty and CSA after    automatically separating the trades based on the Counterparty, the CSAs, the hedging sets, the    netting sets and the risk factors. The basis and volatility transactions are    also identified and treated in specific hedging sets whereby the corresponding     penalty factors are applied. All the examples appearing on the    regulatory papers (both for the margined and the un-margined workflow) have been    implemented including the latest CRR2 developments."
"rocTree",0,2,3,"2019-10-11 01:20","Receiver Operating Characteristic (ROC)-guided survival trees and ensemble algorithms are implemented, providing a unified framework for tree-structured analysis with censored survival outcomes. A time-invariant partition scheme on the survivor population was considered to incorporate time-dependent covariates. Motivated by ideas of randomized tests, generalized time-dependent ROC curves were used to evaluate the performance of survival trees and establish the optimality of the target hazard/survival function. The optimality of the target hazard function motivates us to use a weighted average of the time-dependent area under the curve (AUC) on a set of time points to evaluate the prediction performance of survival trees and to guide splitting and pruning. A detailed description of the implemented methods can be found in Sun et al. (2019) <arXiv:1809.05627>."
"radiant.model",0,2,9,"2020-08-10 19:20","The Radiant Model menu includes interfaces for linear and logistic    regression, naive Bayes, neural networks, classification and regression trees,    model evaluation, collaborative filtering, decision analysis, and simulation.     The application extends the functionality in 'radiant.data'."
"prof.tree",0,2,1,"2016-05-22","An alternative data structure for the profiling information    generated by Rprof()."
"nonlinearICP",0,2,2,"2017-07-31 15:41","Performs 'nonlinear Invariant Causal Prediction' to estimate the     causal parents of a given target variable from data collected in    different experimental or environmental conditions, extending    'Invariant Causal Prediction' from Peters, Buehlmann and Meinshausen (2016),     <arXiv:1501.01332>, to nonlinear settings. For more details, see C. Heinze-Deml,     J. Peters and N. Meinshausen: 'Invariant Causal Prediction for Nonlinear Models',     <arXiv:1706.08576>."
"momentuHMM",1,2,12,"2019-12-18 03:10","Extended tools for analyzing telemetry data using generalized hidden Markov models. Features of momentuHMM (pronounced “momentum”) include data pre-processing and visualization, fitting HMMs to location and auxiliary biotelemetry or environmental data, biased and correlated random walk movement models, hierarchical HMMs, multiple imputation for incorporating location measurement error and missing data, user-specified design matrices and constraints for covariate modelling of parameters, random effects, decoding of the state process, visualization of fitted models, model checking and selection, and simulation. See McClintock and Michelot (2018) <doi:10.1111/2041-210X.12995>."
"mindr",0,2,8,"2019-05-02 13:40","Convert Markdown ('.md') or R markdown ('.Rmd') files into mind map widgets or files ('.mm'), and vice versa. ""FreeMind"" mind map ('.mm') files can be opened by or imported to common mindmap software such as 'FreeMind' (<http://freemind.sourceforge.net/wiki/index.php/Main_Page>) and 'XMind' (<http://www.xmind.net>)."
"justifier",3,3,1,"2019-06-03","Leverages the 'yum' package to             implement a 'YAML' ('YAML Ain't Markup Language', a human             friendly standard for data serialization; see <https:yaml.org>)             standard for documenting justifications, such as for decisions             taken during the planning, execution and analysis of a study             or during the development of a behavior change intervention             as illustrated by Marques & Peters (2019)             <doi:10.17605/osf.io/ndxha>. These justifications are both             human- and machine-readable, facilitating efficient extraction             and organisation."
"hR",1,3,10,"2019-11-10 06:40","Transform and analyze workforce data in meaningful ways for human resources (HR) analytics. Get started with workforce planning using a simple Shiny app."
"HCD",0,3,1,"2019-08-24","Hierarchical community detection on networks by a recursive spectral partitioning strategy, which is shown to be effective and efficient in Li, Lei, Bhattacharyya, Sarkar, Bickel, and Levina (2018) <arXiv:1810.01509>. The package also includes a data generating function for a binary tree stochastic block model, a special case of stochastic block model that admits hierarchy between communities."
"gimme",1,3,24,"2020-09-03 22:02","Automated identification and estimation of group- and    individual-level relations in time series data from within a structural    equation modeling framework."
"forestry",1,3,1,"2020-06-25","A series of utility functions to help with     reshaping hierarchy of data tree, and reform the structure of data tree. "
"echarts4r",0,3,6,"2019-07-18 08:37","Easily create interactive charts by leveraging the 'Echarts Javascript' library which includes    36 chart types, themes, 'Shiny' proxies and animations."
"directotree",0,3,1,"2018-10-07","Represents the content of a directory as an interactive collapsible tree. Offers    the possibility to assign a text (e.g., a 'Readme.txt') to each folder    (represented as a clickable node), so that when the user hovers the pointer    over a node, the corresponding text is displayed as a tooltip."
"DecisionAnalysis",1,3,3,"2019-11-11 16:20","Aides in the multi objective decision analysis process by simplifying   the creation of value hierarchy tree plots, calculating and plotting single and   multi attribute value function scores, and conducting sensitivity analysis. Linear,   exponential, and categorical single attribute value functions are supported. For   details see Parnell (2013, ISBN:978-1-118-17313-8) Kirkwood (1997, ISBN:0-534-51692-0)."
"collapsibleTree",0,3,4,"2017-09-23 19:00","    Interactive Reingold-Tilford tree diagrams created using 'D3.js', where every node can be expanded and collapsed by clicking on it.    Tooltips and color gradients can be mapped to nodes using a numeric column in the source data frame.    See 'collapsibleTree' website for more information and examples."
"Cluster.OBeu",2,794,3,"2019-01-20 20:50","Estimate and return the needed parameters for visualisations designed for 'OpenBudgets' <http://openbudgets.eu/> data. Calculate cluster analysis measures in Budget data of municipalities across Europe, according to the 'OpenBudgets' data model. It involves a set of techniques and algorithms used to find and divide the data into groups of similar observations. Also, can be used generally to extract visualisation parameters convert them to 'JSON' format and use them as input in a different graphical interface."
"behaviorchange",4,794,6,"2019-08-22 21:10","Contains specialised analyses and    visualisation tools for behavior change science.    These facilitate conducting determinant studies    (for example, using confidence interval-based    estimation of relevance, CIBER, or CIBERlite    plots), systematically developing, reporting,    and analysing interventions (for example, using    acyclic behavior change diagrams), and reporting    about intervention effectiveness (for example, using    the Numbers Needed for Change), and computing the    required sample size (using the Meaningful Change    Definition). This package is especially useful for    researchers in the field of behavior change or    health psychology and to behavior change    professionals such as intervention developers and    prevention workers."
"ahp",2,794,6,"2017-01-06 14:44","Model and analyse complex decision making problems    using the Analytic Hierarchy Process (AHP) by Thomas Saaty."
"triversity",0,794,1,"2017-10-11","Computing diversity measures on tripartite graphs. This package first implements a parametrized family of such diversity measures which apply on probability distributions. Sometimes called ""True Diversity"", this family contains famous measures such as the richness, the Shannon entropy, the Herfindahl-Hirschman index, and the Berger-Parker index. Second, the package allows to apply these measures on probability distributions resulting from random walks between the levels of tripartite graphs. By defining an initial distribution at a given level of the graph and a path to follow between the three levels, the probability of the walker's position within the final level is then computed, thus providing a particular instance of diversity to measure."
"pmxTools",1,794,5,"2020-01-10 08:50","Pharmacometric tools for common data analytical tasks; closed-form solutions for calculating concentrations at given     times after dosing based on compartmental PK models (1-compartment, 2-compartment and 3-compartment, covering infusions, zero-     and first-order absorption, and lag times, after single doses and at steady state, per Bertrand & Mentre (2008)     <http://lixoft.com/wp-content/uploads/2016/03/PKPDlibrary.pdf>); parametric simulation from NONMEM-generated parameter estimates     and other output; and parsing, tabulating and plotting results generated by Perl-speaks-NONMEM (PsN)."
"GE",0,794,10,"2020-06-02 17:20","Some tools for developing general equilibrium models and some general equilibrium models. These models can be used for teaching economic theory and are built by the methods of new structural economics (see <https://www.nse.pku.edu.cn/> and LI Wu, 2019, ISBN: 9787521804225, General Equilibrium and Structural Dynamics: Perspectives of New Structural Economics. Beijing: Economic Science Press). The model form and mathematical methods can be traced back to von Neumann, J. (1945, A Model of General Economic Equilibrium. The Review of Economic Studies, 13. pp. 1-9) and Kemeny, J. G., O. Morgenstern and G. L. Thompson (1956, A Generalization of the von Neumann Model of an Expanding Economy, Econometrica, 24, pp. 115-135) et al. By the way, J. G. Kemeny is a co-inventor of the computer language BASIC."
"igraph",0,794,47,"2019-02-13 18:40","Routines for simple graphs and network analysis. It can  handle large graphs very well and provides functions for generating random  and regular graphs, graph visualization, centrality methods and much more."
"vegan",5,180,84,"2019-05-12 16:50","Ordination methods, diversity analysis and other  functions for community and vegetation ecologists."
